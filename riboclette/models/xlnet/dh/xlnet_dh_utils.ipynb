{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchmetrics.functional import pearson_corrcoef\n",
    "from torchmetrics.regression import MeanAbsolutePercentageError, MeanAbsoluteError\n",
    "from torchmetrics import Metric\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model functions\n",
    "class MAECoef(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state(\"mae\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "    def update(self, preds, target, mask):\n",
    "        preds = torch.sum(preds, dim=2)\n",
    "        preds = preds[:, 1:]\n",
    "        assert preds.shape == target.shape\n",
    "        assert preds.shape == mask.shape\n",
    "        coeffs = []\n",
    "        abs_error = MeanAbsoluteError()\n",
    "        for p, t, m in zip(preds, target, mask):\n",
    "            mp, mt = torch.masked_select(p, m), torch.masked_select(t, m)\n",
    "            temp_mae = abs_error(mp, mt)\n",
    "            coeffs.append(temp_mae)\n",
    "        coeffs = torch.stack(coeffs)\n",
    "        self.mae += torch.sum(coeffs)\n",
    "        self.total += len(coeffs)\n",
    "    def compute(self):\n",
    "        return self.mae / self.total\n",
    "\n",
    "class CorrCoef(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state(\"corrcoefs\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "    def update(self, preds, target, mask):\n",
    "        preds = torch.sum(preds, dim=2)\n",
    "        preds = preds[:, 1:]\n",
    "        assert preds.shape == target.shape\n",
    "        assert preds.shape == mask.shape\n",
    "        coeffs = []\n",
    "        for p, t, m in zip(preds, target, mask):\n",
    "            mp, mt = torch.masked_select(p, m), torch.masked_select(t, m)\n",
    "            temp_pearson = pearson_corrcoef(mp, mt)\n",
    "            coeffs.append(temp_pearson)\n",
    "        coeffs = torch.stack(coeffs)\n",
    "        self.corrcoefs += torch.sum(coeffs)\n",
    "        self.total += len(coeffs)\n",
    "    def compute(self):\n",
    "        return self.corrcoefs / self.total\n",
    "\n",
    "class MAPECoef(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state(\"mapecoefs\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "    def update(self, preds, target, mask):\n",
    "        preds = torch.sum(preds, dim=2)\n",
    "        preds = preds[:, 1:]\n",
    "        assert preds.shape == target.shape\n",
    "        assert preds.shape == mask.shape\n",
    "        coeffs = []\n",
    "        perc_error = MeanAbsolutePercentageError()\n",
    "        for p, t, m in zip(preds, target, mask):\n",
    "            # remove first token in p\n",
    "            mp, mt = torch.masked_select(p, m), torch.masked_select(t, m)\n",
    "            temp_mape = perc_error(mp, mt)\n",
    "            coeffs.append(temp_mape)\n",
    "        coeffs = torch.stack(coeffs)\n",
    "        self.mapecoefs += torch.sum(coeffs)\n",
    "        self.total += len(coeffs)\n",
    "    def compute(self):\n",
    "        return self.mapecoefs / self.total\n",
    "\n",
    "# collate function\n",
    "def collate_fn(batch):\n",
    "    # batch is a list of tuples (x, y)\n",
    "    x, y, ctrl_y, gene, transcript = zip(*batch)\n",
    "\n",
    "    # sequence lenghts \n",
    "    lengths = torch.tensor([len(x) for x in x])\n",
    "    \n",
    "    x = pad_sequence(x, batch_first=True, padding_value=384) \n",
    "    y = pad_sequence(y, batch_first=True, padding_value=-1)\n",
    "    ctrl_y = pad_sequence(ctrl_y, batch_first=True, padding_value=-1)\n",
    "\n",
    "    out_batch = {}\n",
    "\n",
    "    out_batch[\"input_ids\"] = x\n",
    "    out_batch[\"labels\"] = y\n",
    "    out_batch[\"lengths\"] = lengths\n",
    "    out_batch[\"labels_ctrl\"] = ctrl_y\n",
    "\n",
    "    return out_batch\n",
    "\n",
    "# compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids \n",
    "    preds = pred.predictions\n",
    "    inputs = pred.inputs\n",
    "    mask = labels != -100.0\n",
    "    labels = torch.tensor(labels)\n",
    "    preds = torch.tensor(preds)\n",
    "    preds = torch.squeeze(preds, dim=2)\n",
    "    \n",
    "    mask = torch.tensor(mask)\n",
    "    \n",
    "    # mask = torch.arange(preds.shape[1])[None, :].to(lengths) < lengths[:, None]\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(labels)))\n",
    "\n",
    "    corr_coef = CorrCoef()\n",
    "    corr_coef.update(preds, labels, mask)\n",
    "\n",
    "    mae_coef = MAECoef()\n",
    "    mae_coef.update(preds, labels, mask)\n",
    "\n",
    "    mape_coef = MAPECoef()\n",
    "    mape_coef.update(preds, labels, mask)\n",
    "\n",
    "    return {\"r\": corr_coef.compute(), \"mae\": mae_coef.compute(), \"mape\": mape_coef.compute()}\n",
    "\n",
    "# compute metrics\n",
    "def compute_metrics_saved(pred):\n",
    "    '''\n",
    "    additional function to just save everything to do analysis later\n",
    "    '''\n",
    "    labels = pred.label_ids \n",
    "    preds = pred.predictions\n",
    "    inputs = pred.inputs\n",
    "    mask = labels != -100.0\n",
    "    labels = torch.tensor(labels)\n",
    "    preds = torch.tensor(preds)\n",
    "    preds = torch.squeeze(preds, dim=2)\n",
    "    \n",
    "    mask = torch.tensor(mask)\n",
    "    \n",
    "    # mask = torch.arange(preds.shape[1])[None, :].to(lengths) < lengths[:, None]\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(labels)))\n",
    "\n",
    "    mae_coef = MAECoef()\n",
    "    mae_coef.update(preds, labels, mask)\n",
    "\n",
    "    corr_coef = CorrCoef()\n",
    "    corr_coef.update(preds, labels, mask)\n",
    "\n",
    "    mape_coef = MAPECoef()\n",
    "    mape_coef.update(preds, labels, mask)\n",
    "\n",
    "    # save predictions\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    np.save(\"preds/preds.npy\", preds)\n",
    "    np.save(\"preds/labels.npy\", labels)\n",
    "    np.save(\"preds/inputs.npy\", inputs)\n",
    "\n",
    "    return {\"r\": corr_coef.compute(), \"mae\": mae_coef.compute(), \"mape\": mape_coef.compute()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "id_to_codon = {idx:''.join(el) for idx, el in enumerate(itertools.product(['A', 'T', 'C', 'G'], repeat=3))}\n",
    "codon_to_id = {v:k for k,v in id_to_codon.items()}\n",
    "\n",
    "def checkArrayEquality(arr1, arr2):\n",
    "    '''\n",
    "    inputs: two arrays\n",
    "    outputs: True if the arrays are equal, False otherwise\n",
    "    '''\n",
    "    if len(arr1) != len(arr2):\n",
    "        return False\n",
    "    \n",
    "    for i in range(len(arr1)):\n",
    "        if arr1[i] != arr2[i]:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# dataset generation functions\n",
    "def longestZeroSeqLength(a):\n",
    "    '''\n",
    "    length of the longest sub-sequence of zeros\n",
    "    '''\n",
    "    a = a[1:-1].split(', ')\n",
    "    a = [float(k) for k in a]\n",
    "    # longest sequence of zeros\n",
    "    longest = 0\n",
    "    current = 0\n",
    "    for i in a:\n",
    "        if i == 0.0:\n",
    "            current += 1\n",
    "        else:\n",
    "            longest = max(longest, current)\n",
    "            current = 0\n",
    "    longest = max(longest, current)\n",
    "    return longest\n",
    "\n",
    "def percNans(a):\n",
    "    '''\n",
    "    returns the percentage of nans in the sequence\n",
    "    '''\n",
    "    a = a[1:-1].split(', ')\n",
    "    a = [float(k) for k in a]\n",
    "    a = np.asarray(a)\n",
    "    perc = np.count_nonzero(np.isnan(a)) / len(a)\n",
    "\n",
    "    return perc\n",
    "\n",
    "def coverageMod(a, window_size=30):\n",
    "    '''\n",
    "    returns the modified coverage function val in the sequence\n",
    "    '''\n",
    "    a = a[1:-1].split(', ')\n",
    "    a = [float(k) for k in a]\n",
    "    for i in range(len(a) - window_size):\n",
    "        if np.all(a[i:i+window_size] == 0.0):\n",
    "            a[i:i+window_size] = np.nan\n",
    "\n",
    "    # num non zero, non nan\n",
    "    num = 0\n",
    "    den = 0\n",
    "    for i in a:\n",
    "        if i != 0.0 and not np.isnan(i):\n",
    "            num += 1\n",
    "        if not np.isnan(i):\n",
    "            den += 1\n",
    "    \n",
    "    return num / den\n",
    "\n",
    "def sequenceLength(a):\n",
    "    '''\n",
    "    returns the length of the sequence\n",
    "    '''\n",
    "    a = a[1:-1].split(', ')\n",
    "    a = [float(k) for k in a]\n",
    "    return len(a)\n",
    "\n",
    "def mergeAnnotations(annots):\n",
    "    '''\n",
    "    merge the annotations for the same gene\n",
    "    '''\n",
    "    # get the annotations\n",
    "    annots = [a[1:-1].split(', ') for a in annots]\n",
    "    annots = [[float(k) for k in a] for a in annots]\n",
    "\n",
    "    # merge the annotations\n",
    "    merged_annots = []\n",
    "    for i in range(len(annots[0])):\n",
    "        # get the ith annotation for all the transcripts, only non zero and non nan\n",
    "        ith_annots = [a[i] for a in annots if a[i] != 0.0 and not np.isnan(a[i])]\n",
    "        # take the mean of the ith annotation\n",
    "        ith_mean = np.mean(ith_annots)\n",
    "        merged_annots.append(ith_mean)\n",
    "\n",
    "    return merged_annots\n",
    "\n",
    "def uniqueGenes(df):\n",
    "    # add sequence length column\n",
    "    df['sequence_length'] = df['annotations'].apply(sequenceLength)\n",
    "\n",
    "    unique_genes = list(df['gene'].unique())\n",
    "\n",
    "    # iterate through each gene, and choose the longest transcript, for the annotation, merge the annotations\n",
    "    for gene in unique_genes:\n",
    "        # get the df for the gene\n",
    "        df_gene = df[df['gene'] == gene]\n",
    "        if len(df_gene) > 1:\n",
    "            # get the transcript with the longest sequence\n",
    "            df_gene = df_gene.sort_values('sequence_length', ascending=False)\n",
    "            # chosen transcript\n",
    "            chosen_transcript = df_gene['transcript'].values[0]\n",
    "            other_transcripts = df_gene['transcript'].values[1:]\n",
    "            # merge the annotations\n",
    "            annotations = df_gene['annotations'].values\n",
    "            merged_annotations = mergeAnnotations(annotations)\n",
    "            # drop the other transcripts from the df\n",
    "            df = df[~df['transcript'].isin(other_transcripts)]\n",
    "\n",
    "            # change the annotations for the chosen transcript\n",
    "            df.loc[df['transcript'] == chosen_transcript, 'annotations'] = str(merged_annotations)\n",
    "\n",
    "    # drop sequence length column\n",
    "    df = df.drop(columns=['sequence_length'])\n",
    "\n",
    "    assert len(df['gene'].unique()) == len(df['gene'])\n",
    "    assert len(df['transcript'].unique()) == len(df['transcript'])\n",
    "    assert len(df['transcript']) == len(df['gene'])\n",
    "\n",
    "    return df\n",
    "    \n",
    "def slidingWindowZeroToNan(a, window_size=30):\n",
    "    '''\n",
    "    use a sliding window, if all the values in the window are 0, then replace them with nan\n",
    "    '''\n",
    "    a = [float(k) for k in a]\n",
    "    a = np.asarray(a)\n",
    "    for i in range(len(a) - window_size):\n",
    "        if np.all(a[i:i+window_size] == 0.0):\n",
    "            a[i:i+window_size] = np.nan\n",
    "\n",
    "    return a\n",
    "\n",
    "def RiboDatasetGWS(depr_folder: str, ds: str, threshold: float = 0.6, longZerosThresh: int = 20, percNansThresh: float = 0.1):\n",
    "    if ds == 'ALL':\n",
    "        ctrl_depr_path = depr_folder + 'CTRL_AA.csv'\n",
    "        ile_path = depr_folder + 'ILE_AA.csv'\n",
    "        leu_path = depr_folder + 'LEU_AA.csv'\n",
    "        val_path = depr_folder + 'VAL_AA.csv'\n",
    "        leu_ile_path = depr_folder + 'LEU-ILE_AA_remBadRep.csv'\n",
    "        leu_ile_val_path = depr_folder + 'LEU-ILE-VAL_AA.csv'\n",
    "        liver_path = depr_folder + 'LIVER.csv'\n",
    "\n",
    "        # load the control data\n",
    "        df_liver = pd.read_csv(liver_path)\n",
    "        df_liver['condition'] = 'CTRL'\n",
    "\n",
    "        # load ctrl_aa data\n",
    "        df_ctrl_depr = pd.read_csv(ctrl_depr_path)\n",
    "        df_ctrl_depr['condition'] = 'CTRL'\n",
    "\n",
    "        # add to the liver data the genes from ctrl depr which are not in liver\n",
    "        tr_liver = df_liver['transcript'].unique()\n",
    "        tr_ctrl_depr = df_ctrl_depr['transcript'].unique()\n",
    "        tr_to_add = [g for g in tr_liver if g not in tr_ctrl_depr]\n",
    "\n",
    "        df_liver = df_liver[df_liver['transcript'].isin(tr_to_add)]\n",
    "\n",
    "        # df_liver transcripts only, save that info\n",
    "        df_liver_transcripts = list(df_liver['transcript'])\n",
    "        np.savez('../../data/extras/liver_transcripts.npz', df_liver_transcripts)\n",
    "        print(\"Saved liver transcripts\")\n",
    "\n",
    "        # df ctrldepr without liver intersection\n",
    "        df_ctrldepr_liver = pd.concat([df_liver, df_ctrl_depr], axis=0)\n",
    "\n",
    "        # unique genes\n",
    "        df_ctrldepr_liver = uniqueGenes(df_ctrldepr_liver)\n",
    "\n",
    "        # get ctrl gene, transcript tuple pairs from the df_ctrldepr_liver\n",
    "        ctrl_genes_transcripts = list(zip(df_ctrldepr_liver['gene'], df_ctrldepr_liver['transcript']))\n",
    "        # make a list of lists\n",
    "        ctrl_genes_transcripts = [[gene, transcript] for gene, transcript in ctrl_genes_transcripts]\n",
    "\n",
    "        # other conditions\n",
    "        df_ile = pd.read_csv(ile_path)\n",
    "        df_ile['condition'] = 'ILE'\n",
    "        # unique genes\n",
    "        df_ile = uniqueGenes(df_ile)\n",
    "        # only choose those genes+transcripts that are in ctrl_depr_liver\n",
    "        # iterate through the df_ile and choose those genes that are in ctrl_genes_transcripts\n",
    "        for index, row in df_ile.iterrows():\n",
    "            if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "                df_ile.drop(index, inplace=True) \n",
    "\n",
    "        df_leu = pd.read_csv(leu_path)\n",
    "        df_leu['condition'] = 'LEU'\n",
    "        # unique genes\n",
    "        df_leu = uniqueGenes(df_leu)\n",
    "        # choose those transcripts that are in ctrl_depr_liver\n",
    "        for index, row in df_leu.iterrows():\n",
    "            if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "                df_leu.drop(index, inplace=True)\n",
    "\n",
    "        df_val = pd.read_csv(val_path)\n",
    "        df_val['condition'] = 'VAL'\n",
    "        # unique genes\n",
    "        df_val = uniqueGenes(df_val)\n",
    "        # choose those transcripts that are in ctrl_depr_liver\n",
    "        for index, row in df_val.iterrows():\n",
    "            if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "                df_val.drop(index, inplace=True)\n",
    "\n",
    "        df_leu_ile = pd.read_csv(leu_ile_path)\n",
    "        df_leu_ile['condition'] = 'LEU_ILE'\n",
    "        # unique genes\n",
    "        df_leu_ile = uniqueGenes(df_leu_ile)\n",
    "        # choose those transcripts that are in ctrl_depr_liver\n",
    "        for index, row in df_leu_ile.iterrows():\n",
    "            if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "                df_leu_ile.drop(index, inplace=True)\n",
    "\n",
    "        df_leu_ile_val = pd.read_csv(leu_ile_val_path)\n",
    "        df_leu_ile_val['condition'] = 'LEU_ILE_VAL'\n",
    "        # unique genes\n",
    "        df_leu_ile_val = uniqueGenes(df_leu_ile_val)\n",
    "        # choose those transcripts that are in ctrl_depr_liver\n",
    "        for index, row in df_leu_ile_val.iterrows():\n",
    "            if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "                df_leu_ile_val.drop(index, inplace=True)\n",
    "\n",
    "        df_full = pd.concat([df_ctrldepr_liver, df_ile, df_leu, df_val, df_leu_ile, df_leu_ile_val], axis=0) # liver + ctrl depr + ile + leu + val + leu ile + leu ile val\n",
    "\n",
    "        df_full.columns = ['index_val', 'gene', 'transcript', 'sequence', 'annotations', 'perc_non_zero_annots', 'condition']\n",
    "\n",
    "        # drop index_val column\n",
    "        df_full = df_full.drop(columns=['index_val'])\n",
    "\n",
    "        assert len(df_full['transcript'].unique()) == len(df_full['gene'].unique())\n",
    "\n",
    "        # apply annot threshold\n",
    "        df_full['coverage_mod'] = df_full['annotations'].apply(coverageMod)\n",
    "        df_full = df_full[df_full['coverage_mod'] >= threshold]\n",
    "\n",
    "        # for all the sequences in a condition that is not CTRL, add their respective CTRL sequence to them\n",
    "        sequences_ctrl = []\n",
    "        annotations_list = list(df_full['annotations'])\n",
    "        condition_df_list = list(df_full['condition'])\n",
    "        genes_list = list(df_full['gene'])\n",
    "\n",
    "        for i in range(len(condition_df_list)):\n",
    "            try:\n",
    "                if condition_df_list[i] != 'CTRL':\n",
    "                    # find the respective CTRL sequence for the transcript\n",
    "                    ctrl_sequence = df_full[(df_full['gene'] == genes_list[i]) & (df_full['condition'] == 'CTRL')]['annotations'].iloc[0]\n",
    "                    sequences_ctrl.append(ctrl_sequence)\n",
    "                else:\n",
    "                    sequences_ctrl.append(annotations_list[i])\n",
    "            except:\n",
    "                sequences_ctrl.append('NA')\n",
    "\n",
    "        # add the sequences_ctrl to the df\n",
    "        print(len(sequences_ctrl), len(annotations_list))\n",
    "        df_full['ctrl_sequence'] = sequences_ctrl\n",
    "\n",
    "        # remove those rows where the ctrl_sequence is NA\n",
    "        df_full = df_full[df_full['ctrl_sequence'] != 'NA']\n",
    "\n",
    "        # sanity check for the ctrl sequences\n",
    "        # get the ds with only condition as CTRL\n",
    "        df_ctrl_full = df_full[df_full['condition'] == 'CTRL']\n",
    "        ctrl_sequences_san = list(df_ctrl_full['annotations'])\n",
    "        ctrl_sequences_san2 = list(df_ctrl_full['ctrl_sequence'])\n",
    "\n",
    "        for i in range(len(ctrl_sequences_san)):\n",
    "            assert ctrl_sequences_san[i] == ctrl_sequences_san2[i]\n",
    "\n",
    "        print(\"Sanity Checked\")\n",
    "\n",
    "        # add the longest zero sequence length to the df\n",
    "        df_full['longest_zero_seq_length_annotation'] = df_full['annotations'].apply(longestZeroSeqLength)\n",
    "        df_full['longest_zero_seq_length_ctrl_sequence'] = df_full['ctrl_sequence'].apply(longestZeroSeqLength)\n",
    "\n",
    "        # add the number of nans to the df\n",
    "        df_full['perc_nans_annotation'] = df_full['annotations'].apply(percNans)\n",
    "        df_full['perc_nans_ctrl_sequence'] = df_full['ctrl_sequence'].apply(percNans)\n",
    "\n",
    "        # apply the threshold for the longest zero sequence length\n",
    "        df_full = df_full[df_full['longest_zero_seq_length_annotation'] <= longZerosThresh]\n",
    "        df_full = df_full[df_full['longest_zero_seq_length_ctrl_sequence'] <= longZerosThresh]\n",
    "\n",
    "        # apply the threshold for the number of nans\n",
    "        df_full = df_full[df_full['perc_nans_annotation'] <= percNansThresh]\n",
    "        df_full = df_full[df_full['perc_nans_ctrl_sequence'] <= percNansThresh]\n",
    "\n",
    "        # GWS for each condition\n",
    "        genes = df_full['gene'].unique()\n",
    "        gene_mean_coverage_mod = []\n",
    "        for gene in genes:\n",
    "            gene_mean_coverage_mod.append(df_full[df_full['gene'] == gene]['coverage_mod'].mean())\n",
    "\n",
    "        gene_mean_coverage_mod = np.asarray(gene_mean_coverage_mod)\n",
    "        genes = np.asarray(genes)\n",
    "\n",
    "        # sort the genes by coverage_mod in descending order\n",
    "        genes = genes[np.argsort(gene_mean_coverage_mod)[::-1]]\n",
    "\n",
    "        # save genes list\n",
    "        np.savez('genes_list.npz', genes)\n",
    "\n",
    "        # num_test_genes = int(0.2 * len(genes))\n",
    "        \n",
    "        # test_genes = []\n",
    "        # train_genes = []\n",
    "\n",
    "        # for i in range(len(genes)):\n",
    "        #     # alternating until 20% of the genes are in the test set\n",
    "        #     # the rest are in the train set\n",
    "        #     if i % 2 == 0 and len(test_genes) < num_test_genes:\n",
    "        #         test_genes.append(genes[i])\n",
    "        #     else:\n",
    "        #         train_genes.append(genes[i])\n",
    "\n",
    "        # # split the dataframe\n",
    "        # df_train = df_full[df_full['gene'].isin(train_genes)]\n",
    "        # df_test = df_full[df_full['gene'].isin(test_genes)]\n",
    "\n",
    "        # out_train_path = '../../data/orig/train_' + str(threshold) + '_NZ_' + str(longZerosThresh) + '_PercNan_' + str(percNansThresh) + '.csv'\n",
    "        # out_test_path = '../../data/orig/test_' + str(threshold) + '_NZ_' + str(longZerosThresh) + '_PercNan_' + str(percNansThresh) + '.csv'\n",
    "        # out_val_path = '../../data/orig/val_' + str(threshold) + '_NZ_' + str(longZerosThresh) + '_PercNan_' + str(percNansThresh) + '.csv'\n",
    "\n",
    "        # out_train_path = 'data/orig/train_' + str(threshold) + '_NZ_' + str(longZerosThresh) + '_PercNan_' + str(percNansThresh) + '.csv'\n",
    "        # out_test_path = 'data/orig/test_' + str(threshold) + '_NZ_' + str(longZerosThresh) + '_PercNan_' + str(percNansThresh) + '.csv'\n",
    "        # out_val_path = 'data/orig/val_' + str(threshold) + '_NZ_' + str(longZerosThresh) + '_PercNan_' + str(percNansThresh) + '.csv'\n",
    "\n",
    "        # df_train.to_csv(out_train_path, index=False)\n",
    "        # df_test.to_csv(out_test_path, index=False)\n",
    "        # df_val.to_csv(out_val_path, index=False)\n",
    "\n",
    "        # df_train = pd.read_csv(out_train_path)\n",
    "        # df_test = pd.read_csv(out_test_path)\n",
    "        # df_val = pd.read_csv(out_val_path)\n",
    "\n",
    "        return df_train, df_val, df_test\n",
    "\n",
    "class GWSDatasetFromPandas(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.counts = list(self.df['annotations'])\n",
    "        self.sequences = list(self.df['sequence'])\n",
    "        self.condition_lists = list(self.df['condition'])\n",
    "        self.condition_values = {'CTRL': 64, 'ILE': 65, 'LEU': 66, 'LEU_ILE': 67, 'LEU_ILE_VAL': 68, 'VAL': 69}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.df['sequence'].iloc[idx]\n",
    "        # convert to int\n",
    "        X = X[1:-1].split(', ')\n",
    "        X = [int(i) for i in X]\n",
    "\n",
    "        y = self.df['annotations'].iloc[idx]\n",
    "        # convert string into list of floats\n",
    "        y = y[1:-1].split(', ')\n",
    "        y = [float(i) for i in y]\n",
    "\n",
    "        y = slidingWindowZeroToNan(y)\n",
    "\n",
    "        y = [1+i for i in y]\n",
    "        y = np.log(y)\n",
    "\n",
    "        # ctrl sequence \n",
    "        ctrl_y = self.df['ctrl_sequence'].iloc[idx]\n",
    "        # convert string into list of floats\n",
    "        ctrl_y = ctrl_y[1:-1].split(', ')\n",
    "        ctrl_y = [float(i) for i in ctrl_y]\n",
    "\n",
    "        ctrl_y = slidingWindowZeroToNan(ctrl_y)\n",
    "\n",
    "        # no min max scaling\n",
    "        ctrl_y = [1+i for i in ctrl_y]\n",
    "        ctrl_y = np.log(ctrl_y)\n",
    "\n",
    "        X = np.array(X)\n",
    "        # multiply X with condition value times 64 + 1\n",
    "        cond_token = self.condition_values[self.condition_lists[idx]]\n",
    "        \n",
    "        # prepend the condition token to X\n",
    "        X = np.insert(X, 0, cond_token)\n",
    "\n",
    "        y = np.array(y)\n",
    "\n",
    "        X = torch.from_numpy(X).long()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        ctrl_y = torch.from_numpy(ctrl_y).float()\n",
    "\n",
    "        gene = self.df['gene'].iloc[idx]\n",
    "        transcript = self.df['transcript'].iloc[idx]\n",
    "\n",
    "        return X, y, ctrl_y, gene, transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions\n",
    "class MaskedPearsonLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def __call__(self, y_pred, y_true, mask, eps=1e-6):\n",
    "        y_pred_mask = torch.masked_select(y_pred, mask)\n",
    "        y_true_mask = torch.masked_select(y_true, mask)\n",
    "        cos = nn.CosineSimilarity(dim=0, eps=eps)\n",
    "        return 1 - cos(\n",
    "            y_pred_mask - y_pred_mask.mean(),\n",
    "            y_true_mask - y_true_mask.mean(),\n",
    "        )\n",
    "\n",
    "class MaskedL1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, y_pred, y_true, mask):\n",
    "        y_pred_mask = torch.masked_select(y_pred, mask).float()\n",
    "        y_true_mask = torch.masked_select(y_true, mask).float()\n",
    "\n",
    "        loss = nn.functional.l1_loss(y_pred_mask, y_true_mask, reduction=\"none\")\n",
    "        return torch.sqrt(loss.mean())\n",
    "\n",
    "class MaskedNormMAELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, y_pred, y_true, mask):\n",
    "        y_pred_mask = torch.masked_select(y_pred, mask).float()\n",
    "        y_true_mask = torch.masked_select(y_true, mask).float()\n",
    "\n",
    "        loss = nn.functional.l1_loss(y_pred_mask, y_true_mask, reduction=\"none\") \n",
    "        # get mean y true without nans\n",
    "        # convert y_true_mask to numpy\n",
    "        y_true_mask = y_true_mask.cpu().numpy()\n",
    "        y_true_max = np.nanmax(y_true_mask)\n",
    "\n",
    "        if y_true_max == 0:\n",
    "            return torch.sqrt(loss.mean())\n",
    "        else:\n",
    "            return torch.sqrt(loss.mean()) / y_true_max\n",
    "\n",
    "class MaskedCombinedFourDH(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pearson = MaskedPearsonLoss()\n",
    "        self.l1 = MaskedL1Loss()\n",
    "    \n",
    "    def __call__(self, y_pred, labels, labels_ctrl, mask_full, mask_ctrl, condition_):\n",
    "        # remove the first output cause that corresponds to the condition token\n",
    "        # y_pred_ctrl = y_pred[:, :, 0]\n",
    "        # relu on ctrl prediction\n",
    "        y_pred_ctrl = torch.relu(y_pred[:, :, 0])\n",
    "        \n",
    "        y_pred_depr_diff = y_pred[:, :, 1]\n",
    "        y_pred_full = torch.sum(y_pred, dim=2)\n",
    "\n",
    "        labels_diff = labels - labels_ctrl\n",
    "\n",
    "        # combine masks to make mask diff \n",
    "        mask_diff = mask_full & mask_ctrl\n",
    "\n",
    "        loss_ctrl = self.pearson(y_pred_ctrl, labels_ctrl, mask_ctrl)\n",
    "        if condition_ != 64:\n",
    "            loss_depr_diff = self.pearson(y_pred_depr_diff, labels_diff, mask_diff)\n",
    "        loss_full = self.pearson(y_pred_full, labels, mask_full) + self.l1(y_pred_full, labels, mask_full)\n",
    "\n",
    "        if condition_ != 64:\n",
    "            return loss_ctrl + loss_depr_diff + loss_full\n",
    "        else:\n",
    "            return loss_ctrl + loss_full\n",
    "\n",
    "class MaskedCombinedFiveDH(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pearson = MaskedPearsonLoss()\n",
    "        self.l1 = MaskedL1Loss()\n",
    "    \n",
    "    def __call__(self, y_pred, labels, labels_ctrl, mask_full, mask_ctrl):\n",
    "        # remove the first output cause that corresponds to the condition token\n",
    "        y_pred_ctrl = y_pred[:, :, 0]\n",
    "        y_pred_depr_diff = y_pred[:, :, 1]\n",
    "        y_pred_full = torch.sum(y_pred, dim=2)\n",
    "\n",
    "        labels_diff = labels - labels_ctrl\n",
    "\n",
    "        # combine masks to make mask diff \n",
    "        mask_diff = mask_full & mask_ctrl\n",
    "\n",
    "        loss_ctrl = self.pearson(y_pred_ctrl, labels_ctrl, mask_ctrl) + self.l1(y_pred_ctrl, labels_ctrl, mask_ctrl)\n",
    "        loss_depr_diff = self.l1(y_pred_depr_diff, labels_diff, mask_diff)\n",
    "        loss_full = self.pearson(y_pred_full, labels, mask_full) + self.l1(y_pred_full, labels, mask_full)\n",
    "\n",
    "        return loss_ctrl + loss_depr_diff + loss_full\n",
    "    \n",
    "class MaskedCombinedSixDH(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pearson = MaskedPearsonLoss()\n",
    "        self.l1 = MaskedL1Loss()\n",
    "    \n",
    "    def __call__(self, y_pred, labels, labels_ctrl, mask_full, mask_ctrl, condition_):\n",
    "        # remove the first output cause that corresponds to the condition token\n",
    "        y_pred_ctrl = y_pred[:, :, 0]\n",
    "        y_pred_depr_diff = y_pred[:, :, 1]\n",
    "        y_pred_full = torch.sum(y_pred, dim=2)\n",
    "\n",
    "        labels_diff = labels - labels_ctrl\n",
    "\n",
    "        # combine masks to make mask diff \n",
    "        mask_diff = mask_full & mask_ctrl\n",
    "\n",
    "        loss_ctrl = self.pearson(y_pred_ctrl, labels_ctrl, mask_ctrl) + self.l1(y_pred_ctrl, labels_ctrl, mask_ctrl)\n",
    "        if condition_ == 64:\n",
    "            loss_depr_diff = self.l1(y_pred_depr_diff, labels_diff, mask_diff)\n",
    "        else:\n",
    "            loss_depr_diff = self.l1(y_pred_depr_diff, labels_diff, mask_diff) + self.pearson(y_pred_depr_diff, labels_diff, mask_diff)\n",
    "        loss_full = self.pearson(y_pred_full, labels, mask_full) + self.l1(y_pred_full, labels, mask_full)\n",
    "\n",
    "        return loss_ctrl + loss_depr_diff + loss_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Four regression trainer\n",
    "class RegressionTrainerFour(Trainer):\n",
    "    def __init__(self, **kwargs,):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        condition_ = inputs['input_ids'][0][0]\n",
    "        labels_ctrl = inputs.pop(\"labels_ctrl\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        logits = torch.squeeze(logits, dim=2)\n",
    "        # remove the first output cause that corresponds to the condition token\n",
    "        logits = logits[:, 1:, :]\n",
    "        lengths = inputs['lengths']\n",
    "\n",
    "        loss_fnc = MaskedCombinedFourDH()\n",
    "        \n",
    "        mask_full = torch.arange(labels.shape[1])[None, :].to(lengths) < lengths[:, None]\n",
    "        mask_full = torch.logical_and(mask_full, torch.logical_not(torch.isnan(labels)))\n",
    "\n",
    "        mask_ctrl = torch.arange(labels_ctrl.shape[1])[None, :].to(lengths) < lengths[:, None]\n",
    "        mask_ctrl = torch.logical_and(mask_ctrl, torch.logical_not(torch.isnan(labels_ctrl)))\n",
    "        \n",
    "        loss = loss_fnc(logits, labels, labels_ctrl, mask_full, mask_ctrl, condition_)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss \n",
    "\n",
    "# custom Five regression trainer\n",
    "class RegressionTrainerFive(Trainer):\n",
    "    def __init__(self, **kwargs,):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        labels_ctrl = inputs.pop(\"labels_ctrl\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        logits = torch.squeeze(logits, dim=2)\n",
    "        # remove the first output cause that corresponds to the condition token\n",
    "        logits = logits[:, 1:, :]\n",
    "        lengths = inputs['lengths']\n",
    "\n",
    "        loss_fnc = MaskedCombinedFiveDH()\n",
    "        \n",
    "        mask_full = torch.arange(labels.shape[1])[None, :].to(lengths) < lengths[:, None]\n",
    "        mask_full = torch.logical_and(mask_full, torch.logical_not(torch.isnan(labels)))\n",
    "\n",
    "        mask_ctrl = torch.arange(labels_ctrl.shape[1])[None, :].to(lengths) < lengths[:, None]\n",
    "        mask_ctrl = torch.logical_and(mask_ctrl, torch.logical_not(torch.isnan(labels_ctrl)))\n",
    "        \n",
    "        loss = loss_fnc(logits, labels, labels_ctrl, mask_full, mask_ctrl)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss \n",
    "\n",
    "# custom Six regression trainer\n",
    "class RegressionTrainerSix(Trainer):\n",
    "    def __init__(self, **kwargs,):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        condition_ = inputs['input_ids'][0][0]\n",
    "        labels_ctrl = inputs.pop(\"labels_ctrl\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        logits = torch.squeeze(logits, dim=2)\n",
    "        # remove the first output cause that corresponds to the condition token\n",
    "        logits = logits[:, 1:, :]\n",
    "        lengths = inputs['lengths']\n",
    "\n",
    "        loss_fnc = MaskedCombinedSixDH()\n",
    "        \n",
    "        mask_full = torch.arange(labels.shape[1])[None, :].to(lengths) < lengths[:, None]\n",
    "        mask_full = torch.logical_and(mask_full, torch.logical_not(torch.isnan(labels)))\n",
    "\n",
    "        mask_ctrl = torch.arange(labels_ctrl.shape[1])[None, :].to(lengths) < lengths[:, None]\n",
    "        mask_ctrl = torch.logical_and(mask_ctrl, torch.logical_not(torch.isnan(labels_ctrl)))\n",
    "        \n",
    "        loss = loss_fnc(logits, labels, labels_ctrl, mask_full, mask_ctrl, condition_)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d8cddb8cf669a224cfe7de41be728b42e6d1e4d2fa8033c260d761c14134291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
