{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from transformers import XLNetConfig, XLNetForTokenClassification, TrainingArguments\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchmetrics.functional import pearson_corrcoef\n",
    "from torchmetrics import Metric\n",
    "import os\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import pytorch_lightning as pl\n",
    "from ipynb.fs.full.utils_dh import RegressionTrainerFive, RiboDatasetGWS, GWSDatasetFromPandas, collate_fn, compute_metrics, compute_metrics_saved  # custom dataset and trainer, CorrCoef, collate_fn, compute_metrics, compute_metrics_saved  # custom dataset and trainer\n",
    "# from ipynb.fs.full.prediction_utils import analyse_dh_outputs, quantile_metric, attention_maps, captum_LayerGradAct, captum_LayerGrad, interpretability_panels\n",
    "from pred_utils import analyse_dh_outputs, quantile_metric, attention_maps, captum_LayerGradAct, captum_LayerGrad, interpretability_panels\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "seed_val = 42\n",
    "pl.seed_everything(seed_val)\n",
    "\n",
    "# model parameters\n",
    "annot_thresh = 0.3\n",
    "longZerosThresh_val = 20\n",
    "percNansThresh_val = 0.05\n",
    "d_model_val = 256\n",
    "n_layers_val = 3\n",
    "n_heads_val = 8\n",
    "dropout_val = 0.1\n",
    "lr_val = 1e-4\n",
    "batch_size_val = 1\n",
    "loss_fun_name = '5L' # 5L\n",
    "\n",
    "# dataset paths \n",
    "data_folder = '/net/lts2gdk0/mnt/scratch/lts2/nallapar/rb-prof/data/Jan_2024/Lina/processed/'\n",
    "\n",
    "# model name and output folder path\n",
    "model_name = 'XLNetDHConds DS: DeprNA [' + str(n_layers_val) + ', ' + str(d_model_val) + ', ' + str(n_heads_val) + '] FT: [PEL] BS: ' + str(batch_size_val) + ' Loss: ' + str(loss_fun_name) + ' Data Conds: [NZ: ' + str(longZerosThresh_val) + ', PNTh: ' + str(percNansThresh_val) + ', AnnotThresh: ' + str(annot_thresh) + '] Seed: ' + str(seed_val)\n",
    "output_loc = \"saved_models/\" + model_name\n",
    "\n",
    "class XLNetDH(XLNetForTokenClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.classifier = torch.nn.Linear(d_model_val, 2, bias=True)\n",
    "\n",
    "config = XLNetConfig(vocab_size=71, pad_token_id=70, d_model = d_model_val, n_layer = n_layers_val, n_head = n_heads_val, d_inner = d_model_val, num_labels = 1, dropout=dropout_val) # 64*6 tokens + 1 for padding\n",
    "model = XLNetDH(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "ds = 'ALL' # uses all the three conditions\n",
    "train_dataset, test_dataset = RiboDatasetGWS(data_folder, ds, threshold = annot_thresh, longZerosThresh = longZerosThresh_val, percNansThresh = percNansThresh_val)\n",
    "\n",
    "# convert pandas dataframes into torch datasets\n",
    "train_dataset = GWSDatasetFromPandas(train_dataset)\n",
    "test_dataset = GWSDatasetFromPandas(test_dataset)\n",
    "print(\"samples in train dataset: \", len(train_dataset))\n",
    "print(\"samples in test dataset: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model best weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# model.load_state_dict(torch.load(output_loc + \"/best_model/pytorch_model.bin\"))\n",
    "# load model from the saved model\n",
    "model = model.from_pretrained(output_loc + \"/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlnet training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_loc,\n",
    "    learning_rate = lr_val,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = batch_size_val, # training batch size = per_device_train_batch_size * gradient_accumulation_steps\n",
    "    per_device_eval_batch_size = 1,\n",
    "    eval_accumulation_steps = 4, \n",
    "    num_train_epochs = 100,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    "    push_to_hub = False,\n",
    "    dataloader_pin_memory = True,\n",
    "    save_total_limit = 5,\n",
    "    dataloader_num_workers = 4,\n",
    "    include_inputs_for_metrics = True\n",
    ")\n",
    "\n",
    "# initialize trainer\n",
    "if loss_fun_name == '5L':\n",
    "    trainer = RegressionTrainerFive(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics_saved\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate on test set\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preds\n",
    "# make directory\n",
    "model_name = 'XLNetDHConds DS: DeprNA [' + str(n_layers_val) + ', ' + str(d_model_val) + ', ' + str(n_heads_val) + '] FT: [PEL] BS: ' + str(batch_size_val) + ' Loss: ' + str(loss_fun_name) + ' Data Conds: [NZ: ' + str(longZerosThresh_val) + ', PNTh: ' + str(percNansThresh_val) + ', AnnotThresh: ' + str(annot_thresh) + '] Seed: 2'\n",
    "model_name = model_name.replace(\" \", \"_\")\n",
    "\n",
    "dir = \"preds/\" + model_name\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "dir = \"preds/\" + model_name + \"/analysis_dh\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "dir = \"preds/\" + model_name + \"/analysis_dh/full_plots\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "else:\n",
    "    shutil.rmtree(dir)\n",
    "    os.makedirs(dir)\n",
    "\n",
    "dir = \"preds/\" + model_name + \"/analysis_dh/interpretability_panels\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "else:\n",
    "    shutil.rmtree(dir)\n",
    "    os.makedirs(dir)\n",
    "\n",
    "dir = \"preds/\" + model_name + \"/analysis_dh/quantile_metric_plots\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "else:\n",
    "    shutil.rmtree(dir)\n",
    "    os.makedirs(dir)\n",
    "\n",
    "dir = \"preds/\" + model_name + \"/analysis_dh/condition_dists\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "else:\n",
    "    shutil.rmtree(dir)\n",
    "    os.makedirs(dir)\n",
    "\n",
    "dir = \"preds/\" + model_name + \"/analysis_dh/attn_plots\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "else:\n",
    "    shutil.rmtree(dir)\n",
    "    os.makedirs(dir)\n",
    "\n",
    "dir = \"preds/\" + model_name + \"/analysis_dh/captum_plots\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "else:\n",
    "    shutil.rmtree(dir)\n",
    "    os.makedirs(dir)\n",
    "\n",
    "# move the preds to the directory\n",
    "os.system(\"mv -f \" + \"preds/preds.npy preds/\" + model_name + \"/preds.npy\")\n",
    "os.system(\"mv -f \" + \"preds/labels.npy preds/\" + model_name + \"/labels.npy\")\n",
    "os.system(\"mv -f \" + \"preds/inputs.npy preds/\" + model_name + \"/inputs.npy\")\n",
    "\n",
    "preds = np.load(\"preds/\" + model_name + \"/preds.npy\")\n",
    "labels = np.load(\"preds/\" + model_name + \"/labels.npy\")\n",
    "inputs = np.load(\"preds/\" + model_name + \"/inputs.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates plots for the best 10 and worst 10 predictions\n",
    "# saves all these plots in the \"preds/model_name/analysis_dh/full_plots\" directory\n",
    "analyse_dh_outputs(preds, labels, inputs, \"preds/\" + model_name + \"/analysis_dh\", 'data/dh/test_' + str(annot_thresh) + '_NZ_' + str(longZerosThresh_val) + '_PercNan_' + str(percNansThresh_val) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile_metric(preds, labels, inputs, \"preds/\" + model_name + \"/analysis_dh/quantile_metric_plots/\", 'data/dh/test_remBadRep_' + str(annot_thresh) + '_NZ_' + str(longZerosThresh_val) + '_PercNan_' + str(percNansThresh_val) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same plots for all the conditions (is this normal?)\n",
    "# attention_maps(model, test_dataset, \"preds/\" + model_name + \"/analysis_dh/attn_plots/\", 'CTRL')\n",
    "# attention_maps(model, test_dataset, \"preds/\" + model_name + \"/analysis_dh/attn_plots/\", 'LEU')\n",
    "# attention_maps(model, test_dataset, \"preds/\" + model_name + \"/analysis_dh/attn_plots/\", 'ILE')\n",
    "# attention_maps(model, test_dataset, \"preds/\" + model_name + \"/analysis_dh/attn_plots/\", 'VAL')\n",
    "# attention_maps(model, test_dataset, \"preds/\" + model_name + \"/analysis_dh/attn_plots/\", 'LEU_ILE')\n",
    "# attention_maps(model, test_dataset, \"preds/\" + model_name + \"/analysis_dh/attn_plots/\", 'LEU_ILE_VAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captum_LayerGradAct(model, test_dataset, \"preds/\" + model_name + \"/analysis_dh/captum_plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captum_LayerGrad(model, test_dataset, \"preds/\" + model_name + \"/analysis_dh/captum_plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretability_panels(model, preds, labels, inputs, \"preds/\" + model_name + \"/analysis_dh/\", 'data/dh/test_' + str(annot_thresh) + '_NZ_' + str(longZerosThresh_val) + '_PercNan_' + str(percNansThresh_val) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d8cddb8cf669a224cfe7de41be728b42e6d1e4d2fa8033c260d761c14134291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
