{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "import os\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr \n",
    "from torchmetrics.functional import pearson_corrcoef\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from scipy.interpolate import make_interp_spline \n",
    "from captum.attr import IntegratedGradients, LayerIntegratedGradients, LayerGradientXActivation\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables defined for all the function to use\n",
    "# one-hot encoding for the conditions\n",
    "condition_values = {'CTRL': 64, 'ILE': 65, 'LEU': 66, 'LEU_ILE': 67, 'LEU_ILE_VAL': 68, 'VAL': 69}\n",
    "inverse_condition_values = {64: 'CTRL', 65: 'ILE', 66: 'LEU', 67: 'LEU_ILE', 68: 'LEU_ILE_VAL', 69: 'VAL'}\n",
    "\n",
    "# one-hot encoding for the codons\n",
    "id_to_codon = {idx:''.join(el) for idx, el in enumerate(itertools.product(['A', 'T', 'C', 'G'], repeat=3))}\n",
    "codon_to_id = {v:k for k,v in id_to_codon.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric functions\n",
    "def pearson_mask(pred, label):\n",
    "    '''\n",
    "    inputs: model prediction, true label\n",
    "    outputs: pearson correlation coefficient\n",
    "    '''\n",
    "    # take the prediction and label\n",
    "    full_pred_tensor = torch.tensor(pred)\n",
    "    label_tensor = torch.tensor(label)\n",
    "\n",
    "    # make mask tensor\n",
    "    # remove the end token from the mask\n",
    "    mask = label_tensor != -100.0\n",
    "    mask = torch.tensor(mask)\n",
    "\n",
    "    # remove the nans from the mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(label_tensor)))\n",
    "    # double mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(full_pred_tensor)))\n",
    "\n",
    "    # set model prediction to same length as label tensor\n",
    "    full_pred_tensor = full_pred_tensor[:len(mask)]\n",
    "\n",
    "    assert full_pred_tensor.shape == label_tensor.shape\n",
    "    assert label_tensor.shape == mask.shape\n",
    "\n",
    "    # select the elements from the tensors that are not nan\n",
    "    mp, mt = torch.masked_select(full_pred_tensor, mask), torch.masked_select(label_tensor, mask)\n",
    "\n",
    "    # calculate pearson correlation coefficient\n",
    "    temp_pearson = pearson_corrcoef(mp, mt)\n",
    "\n",
    "    # get float value from tensor\n",
    "    temp_pearson = temp_pearson.item()\n",
    "\n",
    "    return temp_pearson\n",
    "\n",
    "def mae_mask(pred, label):\n",
    "    '''\n",
    "    inputs: model prediction, true label\n",
    "    outputs: mean absolute error\n",
    "    '''\n",
    "    # take the prediction and label\n",
    "    full_pred_tensor = torch.tensor(pred)\n",
    "    label_tensor = torch.tensor(label)\n",
    "\n",
    "    # make mask tensor\n",
    "    # remove the end token from the mask\n",
    "    mask = label_tensor != -100.0\n",
    "    mask = torch.tensor(mask)\n",
    "\n",
    "    # remove the nans from the mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(label_tensor)))\n",
    "    # double mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(full_pred_tensor)))\n",
    "\n",
    "    # set model prediction to same length as label tensor\n",
    "    full_pred_tensor = full_pred_tensor[:len(mask)]\n",
    "\n",
    "    assert full_pred_tensor.shape == label_tensor.shape\n",
    "    assert label_tensor.shape == mask.shape\n",
    "\n",
    "    # select the elements from the tensors that are not nan\n",
    "    mp, mt = torch.masked_select(full_pred_tensor, mask), torch.masked_select(label_tensor, mask)\n",
    "\n",
    "    # calculate mean absolute error\n",
    "    temp_mae = torch.mean(torch.abs(mp - mt))\n",
    "\n",
    "    # get float value from tensor\n",
    "    temp_mae = temp_mae.item()\n",
    "\n",
    "    return temp_mae\n",
    "\n",
    "def mape_mask(pred, label):\n",
    "    '''\n",
    "    inputs: model prediction, true label\n",
    "    outputs: mean absolute error\n",
    "    '''\n",
    "    # take the prediction and label\n",
    "    full_pred_tensor = torch.tensor(pred)\n",
    "    label_tensor = torch.tensor(label)\n",
    "\n",
    "    # make mask tensor\n",
    "    # remove the end token from the mask\n",
    "    mask = label_tensor != -100.0\n",
    "    mask = torch.tensor(mask)\n",
    "\n",
    "    # remove the nans from the mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(label_tensor)))\n",
    "    # double mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(full_pred_tensor)))\n",
    "\n",
    "    # set model prediction to same length as label tensor\n",
    "    full_pred_tensor = full_pred_tensor[:len(mask)]\n",
    "\n",
    "    assert full_pred_tensor.shape == label_tensor.shape\n",
    "    assert label_tensor.shape == mask.shape\n",
    "\n",
    "    # select the elements from the tensors that are not nan\n",
    "    mp, mt = torch.masked_select(full_pred_tensor, mask), torch.masked_select(label_tensor, mask)\n",
    "\n",
    "    # calculate mean absolute percentage error\n",
    "    temp_mape = torch.mean(torch.abs((mp - mt) / mt))\n",
    "\n",
    "    # get float value from tensor\n",
    "    temp_mape = temp_mape.item()\n",
    "\n",
    "    return temp_mape\n",
    "\n",
    "def f1_score_masked(pred, label):\n",
    "    '''\n",
    "    inputs: model prediction, true label\n",
    "    outputs: f1 score\n",
    "    '''\n",
    "    # take the prediction and label\n",
    "    full_pred_tensor = torch.tensor(pred)\n",
    "    label_tensor = torch.tensor(label)\n",
    "\n",
    "    # make mask tensor\n",
    "    # remove the end token from the mask\n",
    "    mask = label_tensor != -100.0\n",
    "    mask = torch.tensor(mask)\n",
    "\n",
    "    # remove the nans from the mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(label_tensor)))\n",
    "\n",
    "    # set model prediction to same length as label tensor\n",
    "    full_pred_tensor = full_pred_tensor[:len(mask)]\n",
    "\n",
    "    assert full_pred_tensor.shape == label_tensor.shape\n",
    "    assert label_tensor.shape == mask.shape\n",
    "\n",
    "    # select the elements from the tensors that are not nan\n",
    "    mp, mt = torch.masked_select(full_pred_tensor, mask), torch.masked_select(label_tensor, mask)\n",
    "\n",
    "    # calculate f1 score\n",
    "    temp_f1 = f1_score(mp, mt, average='macro')\n",
    "\n",
    "    # get float value from tensor\n",
    "    temp_f1 = temp_f1.item()\n",
    "\n",
    "    return temp_f1\n",
    "\n",
    "def prec_score_masked(pred, label):\n",
    "    '''\n",
    "    inputs: model prediction, true label\n",
    "    outputs: precision score\n",
    "    ''' \n",
    "    # take the prediction and label\n",
    "    full_pred_tensor = torch.tensor(pred)\n",
    "    label_tensor = torch.tensor(label)\n",
    "\n",
    "    # make mask tensor\n",
    "    # remove the end token from the mask\n",
    "    mask = label_tensor != -100.0\n",
    "    mask = torch.tensor(mask)\n",
    "\n",
    "    # remove the nans from the mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(label_tensor)))\n",
    "\n",
    "    # set model prediction to same length as label tensor\n",
    "    full_pred_tensor = full_pred_tensor[:len(mask)]\n",
    "\n",
    "    assert full_pred_tensor.shape == label_tensor.shape\n",
    "    assert label_tensor.shape == mask.shape\n",
    "\n",
    "    # select the elements from the tensors that are not nan\n",
    "    mp, mt = torch.masked_select(full_pred_tensor, mask), torch.masked_select(label_tensor, mask)\n",
    "\n",
    "    # calculate precision score\n",
    "    temp_prec = precision_score(mp, mt, average='macro')\n",
    "\n",
    "    # get float value from tensor\n",
    "    temp_prec = temp_prec.item()\n",
    "\n",
    "    return temp_prec \n",
    "\n",
    "def recall_score_masked(pred, label):\n",
    "    '''\n",
    "    inputs: model prediction, true label\n",
    "    outputs: recall score\n",
    "    '''\n",
    "    # take the prediction and label\n",
    "    full_pred_tensor = torch.tensor(pred)\n",
    "    label_tensor = torch.tensor(label)\n",
    "\n",
    "    # make mask tensor\n",
    "    # remove the end token from the mask\n",
    "    mask = label_tensor != -100.0\n",
    "    mask = torch.tensor(mask)\n",
    "\n",
    "    # remove the nans from the mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(label_tensor)))\n",
    "\n",
    "    # set model prediction to same length as label tensor\n",
    "    full_pred_tensor = full_pred_tensor[:len(mask)]\n",
    "\n",
    "    assert full_pred_tensor.shape == label_tensor.shape\n",
    "    assert label_tensor.shape == mask.shape\n",
    "\n",
    "    # select the elements from the tensors that are not nan\n",
    "    mp, mt = torch.masked_select(full_pred_tensor, mask), torch.masked_select(label_tensor, mask)\n",
    "\n",
    "    # calculate recall score\n",
    "    temp_rec = recall_score(mp, mt, average='macro')\n",
    "\n",
    "    # get float value from tensor\n",
    "    temp_rec = temp_rec.item()\n",
    "\n",
    "    return temp_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_finalexp(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model \n",
    "    \n",
    "    def forward(self, x, index_val):\n",
    "        # input dict\n",
    "        out_batch = {}\n",
    "\n",
    "        out_batch[\"input_ids\"] = x\n",
    "\n",
    "        # move to device\n",
    "        for k, v in out_batch.items():\n",
    "            out_batch[k] = v.to(device)\n",
    "\n",
    "        out_batch[\"input_ids\"] = torch.tensor(out_batch[\"input_ids\"]).to(device).to(torch.int32)\n",
    "\n",
    "        pred = self.model(out_batch[\"input_ids\"])\n",
    "\n",
    "        # add the values in the last dims\n",
    "        pred_fin = torch.sum(pred[\"logits\"], dim=2)\n",
    "\n",
    "        # squeeze\n",
    "        pred_fin = pred_fin.squeeze(0)\n",
    "\n",
    "        out = pred_fin[index_val].unsqueeze(0)\n",
    "\n",
    "        return out \n",
    "    \n",
    "class model_finalexpLIG(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model \n",
    "    \n",
    "    def forward(self, x, index_val):\n",
    "        # input dict\n",
    "        out_batch = {}\n",
    "\n",
    "        out_batch[\"input_ids\"] = x.unsqueeze(0)\n",
    "        for k, v in out_batch.items():\n",
    "            out_batch[k] = v.to(device)\n",
    "\n",
    "        out_batch[\"input_ids\"] = torch.tensor(out_batch[\"input_ids\"]).to(device).to(torch.int32)\n",
    "\n",
    "        pred = self.model(out_batch[\"input_ids\"])\n",
    "\n",
    "        pred_fin = torch.sum(pred[\"logits\"], dim=2)\n",
    "\n",
    "        pred_fin = pred_fin.squeeze(0)\n",
    "\n",
    "        out = pred_fin[index_val].unsqueeze(0)\n",
    "\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def attention_output(model, x, y, ctrl_y):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    list_attn_matrices = []\n",
    "    max_len = 0\n",
    "    lens_list = []\n",
    "    with torch.no_grad():\n",
    "        lengths = torch.tensor([len(x)])\n",
    "\n",
    "        x = torch.tensor(x).unsqueeze(0)\n",
    "        y = torch.tensor(y).unsqueeze(0)\n",
    "        ctrl_y = torch.tensor(ctrl_y).unsqueeze(0)\n",
    "        \n",
    "        x = pad_sequence(x, batch_first=True, padding_value=192) \n",
    "        y = pad_sequence(y, batch_first=True, padding_value=-1)\n",
    "        ctrl_y = pad_sequence(ctrl_y, batch_first=True, padding_value=-1)\n",
    "\n",
    "        out_batch = {}\n",
    "\n",
    "        out_batch[\"input_ids\"] = x\n",
    "        out_batch[\"labels\"] = y\n",
    "        out_batch[\"lengths\"] = lengths\n",
    "        out_batch[\"labels_ctrl\"] = ctrl_y\n",
    "\n",
    "        # send batch to device\n",
    "        for k, v in out_batch.items():\n",
    "            out_batch[k] = v.to(device)\n",
    "\n",
    "        out = model(out_batch[\"input_ids\"], output_attentions = True, return_dict = True)\n",
    "        attn_vec1 = out.attentions[0].cpu().detach().numpy()\n",
    "\n",
    "        attn_vec_full = attn_vec1 # only first layer because this is the only one that looks at the input\n",
    "\n",
    "        # remove dim 0\n",
    "        attn_vec_full = np.squeeze(attn_vec_full, axis=0)\n",
    "\n",
    "        # average across heads\n",
    "        attn_vec_full = np.mean(attn_vec_full, axis=0)\n",
    "\n",
    "    # make dataframe position_to_Asite and the attention weight for that position\n",
    "    pos_A_site_df = []\n",
    "    attn_weight_df = []\n",
    "    for i in range(len(attn_vec_full)):\n",
    "        for j in range(len(attn_vec_full)):\n",
    "            pos_A_site_df.append(j-i)\n",
    "            attn_weight_df.append(np.abs(attn_vec_full[i][j]))\n",
    "\n",
    "    df_attn_weights = pd.DataFrame({'Relative Distance from A Site': pos_A_site_df, 'Attention Weight': attn_weight_df})\n",
    "\n",
    "    return df_attn_weights\n",
    "\n",
    "def layergradactivation_output(model, x):\n",
    "    model_fin = model_finalexp(model)\n",
    "        \n",
    "    lig = LayerGradientXActivation(model_fin, model_fin.model.transformer.word_embedding)\n",
    "\n",
    "    # set torch graph to allow unused tensors\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        x = torch.tensor(x).unsqueeze(0)\n",
    "        \n",
    "        x = pad_sequence(x, batch_first=True, padding_value=192) \n",
    "\n",
    "        out_batch = {}\n",
    "\n",
    "        out_batch[\"input_ids\"] = x\n",
    "        \n",
    "        out_batch[\"input_ids\"] = torch.tensor(out_batch[\"input_ids\"]).to(device).to(torch.int32)\n",
    "        \n",
    "        # make len(x[0]) x len(x[0]) matrix\n",
    "        len_sample = len(x[0])\n",
    "        attributions_sample = np.zeros((len_sample, len_sample))\n",
    "\n",
    "        for j in range(len_sample):\n",
    "            index_val = j\n",
    "\n",
    "            index_val = torch.tensor(index_val).to(device)\n",
    "\n",
    "            attributions = lig.attribute((out_batch[\"input_ids\"]), additional_forward_args=index_val)\n",
    "            attributions = attributions.squeeze(1)\n",
    "            attributions = torch.sum(attributions, dim=1)\n",
    "            attributions = attributions / torch.norm(attributions)\n",
    "            attributions = attributions.detach().cpu().numpy()\n",
    "            attributions_sample[j] = attributions\n",
    "        \n",
    "        attributions_sample = np.array(attributions_sample)\n",
    "\n",
    "    # make dataframe position_to_Asite and the attention weight for that position\n",
    "    pos_A_site_df = []\n",
    "    attr_weight_df = []\n",
    "    for i in range(len(attributions_sample)):\n",
    "        for j in range(len(attributions_sample)):\n",
    "            pos_A_site_df.append(j-i)\n",
    "            attr_weight_df.append(np.abs(attributions_sample[i][j]))\n",
    "\n",
    "    df_attr_weights = pd.DataFrame({'Relative Distance from A Site': pos_A_site_df, 'GradxAct Weight': attr_weight_df})\n",
    "\n",
    "    return df_attr_weights\n",
    "\n",
    "def integratedgrad_output(model, x):\n",
    "    model_fin = model_finalexpLIG(model)\n",
    "        \n",
    "    lig = LayerIntegratedGradients(model_fin, model_fin.model.transformer.word_embedding)\n",
    "\n",
    "    # set torch graph to allow unused tensors\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        x = torch.tensor(x)\n",
    "\n",
    "        out_batch = {}\n",
    "\n",
    "        out_batch[\"input_ids\"] = x\n",
    "        \n",
    "        out_batch[\"input_ids\"] = torch.tensor(out_batch[\"input_ids\"]).to(device).to(torch.int32)\n",
    "\n",
    "        baseline_inp = torch.ones(out_batch[\"input_ids\"].shape) * 192\n",
    "        baseline_inp = baseline_inp.to(device).to(torch.int32)\n",
    "        \n",
    "        # make len(x[0]) x len(x[0]) matrix\n",
    "        len_sample = len(x)\n",
    "        attributions_sample = np.zeros((len_sample, len_sample))\n",
    "\n",
    "        for j in range(len_sample):\n",
    "            index_val = j\n",
    "\n",
    "            index_val = torch.tensor(index_val).to(device)\n",
    "\n",
    "            attributions, approximation_error = lig.attribute((out_batch[\"input_ids\"]), baselines=baseline_inp, \n",
    "                                                    method = 'gausslegendre', return_convergence_delta = True, additional_forward_args=index_val, n_steps=20, internal_batch_size=2048)\n",
    "\n",
    "            \n",
    "            attributions = attributions.squeeze(1)\n",
    "            attributions = torch.sum(attributions, dim=1)\n",
    "            attributions = attributions / torch.norm(attributions)\n",
    "            attributions = attributions.detach().cpu().numpy()\n",
    "            attributions_sample[j] = attributions\n",
    "        \n",
    "        attributions_sample = np.array(attributions_sample)\n",
    "\n",
    "    # make dataframe position_to_Asite and the attention weight for that position\n",
    "    pos_A_site_df = []\n",
    "    attr_weight_df = []\n",
    "    for i in range(len(attributions_sample)):\n",
    "        for j in range(len(attributions_sample)):\n",
    "            pos_A_site_df.append(j-i)\n",
    "            attr_weight_df.append(np.abs(attributions_sample[i][j]))\n",
    "\n",
    "    df_attr_weights = pd.DataFrame({'Relative Distance from A Site': pos_A_site_df, 'LIG Weight': attr_weight_df})\n",
    "\n",
    "    return df_attr_weights\n",
    "\n",
    "def interpretability_plot(model, x, y, ctrl_y, pred_y, save_path):\n",
    "    y = np.array(y)\n",
    "    pred_y = np.array(pred_y)\n",
    "    codon_position = np.array([i for i in range(len(y))])\n",
    "    ylabel_df = pd.DataFrame({'Codon Position': codon_position, 'Ribosome Density': y})\n",
    "    pred_df = pd.DataFrame({'Codon Position': codon_position, 'Ribosome Density': pred_y})\n",
    "    attn_df = attention_output(model, x, y, ctrl_y)\n",
    "    print(attn_df)\n",
    "    print('attention done')\n",
    "    gradxact_df = layergradactivation_output(model, x)\n",
    "    print(gradxact_df)\n",
    "    print('gradxact done')\n",
    "    intgrad_df = integratedgrad_output(model, x)\n",
    "    print(intgrad_df)\n",
    "    print('intgrad done')\n",
    "\n",
    "    print('making plots')\n",
    "\n",
    "    # 5 line plots: attention, layergradactivation, integratedgrad, label y, predicted y\n",
    "    # make the plots\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(30, 10))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    fig.suptitle('Interpretability Plots for the Model', fontsize=20)\n",
    "\n",
    "    # label y with array y \n",
    "    sns.lineplot(data=ylabel_df, x='Codon Position', y='Ribosome Density', ax=axs[0], color='#f1c40f', label='Label')\n",
    "    print('label done')\n",
    "\n",
    "    # predicted y with array pred_y\n",
    "    sns.lineplot(data=pred_df, x='Codon Position', y='Ribosome Density', ax=axs[1], color='#3498db', label='Predicted')\n",
    "\n",
    "    # attention plot\n",
    "    sns.lineplot(data=attn_df, x='Relative Distance from A Site', y='Attention Weight', ax=axs[2], color='#e74c3c', label='Attention')\n",
    "\n",
    "    # layergradactivation plot\n",
    "    sns.lineplot(data=gradxact_df, x='Relative Distance from A Site', y='GradxAct Weight', ax=axs[3], color='#2ecc71', label='GradxAct')\n",
    "\n",
    "    # integratedgrad plot\n",
    "    sns.lineplot(data=intgrad_df, x='Relative Distance from A Site', y='LIG Weight', ax=axs[4], color='#30336b', label='LIG')\n",
    "\n",
    "    # save the plot\n",
    "    plt.savefig(save_path)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(full_preds_sample, depr_diffs_sample, ctrl_preds_sample, labels_sample, labels_ctrl_sample, labels_depr_diff_sample, out_loc_sample, transcript_sample, gene_sample, inverse_condition_values_sample):\n",
    "    pearson_corr_full = pearson_mask(full_preds_sample, labels_sample)\n",
    "    mae_full = mae_mask(full_preds_sample, labels_sample)\n",
    "    pearson_corr_ctrl = pearson_mask(ctrl_preds_sample, labels_ctrl_sample)\n",
    "    mae_ctrl = mae_mask(ctrl_preds_sample, labels_ctrl_sample)\n",
    "\n",
    "    mae_dd = mae_mask(depr_diffs_sample, labels_depr_diff_sample)\n",
    "\n",
    "    plot_title = \"Transcript: \" + str(transcript_sample) + \" Gene: \" + str(gene_sample) + \" Condition: \" + str(inverse_condition_values_sample) + \"\\n\\nPearson Correlation \" + str(inverse_condition_values_sample) + \": \" + str(pearson_corr_full) + \" || MAE \" + str(inverse_condition_values_sample) + \":\" + str(mae_full) + \"\\n\\nPearson Correlation CTRL: \" + str(pearson_corr_ctrl) + \" || MAE CTRL: \" + str(mae_ctrl) + \"\\n\\n\" + \"MAE Depr Diffs: \" + str(mae_dd) + \"\\n\\n\"\n",
    "    \n",
    "    print(plot_title)    \n",
    "\n",
    "    # set min and max y values for the sub plot\n",
    "    # control\n",
    "    min_y_c = min(min(labels_ctrl_sample), min(ctrl_preds_sample)) - 0.1\n",
    "    max_y_c = max(max(labels_ctrl_sample), max(ctrl_preds_sample)) + 0.1\n",
    "\n",
    "    # deprivation difference\n",
    "    min_y_diff = min(min(depr_diffs_sample), min(labels_depr_diff_sample)) - 0.1\n",
    "    max_y_diff = max(max(depr_diffs_sample), max(labels_depr_diff_sample)) + 0.1\n",
    "\n",
    "    # full prediction\n",
    "    min_y_f = min(min(labels_sample), min(full_preds_sample)) - 0.1\n",
    "    max_y_f = max(max(labels_sample), max(full_preds_sample)) + 0.1\n",
    "\n",
    "    # subplots for ctrl, deprivation difference, and full prediction (with labels)\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(20, 20))\n",
    "    # add title \n",
    "    fig.suptitle(plot_title, fontsize=16)\n",
    "    # add space after title\n",
    "    fig.tight_layout(pad=10.0)\n",
    "\n",
    "    # FIRST SUBPLOT: CTRL\n",
    "    # make it bar plots\n",
    "    x = np.arange(len(ctrl_preds_sample))\n",
    "    axs[0, 0].bar(height = ctrl_preds_sample, x = x, color='#00A757', label='CTRL Prediction')\n",
    "    axs[0, 1].bar(height = labels_ctrl_sample, x = x, color='#82BA4F', label='CTRL Label')\n",
    "\n",
    "    # axs[0, 0].plot(ctrl_preds_sample, color='#00A757', label='CTRL Prediction')\n",
    "    # axs[0, 1].plot(labels_ctrl_sample, color='#82BA4F', label='CTRL Label')\n",
    "\n",
    "    # make a vector marking the nans with 0, and the rest of the values with nan\n",
    "    # make a vector of nans the same size as ctrl_preds_sample\n",
    "    labels_ctrl_nans = np.empty(len(labels_ctrl_sample))\n",
    "    labels_ctrl_nans[:] = np.nan\n",
    "    for k in range(len(labels_ctrl_sample)):\n",
    "        if np.isnan(labels_ctrl_sample[k]):\n",
    "            labels_ctrl_nans[k] = 0\n",
    "            try:\n",
    "                labels_ctrl_nans[k+1] = 0\n",
    "                labels_ctrl_nans[k-1] = 0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    label_ctrl_zeros = np.empty(len(labels_ctrl_sample))\n",
    "    for k in range(len(labels_ctrl_sample)):\n",
    "        if labels_ctrl_sample[k]:\n",
    "            label_ctrl_zeros[k] = np.nan\n",
    "        else:\n",
    "            label_ctrl_zeros[k] = 0\n",
    "\n",
    "    # plot the nans\n",
    "    axs[0, 1].plot(labels_ctrl_nans, color='black')\n",
    "    axs[0, 1].plot(label_ctrl_zeros, color='#82BA4F')\n",
    "\n",
    "    # set y limits\n",
    "    axs[0, 0].set_ylim([min_y_c, max_y_c])\n",
    "    axs[0, 1].set_ylim([min_y_c, max_y_c])\n",
    "\n",
    "    # SECOND SUBPLOT: DEPRIVATION DIFFERENCE\n",
    "    x = np.arange(len(depr_diffs_sample))\n",
    "    axs[1, 0].bar(height = depr_diffs_sample, x = x, color='#C82E6B', label='Deprivation Difference Prediction')\n",
    "    axs[1, 1].bar(height = labels_depr_diff_sample, x = x, color='#D4668F', label='Deprivation Difference Label')\n",
    "\n",
    "    # axs[1, 0].plot(depr_diffs_sample, color='#C82E6B', label='Deprivation Difference Prediction')\n",
    "    # axs[1, 1].plot(labels_depr_diff_sample, color='#D4668F', label='Deprivation Difference Label')\n",
    "\n",
    "    # make a vector marking the nans in labels_depr_diff_sample\n",
    "    labels_depr_diff_nans = np.empty(len(labels_depr_diff_sample))\n",
    "    labels_depr_diff_nans[:] = np.nan\n",
    "    for k in range(len(labels_depr_diff_sample)):\n",
    "        if np.isnan(labels_depr_diff_sample[k]):\n",
    "            labels_depr_diff_nans[k] = 0\n",
    "            try:\n",
    "                labels_depr_diff_nans[k+1] = 0\n",
    "                labels_depr_diff_nans[k-1] = 0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    labels_depr_diff_zeros = np.empty(len(labels_depr_diff_sample))\n",
    "    for k in range(len(labels_depr_diff_sample)):\n",
    "        if labels_depr_diff_sample[k]:\n",
    "            labels_depr_diff_zeros[k] = np.nan\n",
    "        else:\n",
    "            labels_depr_diff_zeros[k] = 0\n",
    "\n",
    "    # plot the nans\n",
    "    axs[1, 1].plot(labels_depr_diff_nans, color='black')\n",
    "    axs[1, 1].plot(labels_depr_diff_zeros, color='#D4668F')\n",
    "\n",
    "    # set y limits\n",
    "    axs[1, 0].set_ylim([min_y_diff, max_y_diff])\n",
    "    axs[1, 1].set_ylim([min_y_diff, max_y_diff])\n",
    "\n",
    "    # THIRD SUBPLOT: DEPRIVATION FULL PREDICTION\n",
    "    x = np.arange(len(full_preds_sample))\n",
    "    axs[2, 0].bar(height = full_preds_sample, x = x, color='#65BADA', label= inverse_condition_values_sample + ' Prediction')\n",
    "    axs[2, 1].bar(height = labels_sample, x = x, color='#87D0E2', label=inverse_condition_values_sample + ' Label')\n",
    "\n",
    "    # axs[2, 0].plot(full_preds_sample, color='#65BADA', label= inverse_condition_values_sample + ' Prediction')\n",
    "    # axs[2, 1].plot(labels_sample, color='#87D0E2', label=inverse_condition_values_sample + ' Label')\n",
    "\n",
    "    # make a vector marking the nans in labels_sample\n",
    "    labels_full_nans = np.empty(len(labels_sample))\n",
    "    labels_full_nans[:] = np.nan\n",
    "    for k in range(len(labels_sample)):\n",
    "        if np.isnan(labels_sample[k]):\n",
    "            labels_full_nans[k] = 0\n",
    "            try:\n",
    "                labels_full_nans[k+1] = 0\n",
    "                labels_full_nans[k-1] = 0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    labels_full_zeros = np.empty(len(labels_sample))\n",
    "    for k in range(len(labels_sample)):\n",
    "        if labels_sample[k]:\n",
    "            labels_full_zeros[k] = np.nan\n",
    "        else:\n",
    "            labels_full_zeros[k] = 0\n",
    "\n",
    "    # plot the nans\n",
    "    axs[2, 1].plot(labels_full_nans, color='black')\n",
    "    axs[2, 1].plot(labels_full_zeros, color='#87D0E2')\n",
    "\n",
    "    # set y limits\n",
    "    axs[2, 0].set_ylim([min_y_f, max_y_f])\n",
    "    axs[2, 1].set_ylim([min_y_f, max_y_f])\n",
    "\n",
    "    # set x and y labels for all the subplots \n",
    "    for k in range(3):\n",
    "        for j in range(2):\n",
    "            axs[k, j].set_xlabel('Codon Position', fontsize=16)\n",
    "            axs[k, j].set_ylabel('Ribosome Read Counts', fontsize=16)\n",
    "            axs[k, j].legend(fontsize=16, loc=\"upper right\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(out_loc_sample)\n",
    "\n",
    "    # display the plot\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def checkArrayEquality(arr1, arr2):\n",
    "    '''\n",
    "    inputs: two arrays\n",
    "    outputs: True if the arrays are equal, False otherwise\n",
    "    '''\n",
    "    if len(arr1) != len(arr2):\n",
    "        return False\n",
    "    \n",
    "    for i in range(len(arr1)):\n",
    "        if arr1[i] != arr2[i]:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def analyse_dh_outputs(preds, labels, inputs, output_loc, test_data_path):\n",
    "    '''\n",
    "    inputs: model predictions, true labels, inputs, output folder path\n",
    "    outputs: \n",
    "    1. prints the condition wise mean pearson correlation coefficient\n",
    "    2. generates plots for the two heads (ctrl, and deprivation difference), full prediction, and the two labels (ctrl, and deprivation) for the 10 best and 10 worst performing transcripts\n",
    "    '''\n",
    "\n",
    "    # load data\n",
    "    ds = pd.read_csv(test_data_path)\n",
    "\n",
    "    # make masks for all the transcripts\n",
    "    mask = inputs != -100.0\n",
    "\n",
    "    # obtain lengths of all the transcripts\n",
    "    lengths = np.sum(mask, axis=1)\n",
    "\n",
    "    # convert to lists and remove padding + first cond token\n",
    "    preds = preds.tolist()\n",
    "    preds = [pred[1:lengths[i]] for i, pred in enumerate(preds)]\n",
    "\n",
    "    labels = labels.tolist()\n",
    "    labels = [label[:lengths[i]-1] for i, label in enumerate(labels)]\n",
    "\n",
    "    inputs = inputs.tolist()\n",
    "    inputs = [input[:lengths[i]] for i, input in enumerate(inputs)]\n",
    "\n",
    "    # get the condition for each of the samples\n",
    "    # take the first token to get the condition\n",
    "    condition_samples = []\n",
    "    for i in range(len(inputs)):\n",
    "        condition_samples.append(inputs[i][0])\n",
    "\n",
    "    labels_ctrl = []\n",
    "    genes = []\n",
    "    transcripts = []\n",
    "    sequences_ds = []\n",
    "\n",
    "    sequence_list = list(ds['sequence'])\n",
    "    ctrl_sequence_list_ds = list(ds['ctrl_sequence'])\n",
    "    genes_list_ds = list(ds['gene'])\n",
    "    transcripts_list_ds = list(ds['transcript'])\n",
    "    condition_list = list(ds['condition'])\n",
    "    codon_sequences = []\n",
    "\n",
    "    # for loop which takes the original transcript one-hot sequence and converts into the [cond_val, x] version (stored in sequences_ds) \n",
    "    for i in range(len(sequence_list)):\n",
    "        x = sequence_list[i][1:-1].split(', ')\n",
    "        x = [int(i) for i in x]\n",
    "        cond_val = condition_values[condition_list[i]]\n",
    "        # get codon sequence from x\n",
    "        codon_seq = [id_to_codon[i] for i in x]\n",
    "        # convert to string\n",
    "        codon_seq = ''.join(codon_seq)\n",
    "        codon_sequences.append(codon_seq)\n",
    "        # get the remainder\n",
    "        # prepend the condition value to the sequence\n",
    "        x = np.insert(x, 0, cond_val)\n",
    "        sequences_ds.append(x)\n",
    "\n",
    "    # for loop to get the control label for all the transcripts\n",
    "    for i in range(len(inputs)):\n",
    "        condition_sample = inverse_condition_values[condition_samples[i]]\n",
    "        # search for inputs[i] in sequences_ds get index\n",
    "        for j in range(len(sequences_ds)):\n",
    "            if checkArrayEquality(sequences_ds[j], inputs[i]) and condition_sample == condition_list[j]:\n",
    "                index = j\n",
    "                break\n",
    "\n",
    "        ctrl_sample = ctrl_sequence_list_ds[index]\n",
    "        ctrl_sample = ctrl_sample[1:-1].split(', ')\n",
    "        ctrl_sample = [float(k) for k in ctrl_sample]\n",
    "        labels_ctrl.append(ctrl_sample)\n",
    "        genes.append(genes_list_ds[index])\n",
    "        transcripts.append(transcripts_list_ds[index])\n",
    "\n",
    "    # Model output: the first dim of pred is ctrl, second is depr difference\n",
    "    # process ctrl predictions\n",
    "    ctrl_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_sample = preds[i]\n",
    "        pred_sample = np.asarray(pred_sample)\n",
    "        # get first dim\n",
    "        pred_sample = pred_sample[:, 0]\n",
    "        ctrl_preds.append(pred_sample)\n",
    "\n",
    "    # process depr difference predictions\n",
    "    depr_diffs = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_sample = preds[i]\n",
    "        pred_sample = np.asarray(pred_sample)\n",
    "        # get second dim\n",
    "        pred_sample = pred_sample[:, 1]\n",
    "        depr_diffs.append(pred_sample)\n",
    "\n",
    "    # obtain the full predictions: the summation of the ctrl and depr difference predictions\n",
    "    full_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        full_preds.append(ctrl_preds[i] + depr_diffs[i])\n",
    "\n",
    "    # np log 1 + x the labels\n",
    "    labels_ctrl = [np.log1p(label) for label in labels_ctrl]\n",
    "\n",
    "    # labels depr difference\n",
    "    labels_depr_diff = []\n",
    "    for i in range(len(labels)):\n",
    "        labels_depr_diff.append(np.asarray(labels[i]) - np.asarray(labels_ctrl[i]))\n",
    "\n",
    "    # plot ten best samples\n",
    "    # get pearson corr for each sample\n",
    "    pearson_corrs = []\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    for i in range(len(full_preds)):\n",
    "        pearson_corrs.append(pearson_mask(full_preds[i], labels[i]))\n",
    "        mae_list.append(mae_mask(full_preds[i], labels[i]))\n",
    "        mape_list.append(mape_mask(full_preds[i], labels[i]))\n",
    "\n",
    "    # pearson mean for each condition\n",
    "    pearson_means = [[] for i in range(6)]\n",
    "    for i in range(len(pearson_corrs)):\n",
    "        pearson_means[condition_samples[i]-64].append(pearson_corrs[i])\n",
    "    \n",
    "    # print means\n",
    "    print(\"Pearson Correlation Coefficient Means - Per Condition\")\n",
    "    for i in range(len(pearson_means)):\n",
    "        print(\"Condition: \", inverse_condition_values[i+64], \" Mean: \", np.mean(pearson_means[i]), \" Std: \", np.std(pearson_means[i]), \" Num Samples: \", len(pearson_means[i]))\n",
    "\n",
    "    conds_colors = [\"#264653\", \"#2A9D8F\", \"#E9C46A\", '#F4A261', '#E76F51', '#ef476f']\n",
    "    # make distribution plot for each condition\n",
    "    for i in range(len(pearson_means)):\n",
    "        # sns histogram\n",
    "        sns.histplot(pearson_means[i], color=conds_colors[i], kde=True, bins=100)\n",
    "        plt.title(\"Pearson Correlation Coefficient Distribution - \" + inverse_condition_values[i+64])\n",
    "        plt.xlabel(\"Pearson Correlation Coefficient\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.savefig(output_loc + \"/condition_dists/pearson_\" + inverse_condition_values[i+64] + \".png\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "    pearson_corrs_ctrl = []\n",
    "    for i in range(len(ctrl_preds)):\n",
    "        pearson_corrs_ctrl.append(pearson_mask(ctrl_preds[i], labels_ctrl[i]))\n",
    "\n",
    "    # output all the predictions into df from lists\n",
    "    output_analysis_df = pd.DataFrame(list(zip(transcripts, genes, codon_sequences, pearson_corrs, pearson_corrs_ctrl, mae_list, mape_list, condition_list)), columns =['Transcript', 'Gene', 'Sequence', 'Full Prediction Pearson Correlation', 'Control Prediction Pearson Correlation', 'MAE', 'MAPE', 'Deprivation Condition'])\n",
    "    output_analysis_df.to_csv(output_loc + \"/analysis.csv\", index=False)\n",
    "\n",
    "    print(\"Saved model prediction outputs file to \", output_loc + \"/analysis.csv\")\n",
    "\n",
    "    print(\"#\"*20)\n",
    "\n",
    "    num_plots = 10\n",
    "\n",
    "    ######### \n",
    "    # plot (num_plots) best samples\n",
    "    #########\n",
    "    best_samples = sorted(range(len(pearson_corrs)), key = lambda sub: pearson_corrs[sub])[-num_plots:]\n",
    "    # print best pearson corrs\n",
    "    print(\"List of best pearson correlations: \", [pearson_corrs[i] for i in best_samples])\n",
    "\n",
    "    print(\"#\"*20)\n",
    "\n",
    "    for i in range(num_plots):\n",
    "        out_loc = output_loc + \"/full_plots/sample_\" + str(best_samples[i]) + '_' + str(inverse_condition_values[condition_samples[best_samples[i]]]) + \"_best_\" + transcripts[best_samples[i]] + \"_\" + genes[best_samples[i]] + \".png\"\n",
    "        make_plot(full_preds[best_samples[i]], depr_diffs[best_samples[i]], ctrl_preds[best_samples[i]], labels[best_samples[i]], labels_ctrl[best_samples[i]], labels_depr_diff[best_samples[i]], out_loc, transcripts[best_samples[i]], genes[best_samples[i]], inverse_condition_values[condition_samples[best_samples[i]]])\n",
    "\n",
    "    ######### \n",
    "    # plot (num_plots) worst samples\n",
    "    #########\n",
    "    worst_samples = sorted(range(len(pearson_corrs)), key = lambda sub: pearson_corrs[sub])[:num_plots]\n",
    "    \n",
    "    # print worst pearson corrs\n",
    "    print(\"Worst Pearson Correlations: \", [pearson_corrs[i] for i in worst_samples])\n",
    "\n",
    "    print(\"#\"*20)\n",
    "\n",
    "    for i in range(num_plots):\n",
    "        out_loc = output_loc + \"/full_plots/sample_\" + str(worst_samples[i]) + '_' + str(inverse_condition_values[condition_samples[worst_samples[i]]]) + \"_worst_\" + transcripts[worst_samples[i]] + \"_\" + genes[worst_samples[i]] + \".png\"\n",
    "        make_plot(full_preds[worst_samples[i]], depr_diffs[worst_samples[i]], ctrl_preds[worst_samples[i]], labels[worst_samples[i]], labels_ctrl[worst_samples[i]], labels_depr_diff[worst_samples[i]], out_loc, transcripts[worst_samples[i]], genes[worst_samples[i]], inverse_condition_values[condition_samples[worst_samples[i]]])\n",
    "\n",
    "def quantile_metric(preds, labels, inputs, output_loc, test_data_path):\n",
    "    # make mask removing those that have a input of -100\n",
    "    mask = inputs != -100.0\n",
    "\n",
    "    # get lengths of each sequence\n",
    "    lengths = np.sum(mask, axis=1)\n",
    "\n",
    "    # convert to lists and remove padding\n",
    "    preds = preds.tolist()\n",
    "    labels = labels.tolist()\n",
    "    inputs = inputs.tolist()\n",
    "\n",
    "    preds = [pred[:lengths[i]] for i, pred in enumerate(preds)]\n",
    "    labels = [label[:lengths[i]] for i, label in enumerate(labels)]\n",
    "    inputs = [input[:lengths[i]] for i, input in enumerate(inputs)]\n",
    "\n",
    "    condition_samples = []\n",
    "\n",
    "    # get conditions for each sample\n",
    "    # take cond_val to get the condition\n",
    "    for i in range(len(inputs)):\n",
    "        condition_samples.append(inputs[i][0])\n",
    "\n",
    "    genes = []\n",
    "    transcripts = []\n",
    "\n",
    "    ds = pd.read_csv(test_data_path)\n",
    "\n",
    "    sequences_ds = []\n",
    "\n",
    "    sequence_list = list(ds['sequence'])\n",
    "    genes_list_ds = list(ds['gene'])\n",
    "    transcripts_list_ds = list(ds['transcript'])\n",
    "    condition_list = list(ds['condition'])\n",
    "    codon_sequences = []\n",
    "\n",
    "    for i in range(len(sequence_list)):\n",
    "        x = sequence_list[i][1:-1].split(', ')\n",
    "        x = [int(i) for i in x]\n",
    "        cond_val = condition_values[condition_list[i]]\n",
    "        # get codon sequence from x\n",
    "        codon_seq = [id_to_codon[i] for i in x]\n",
    "        # convert to string\n",
    "        codon_seq = ''.join(codon_seq)\n",
    "        codon_sequences.append(codon_seq)\n",
    "        # get the remainder\n",
    "        add_val = (cond_val) * 64\n",
    "        x = [i + add_val for i in x]\n",
    "        sequences_ds.append(x)\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "        condition_sample = inverse_condition_values[condition_samples[i]]\n",
    "        # search for inputs[i] in sequences_ds get index\n",
    "        for j in range(len(sequences_ds)):\n",
    "            if sequences_ds[j] == inputs[i] and condition_sample == condition_list[j]:\n",
    "                index = j\n",
    "                break\n",
    "\n",
    "        genes.append(genes_list_ds[index])\n",
    "        transcripts.append(transcripts_list_ds[index])\n",
    "\n",
    "    # ctrl predictions\n",
    "    ctrl_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_sample = preds[i]\n",
    "        pred_sample = np.asarray(pred_sample)\n",
    "        # get first dim\n",
    "        pred_sample = pred_sample[:, 0]\n",
    "        ctrl_preds.append(pred_sample)\n",
    "\n",
    "    depr_diffs = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_sample = preds[i]\n",
    "        pred_sample = np.asarray(pred_sample)\n",
    "        # get second dim\n",
    "        pred_sample = pred_sample[:, 1]\n",
    "        depr_diffs.append(pred_sample)\n",
    "\n",
    "    full_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        full_preds.append(ctrl_preds[i] + depr_diffs[i])\n",
    "        # print(len(full_preds[i]), len(labels[i]))\n",
    "\n",
    "    # plot ten best samples\n",
    "    # get pearson corr for each sample\n",
    "    pearson_corrs = []\n",
    "    for i in range(len(full_preds)):\n",
    "        pearson_corrs.append(pearson_mask(full_preds[i], labels[i]))\n",
    "\n",
    "    # go through each prediction\n",
    "    quantiles = [j*0.1 for j in range(10)]\n",
    "    all_f1_scores = []\n",
    "    all_prec_scores = []\n",
    "    all_recall_scores = []\n",
    "    for i in range(len(pearson_corrs)):\n",
    "        # for each sample do metric for each quantile\n",
    "        # 10 quantiles\n",
    "        f1_sample = []  # f1 score for each sample\n",
    "        prec_sample = []  # precision score for each sample\n",
    "        recall_sample = []  # recall score for each sample\n",
    "        # iterate through each quantile\n",
    "        for k in range(len(quantiles)): # label quantile val is nan\n",
    "            pred_quantile_val = np.quantile(full_preds[i], quantiles[k])\n",
    "            label_quantile_val = np.nanquantile(labels[i], quantiles[k])\n",
    "\n",
    "            # binarize the prediction and label based on the quantile including the nans\n",
    "            # copy the prediction\n",
    "            pred_quantile = np.copy(full_preds[i])\n",
    "            # set all values below the quantile to 0\n",
    "            pred_quantile[pred_quantile < pred_quantile_val] = 0\n",
    "            # set all values above the quantile to 1\n",
    "            pred_quantile[pred_quantile >= pred_quantile_val] = 1\n",
    "            \n",
    "            # copy the label\n",
    "            label_quantile = np.copy(labels[i])\n",
    "            # set all values below the quantile to 0\n",
    "            label_quantile[label_quantile < label_quantile_val] = 0\n",
    "            # set all values above the quantile to 1\n",
    "            label_quantile[label_quantile >= label_quantile_val] = 1\n",
    "\n",
    "            # get f1 score for the pred_quantile and label_quantile\n",
    "            # print(\"label quantile val:\", label_quantile_val)\n",
    "            # print(\"pred quantile val:\", pred_quantile_val)\n",
    "            # print(pred_quantile, label_quantile)\n",
    "            # print(\"TO FUNCTION\")\n",
    "            f1_val = f1_score_masked(pred_quantile, label_quantile)\n",
    "            prec_val = prec_score_masked(pred_quantile, label_quantile)\n",
    "            recall_val = recall_score_masked(pred_quantile, label_quantile)\n",
    "\n",
    "            # print(f1_val)\n",
    "\n",
    "            f1_sample.append(f1_val)\n",
    "            prec_sample.append(prec_val)\n",
    "            recall_sample.append(recall_val)\n",
    "\n",
    "        all_f1_scores.append(f1_sample)\n",
    "        all_prec_scores.append(prec_sample)\n",
    "        all_recall_scores.append(recall_sample)\n",
    "\n",
    "    # make stats for each quantile from all samples\n",
    "    all_f1_scores = np.asarray(all_f1_scores)\n",
    "    all_f1_scores = np.transpose(all_f1_scores)\n",
    "\n",
    "    all_prec_scores = np.asarray(all_prec_scores)\n",
    "    all_prec_scores = np.transpose(all_prec_scores)\n",
    "\n",
    "    all_recall_scores = np.asarray(all_recall_scores)\n",
    "    all_recall_scores = np.transpose(all_recall_scores)\n",
    "\n",
    "    # print(all_f1_scores.shape)\n",
    "\n",
    "    # get mean and std for each quantile\n",
    "    quantile_means_f1 = []\n",
    "    quantile_stds_f1 = []\n",
    "    quantile_means_prec = []\n",
    "    quantile_stds_prec = []\n",
    "    quantile_means_recall = []\n",
    "    quantile_stds_recall = []\n",
    "    for f in range(len(all_f1_scores)):\n",
    "        quantile_means_f1.append(np.mean(all_f1_scores[f]))\n",
    "        quantile_stds_f1.append(np.std(all_f1_scores[f]))\n",
    "\n",
    "        quantile_means_prec.append(np.mean(all_prec_scores[f]))\n",
    "        quantile_stds_prec.append(np.std(all_prec_scores[f]))\n",
    "\n",
    "        quantile_means_recall.append(np.mean(all_recall_scores[f]))\n",
    "        quantile_stds_recall.append(np.std(all_recall_scores[f]))\n",
    "\n",
    "    # plot the mean and std for each quantile\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.errorbar(quantiles, quantile_means_f1, yerr=quantile_stds_f1, fmt='o', color='#130f40')\n",
    "    # draw a smooth curve connecting the points from means\n",
    "    X_Y_Spline = make_interp_spline(quantiles, quantile_means_f1)\n",
    "    # Returns evenly spaced numbers\n",
    "    # over a specified interval.\n",
    "    X_ = np.linspace(min(quantiles), max(quantiles), 500)\n",
    "    Y_ = X_Y_Spline(X_)\n",
    "    plt.plot(X_, Y_, color='#0097e6')\n",
    "    # shade the area under the curve\n",
    "    plt.fill_between(X_, Y_, color='#0097e6', alpha=0.4)\n",
    "    plt.xlabel('Quantile')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs Quantile')\n",
    "    plt.savefig(output_loc + 'f1_quantile.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.errorbar(quantiles, quantile_means_prec, yerr=quantile_stds_prec, fmt='o', color='#130f40')\n",
    "    # draw a smooth curve connecting the points from means\n",
    "    X_Y_Spline = make_interp_spline(quantiles, quantile_means_prec)\n",
    "    # Returns evenly spaced numbers\n",
    "    # over a specified interval.\n",
    "    X_ = np.linspace(min(quantiles), max(quantiles), 500)\n",
    "    Y_ = X_Y_Spline(X_)\n",
    "    plt.plot(X_, Y_, color='#0097e6')\n",
    "    # shade the area under the curve\n",
    "    plt.fill_between(X_, Y_, color='#0097e6', alpha=0.4)\n",
    "    plt.xlabel('Quantile')\n",
    "    plt.ylabel('Precision Score')\n",
    "    plt.title('Precision Score vs Quantile')\n",
    "    plt.savefig(output_loc + 'prec_quantile.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.errorbar(quantiles, quantile_means_recall, yerr=quantile_stds_recall, fmt='o', color='#130f40')\n",
    "    # draw a smooth curve connecting the points from means\n",
    "    X_Y_Spline = make_interp_spline(quantiles, quantile_means_recall)\n",
    "    # Returns evenly spaced numbers\n",
    "    # over a specified interval.\n",
    "    X_ = np.linspace(min(quantiles), max(quantiles), 500)\n",
    "    Y_ = X_Y_Spline(X_)\n",
    "    plt.plot(X_, Y_, color='#0097e6')\n",
    "    # shade the area under the curve\n",
    "    plt.fill_between(X_, Y_, color='#0097e6', alpha=0.4)\n",
    "    plt.xlabel('Quantile')\n",
    "    plt.ylabel('Recall Score')\n",
    "    plt.title('Recall Score vs Quantile')\n",
    "    plt.savefig(output_loc + 'recall_quantile.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # using quantile means get the area under the curve\n",
    "    auc_f1 = np.trapz(quantile_means_f1, dx=0.1)\n",
    "    auc_prec = np.trapz(quantile_means_prec, dx=0.1)\n",
    "    auc_recall = np.trapz(quantile_means_recall, dx=0.1)\n",
    "    print(\"AUC F1 Score: \", auc_f1)\n",
    "    print(\"AUC Precision Score: \", auc_prec)\n",
    "    print(\"AUC Recall Score: \", auc_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention maps interpretability functions\n",
    "def attention_maps(model, test_dataset, output_loc, cond_attr):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    list_attn_matrices = []\n",
    "    max_len = 0\n",
    "    lens_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y, ctrl_y, gene, transcript) in enumerate(test_dataset):\n",
    "            # number of zero elements in y\n",
    "            # get condition from x \n",
    "            condition = inverse_condition_values[(x // 64)[0].item()]\n",
    "\n",
    "            if condition == cond_attr or cond_attr == 'ALL':\n",
    "                y_zero = np.nan_to_num(y)\n",
    "                num_zeros = len(y) - np.count_nonzero(y_zero)\n",
    "\n",
    "                lengths = torch.tensor([len(x)])\n",
    "\n",
    "                x = torch.tensor(x).unsqueeze(0)\n",
    "                y = torch.tensor(y).unsqueeze(0)\n",
    "                ctrl_y = torch.tensor(ctrl_y).unsqueeze(0)\n",
    "                \n",
    "                x = pad_sequence(x, batch_first=True, padding_value=192) \n",
    "                y = pad_sequence(y, batch_first=True, padding_value=-1)\n",
    "                ctrl_y = pad_sequence(ctrl_y, batch_first=True, padding_value=-1)\n",
    "\n",
    "                out_batch = {}\n",
    "\n",
    "                out_batch[\"input_ids\"] = x\n",
    "                out_batch[\"labels\"] = y\n",
    "                out_batch[\"lengths\"] = lengths\n",
    "                out_batch[\"labels_ctrl\"] = ctrl_y\n",
    "\n",
    "                # send batch to device\n",
    "                for k, v in out_batch.items():\n",
    "                    out_batch[k] = v.to(device)\n",
    "\n",
    "                out = model(out_batch[\"input_ids\"], output_attentions = True, return_dict = True)\n",
    "                attn_vec1 = out.attentions[0].cpu().detach().numpy()\n",
    "                attn_vec2 = out.attentions[1].cpu().detach().numpy()\n",
    "                attn_vec3 = out.attentions[2].cpu().detach().numpy()\n",
    "\n",
    "                attn_vec_full = attn_vec1 # only first layer because this is the only one that looks at the input\n",
    "\n",
    "                # remove dim 0\n",
    "                attn_vec_full = np.squeeze(attn_vec_full, axis=0)\n",
    "\n",
    "                # average across heads\n",
    "                attn_vec_full = np.mean(attn_vec_full, axis=0)\n",
    "\n",
    "                list_attn_matrices.append(attn_vec_full)\n",
    "\n",
    "                if len(attn_vec_full) > max_len:\n",
    "                    max_len = len(attn_vec_full)\n",
    "                \n",
    "                lens_list.append(len(attn_vec_full))\n",
    "\n",
    "    mean_len = int(np.mean(lens_list))\n",
    "    # make matrix of shape max_len x max_len\n",
    "    attn_matrix = np.zeros((max_len, max_len))\n",
    "    # add all attention matrices to the matrix\n",
    "    count_lengths = np.zeros(max_len)\n",
    "    for i in range(len(list_attn_matrices)):\n",
    "        count_lengths[:len(list_attn_matrices[i])] += 1\n",
    "\n",
    "    for i in range(len(list_attn_matrices)):\n",
    "        attn_matrix[:len(list_attn_matrices[i]), :len(list_attn_matrices[i])] += (list_attn_matrices[i] / count_lengths[:len(list_attn_matrices[i])])\n",
    "\n",
    "    # crop the matrix to the mean length\n",
    "    attn_matrix = attn_matrix[:mean_len, :mean_len]\n",
    "\n",
    "    # plot the attention matrix\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(attn_matrix)\n",
    "    plt.savefig(output_loc + 'attention_map_' + str(cond_attr) + '.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # make dataframe position_to_Asite and the attention weight for that position\n",
    "    pos_A_site_df = []\n",
    "    attn_weight_df = []\n",
    "    for i in range(len(attn_matrix)):\n",
    "        for j in range(len(attn_matrix)):\n",
    "            pos_A_site_df.append(j-i)\n",
    "            attn_weight_df.append(attn_matrix[i][j])\n",
    "\n",
    "    # make dataframe\n",
    "    df_attn_weights = pd.DataFrame({'pos_A_site': pos_A_site_df, 'attn_weight': attn_weight_df})\n",
    "\n",
    "    # make a lineplot from the dataframe using seaborn\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.lineplot(x='pos_A_site', y='attn_weight', data=df_attn_weights, errorbar=\"sd\")\n",
    "    plt.savefig(output_loc + 'amaps_crop_lineplot_sns_xlimfull_' + str(cond_attr) + '.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # make a lineplot from the dataframe using seaborn\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.lineplot(x='pos_A_site', y='attn_weight', data=df_attn_weights, errorbar=\"sd\")\n",
    "    # set x scale to be from -20 to 20\n",
    "    plt.xlim(-20, 20)\n",
    "    plt.savefig(output_loc + 'amaps_crop_lineplot_sns_xlim20_' + str(cond_attr) + '.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # log transform the attention maps\n",
    "    attention_maps_logged = np.log(attn_matrix)\n",
    "\n",
    "    # plot the log transformed attention matrix\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(attention_maps_logged)\n",
    "    plt.savefig(output_loc + 'attention_map_logged_' + str(cond_attr) + '.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # make dataframe position_to_Asite and the attention weight for that position\n",
    "    pos_A_site_df_log = []\n",
    "    attn_weight_df_log = []\n",
    "    for i in range(len(attention_maps_logged)):\n",
    "        for j in range(len(attention_maps_logged)):\n",
    "            pos_A_site_df_log.append(j-i)\n",
    "            attn_weight_df_log.append(attention_maps_logged[i][j])\n",
    "\n",
    "    # make dataframe\n",
    "    df_attn_weights_log = pd.DataFrame({'pos_A_site': pos_A_site_df_log, 'attn_weight': attn_weight_df_log})\n",
    "\n",
    "     # make a lineplot from the dataframe using seaborn\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.lineplot(x='pos_A_site', y='attn_weight', data=df_attn_weights_log, errorbar=\"sd\")\n",
    "    plt.savefig(output_loc + 'amaps_crop_logged_lineplot_sns_xlimfull_' + str(cond_attr) + '.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # make a lineplot from the dataframe using seaborn\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.lineplot(x='pos_A_site', y='attn_weight', data=df_attn_weights_log, errorbar=\"sd\")\n",
    "    plt.xlim(-20, 20)\n",
    "    plt.savefig(output_loc + 'amaps_crop_logged_lineplot_sns_xlim20_' + str(cond_attr) + '.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # save the attention matrix\n",
    "    np.save(output_loc + 'attention_map_' + str(cond_attr) + '.npy', attn_matrix)\n",
    "\n",
    "    # check how many elements next to the principal diagonal have high values on average\n",
    "    left_diag = []\n",
    "    right_diag = []\n",
    "\n",
    "    threshold_val = 3\n",
    "\n",
    "    attention_maps_logged_thresholded = [] \n",
    "\n",
    "    queue_ribo_dist_start = []\n",
    "    queue_ribo_dist_end = []\n",
    "\n",
    "    for i in range(attention_maps_logged.shape[0]):\n",
    "        # A site value\n",
    "        max_val_i = np.max(attention_maps_logged[i])\n",
    "        max_val_i_index = np.argmax(attention_maps_logged[i])\n",
    "\n",
    "        # left elements\n",
    "        left_i = attention_maps_logged[i, :i]\n",
    "        # number of elements with values between max_val_i and max_val_i - threshold_val\n",
    "        left_i_threshold = left_i > (max_val_i - threshold_val)\n",
    "        # sum of non zero elements\n",
    "        left_diag.append(np.sum(left_i_threshold))\n",
    "\n",
    "        # right elements\n",
    "        right_i = attention_maps_logged[i, i+1:]\n",
    "        # number of elements with values between max_val_i and max_val_i - threshold_val\n",
    "        right_i_threshold = right_i > (max_val_i - threshold_val)\n",
    "        right_diag.append(np.sum(right_i_threshold))\n",
    "\n",
    "        # row of the thresholded attention map \n",
    "        row = np.concatenate((left_i_threshold, [1], right_i_threshold))\n",
    "        attention_maps_logged_thresholded.append(row)\n",
    "\n",
    "        # find the distance of the queueing ribosome using the row vector of the thresholded attention map\n",
    "        # find the index of the element with value 1 behind i, such that it is after the small block of 0 \n",
    "        queue_ribo_dist_start.append(i - np.argmax(row))\n",
    "\n",
    "    # print means of left and right diagonals\n",
    "    print(\"Number of important codons to the left (avg): \", np.mean(left_diag))\n",
    "    print(\"Number of important codons to the right (avg): \", np.mean(right_diag))\n",
    "    # get the mode of the queueing ribosome distances\n",
    "    print(\"Average distance of the queued ribosomes from the A site: \", stats.mode(queue_ribo_dist_start)[0], \" codons, \", \" (\", (stats.mode(queue_ribo_dist_start)[1] / len(queue_ribo_dist_start)) * 100, \" percent of the times)\")\n",
    "\n",
    "    attention_maps_logged_thresholded = np.array(attention_maps_logged_thresholded)\n",
    "\n",
    "    # plot the log transformed attention matrix\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(attention_maps_logged_thresholded)\n",
    "    plt.savefig(output_loc + 'attention_map_logged_Thresh3_' + str(cond_attr) + '.png')  \n",
    "    plt.show()\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def captum_LayerGradAct(model, test_dataset, output_loc):\n",
    "    model_fin = model_finalexp(model)\n",
    "        \n",
    "    lig = LayerGradientXActivation(model_fin, model_fin.model.transformer.word_embedding)\n",
    "\n",
    "    attributions_total = []\n",
    "    lens_list = []\n",
    "    indices_main_list = []\n",
    "\n",
    "    # set torch graph to allow unused tensors\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for i, (x, y, _, _, _) in enumerate(test_dataset):\n",
    "            x = torch.tensor(x).unsqueeze(0)\n",
    "            \n",
    "            x = pad_sequence(x, batch_first=True, padding_value=192) \n",
    "\n",
    "            out_batch = {}\n",
    "\n",
    "            out_batch[\"input_ids\"] = x\n",
    "            \n",
    "            out_batch[\"input_ids\"] = torch.tensor(out_batch[\"input_ids\"]).to(device).to(torch.int32)\n",
    "\n",
    "            # get indices of top 95% of values in y \n",
    "            quantile_95 = np.nanquantile(y, 0.95)\n",
    "            indices = np.where(y > quantile_95)[0]\n",
    "\n",
    "            # indices append\n",
    "            for k in indices:\n",
    "                indices_main_list.append(k)\n",
    "            \n",
    "            # make len(x[0]) x len(x[0]) matrix\n",
    "            len_sample = len(x[0])\n",
    "            attributions_sample = np.zeros((len_sample, len_sample))\n",
    "\n",
    "            for j in indices:\n",
    "                index_val = j\n",
    "\n",
    "                index_val = torch.tensor(index_val).to(device)\n",
    "\n",
    "                attributions = lig.attribute((out_batch[\"input_ids\"]), additional_forward_args=index_val)\n",
    "                attributions = attributions.squeeze(1)\n",
    "                attributions = torch.sum(attributions, dim=1)\n",
    "                attributions = attributions / torch.norm(attributions)\n",
    "                attributions = attributions.detach().cpu().numpy()\n",
    "                attributions_sample[j] = attributions\n",
    "            \n",
    "            attributions_sample = np.array(attributions_sample)\n",
    "            print(i, attributions_sample.shape)\n",
    "            attributions_total.append(attributions_sample)\n",
    "            lens_list.append(len_sample)\n",
    "\n",
    "    # save all the attributions\n",
    "    model_name = output_loc.split('/')[-4]\n",
    "    attr_save_loc = '/net/lts2gdk0/mnt/scratch/lts2/nallapar/rb-prof/data/Darnell_Full/Captum/' + model_name + '/'\n",
    "    # make directory if it doesn't exist\n",
    "    if not os.path.exists(attr_save_loc):\n",
    "        os.makedirs(attr_save_loc)\n",
    "\n",
    "    np.save(attr_save_loc + 'ALL_Attributions_LayerGradxAct.npy', attributions_total)\n",
    "\n",
    "    max_len = max(lens_list)\n",
    "    # make matrix of shape max_len x max_len\n",
    "    attr_matrix = np.zeros((max_len, max_len))\n",
    "    # add all attention matrices to the matrix\n",
    "    for i in range(len(attributions_total)):\n",
    "        attr_matrix[:len(attributions_total[i]), :len(attributions_total[i])] += attributions_total[i]\n",
    "\n",
    "    # div each index by the count of the index in the indices_main_list\n",
    "    for i in range(len(attr_matrix)):\n",
    "        count_i = np.count_nonzero(indices_main_list == i)\n",
    "        if count_i != 0:\n",
    "            attr_matrix[i] /= count_i\n",
    "\n",
    "    # trim the matrix to the mean of the lengths\n",
    "    attr_matrix = attr_matrix[:int(np.mean(lens_list)), :int(np.mean(lens_list))]\n",
    "\n",
    "    # save the matrix\n",
    "    np.save(output_loc + \"LGAct_matrix.npy\", attr_matrix)\n",
    "\n",
    "    # plot the matrix\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(attr_matrix)\n",
    "    plt.savefig(output_loc + 'LGXAct_Sum.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # make dataframe position_to_Asite and the attention weight for that position\n",
    "    pos_A_site_df = []\n",
    "    attr_weight_df = []\n",
    "    for i in range(len(attr_matrix)):\n",
    "        for j in range(len(attr_matrix)):\n",
    "            pos_A_site_df.append(j-i)\n",
    "            attr_weight_df.append(attr_matrix[i][j])\n",
    "\n",
    "    # make dataframe\n",
    "    df_attn_weights = pd.DataFrame({'pos_A_site': pos_A_site_df, 'attn_weight': attr_weight_df})\n",
    "\n",
    "    # make a lineplot from the dataframe using seaborn\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.lineplot(x='pos_A_site', y='attn_weight', data=df_attn_weights, errorbar=\"sd\")\n",
    "    plt.savefig(output_loc + 'LGXAct_crop_lineplot_sns_xlimfull.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # make a lineplot from the dataframe using seaborn\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.lineplot(x='pos_A_site', y='attn_weight', data=df_attn_weights, errorbar=\"sd\")\n",
    "    # set x scale to be from -20 to 20\n",
    "    plt.xlim(-20, 20)\n",
    "    plt.savefig(output_loc + 'LGXAct_crop_lineplot_sns_xlim20.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # log the matrix \n",
    "    attr_matrix_logged = np.log10(attr_matrix)\n",
    "\n",
    "    # set nans to -10\n",
    "    attr_matrix_logged[np.isnan(attr_matrix_logged)] = -7\n",
    "\n",
    "    # # plot the logged matrix\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(attr_matrix_logged)\n",
    "    plt.savefig(output_loc + 'LGXAct_Sum_Logged.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # make dataframe position_to_Asite and the attention weight for that position\n",
    "    pos_A_site_df_log = []\n",
    "    attr_weight_df_log = []\n",
    "    for i in range(len(attr_matrix_logged)):\n",
    "        for j in range(len(attr_matrix_logged)):\n",
    "            pos_A_site_df_log.append(j-i)\n",
    "            attr_weight_df_log.append(attr_matrix_logged[i][j])\n",
    "\n",
    "    # make dataframe\n",
    "    df_attn_weights_log = pd.DataFrame({'pos_A_site': pos_A_site_df_log, 'attn_weight': attr_weight_df_log})\n",
    "\n",
    "     # make a lineplot from the dataframe using seaborn\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.lineplot(x='pos_A_site', y='attn_weight', data=df_attn_weights_log, errorbar=\"sd\")\n",
    "    plt.savefig(output_loc + 'LGXAct_crop_logged_lineplot_sns_xlimfull.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # make a lineplot from the dataframe using seaborn\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.lineplot(x='pos_A_site', y='attn_weight', data=df_attn_weights_log, errorbar=\"sd\")\n",
    "    plt.xlim(-20, 20)\n",
    "    plt.savefig(output_loc + 'LGXAct_crop_logged_lineplot_sns_xlim20.png')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def captum_LayerGrad(model, test_dataset, output_loc):\n",
    "    model_fin = model_finalexpLIG(model)\n",
    "        \n",
    "    lig = LayerIntegratedGradients(model_fin, model_fin.model.transformer.word_embedding)\n",
    "\n",
    "    attributions_total = []\n",
    "    lens_list = []\n",
    "    indices_main_list = []\n",
    "\n",
    "    # set torch graph to allow unused tensors\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for i, (x, y, _, _, _) in enumerate(test_dataset):\n",
    "            x = torch.tensor(x)\n",
    "            \n",
    "            # x = pad_sequence(x, batch_first=True, padding_value=192) \n",
    "\n",
    "            out_batch = {}\n",
    "\n",
    "            out_batch[\"input_ids\"] = x\n",
    "            \n",
    "            out_batch[\"input_ids\"] = torch.tensor(out_batch[\"input_ids\"]).to(device).to(torch.int32)\n",
    "\n",
    "            baseline_inp = torch.ones(out_batch[\"input_ids\"].shape) * 192\n",
    "            baseline_inp = baseline_inp.to(device).to(torch.int32)\n",
    "\n",
    "            # get indices of top 95% of values in y \n",
    "            quantile_95 = np.nanquantile(y, 0.95)\n",
    "            indices = np.where(y > quantile_95)[0]\n",
    "\n",
    "            # indices append\n",
    "            for k in indices:\n",
    "                indices_main_list.append(k)\n",
    "            \n",
    "            # make len(x[0]) x len(x[0]) matrix\n",
    "            len_sample = len(x)\n",
    "            attributions_sample = np.zeros((len_sample, len_sample))\n",
    "\n",
    "            for j in indices:\n",
    "                index_val = j\n",
    "\n",
    "                index_val = torch.tensor(index_val).to(device)\n",
    "\n",
    "                attributions, approximation_error = lig.attribute((out_batch[\"input_ids\"]), baselines=baseline_inp, \n",
    "                                                        method = 'gausslegendre', return_convergence_delta = True, additional_forward_args=index_val, n_steps=20, internal_batch_size=2048)\n",
    "\n",
    "                \n",
    "                attributions = attributions.squeeze(1)\n",
    "                attributions = torch.sum(attributions, dim=1)\n",
    "                attributions = attributions / torch.norm(attributions)\n",
    "                attributions = attributions.detach().cpu().numpy()\n",
    "                attributions_sample[j] = attributions\n",
    "            \n",
    "            attributions_sample = np.array(attributions_sample)\n",
    "            print(i, attributions_sample.shape)\n",
    "            attributions_total.append(attributions_sample)\n",
    "            lens_list.append(len_sample)\n",
    "\n",
    "    # save all the attributions\n",
    "    model_name = output_loc.split('/')[-4]\n",
    "    attr_save_loc = '/net/lts2gdk0/mnt/scratch/lts2/nallapar/rb-prof/data/Darnell_Full/Captum/' + model_name + '/'\n",
    "    # make directory if it doesn't exist\n",
    "    if not os.path.exists(attr_save_loc):\n",
    "        os.makedirs(attr_save_loc)\n",
    "\n",
    "    np.save(attr_save_loc + 'ALL_Attributions_LIG.npy', attributions_total)\n",
    "\n",
    "    max_len = max(lens_list)\n",
    "    # make matrix of shape max_len x max_len\n",
    "    attr_matrix = np.zeros((max_len, max_len))\n",
    "    # add all attention matrices to the matrix\n",
    "    for i in range(len(attributions_total)):\n",
    "        attr_matrix[:len(attributions_total[i]), :len(attributions_total[i])] += attributions_total[i]\n",
    "\n",
    "    # div each index by the count of the index in the indices_main_list\n",
    "    for i in range(len(attr_matrix)):\n",
    "        count_i = np.count_nonzero(indices_main_list == i)\n",
    "        if count_i != 0:\n",
    "            attr_matrix[i] /= count_i\n",
    "\n",
    "    # trim the matrix to the mean of the lengths\n",
    "    attr_matrix = attr_matrix[:int(np.mean(lens_list)), :int(np.mean(lens_list))]\n",
    "\n",
    "    # save the matrix\n",
    "    np.save(output_loc + \"LIG_matrix.npy\", attr_matrix)\n",
    "\n",
    "    # plot the matrix\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(attr_matrix)\n",
    "    plt.savefig(output_loc + 'LIG_Sum.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # make dataframe position_to_Asite and the attention weight for that position\n",
    "    pos_A_site_df = []\n",
    "    attr_weight_df = []\n",
    "    for i in range(len(attr_matrix)):\n",
    "        for j in range(len(attr_matrix)):\n",
    "            pos_A_site_df.append(j-i)\n",
    "            attr_weight_df.append(attr_matrix[i][j])\n",
    "\n",
    "    # make dataframe\n",
    "    df_attn_weights = pd.DataFrame({'pos_A_site': pos_A_site_df, 'attn_weight': attr_weight_df})\n",
    "\n",
    "    # make a lineplot from the dataframe using seaborn\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.lineplot(x='pos_A_site', y='attn_weight', data=df_attn_weights, errorbar=\"sd\")\n",
    "    plt.savefig(output_loc + 'LIG_crop_lineplot_sns_xlimfull.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # make a lineplot from the dataframe using seaborn\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.lineplot(x='pos_A_site', y='attn_weight', data=df_attn_weights, errorbar=\"sd\")\n",
    "    # set x scale to be from -20 to 20\n",
    "    plt.xlim(-20, 20)\n",
    "    plt.savefig(output_loc + 'LIG_crop_lineplot_sns_xlim20.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # log the matrix \n",
    "    attr_matrix_logged = np.log10(attr_matrix)\n",
    "\n",
    "    # set nans to -10\n",
    "    attr_matrix_logged[np.isnan(attr_matrix_logged)] = -7\n",
    "\n",
    "    # # plot the logged matrix\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(attr_matrix_logged)\n",
    "    plt.savefig(output_loc + 'LIG_Sum_Logged.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # make dataframe position_to_Asite and the attention weight for that position\n",
    "    pos_A_site_df_log = []\n",
    "    attr_weight_df_log = []\n",
    "    for i in range(len(attr_matrix_logged)):\n",
    "        for j in range(len(attr_matrix_logged)):\n",
    "            pos_A_site_df_log.append(j-i)\n",
    "            attr_weight_df_log.append(attr_matrix_logged[i][j])\n",
    "\n",
    "    # make dataframe\n",
    "    df_attn_weights_log = pd.DataFrame({'pos_A_site': pos_A_site_df_log, 'attn_weight': attr_weight_df_log})\n",
    "\n",
    "     # make a lineplot from the dataframe using seaborn\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.lineplot(x='pos_A_site', y='attn_weight', data=df_attn_weights_log, errorbar=\"sd\")\n",
    "    plt.savefig(output_loc + 'LIG_crop_logged_lineplot_sns_xlimfull.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # make a lineplot from the dataframe using seaborn\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.lineplot(x='pos_A_site', y='attn_weight', data=df_attn_weights_log, errorbar=\"sd\")\n",
    "    plt.xlim(-20, 20)\n",
    "    plt.savefig(output_loc + 'LIG_crop_logged_lineplot_sns_xlim20.png')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretability_panels(model, preds, labels, inputs, output_loc, test_data_path):\n",
    "    '''\n",
    "    panel for interpretability\n",
    "    1. labels\n",
    "    2. prediction of the model\n",
    "    3. attention maps\n",
    "    4. layer grad x activation\n",
    "    5. layer integrated gradients\n",
    "    '''\n",
    "    # load data\n",
    "    ds = pd.read_csv(test_data_path)\n",
    "\n",
    "    # make masks for all the transcripts\n",
    "    mask = inputs != -100.0\n",
    "\n",
    "    # obtain lengths of all the transcripts\n",
    "    lengths = np.sum(mask, axis=1)\n",
    "\n",
    "    # convert to lists and remove padding\n",
    "    preds = preds.tolist()\n",
    "    preds = [pred[:lengths[i]] for i, pred in enumerate(preds)]\n",
    "\n",
    "    labels = labels.tolist()\n",
    "    labels = [label[:lengths[i]] for i, label in enumerate(labels)]\n",
    "\n",
    "    inputs = inputs.tolist()\n",
    "    inputs = [input[:lengths[i]] for i, input in enumerate(inputs)]\n",
    "\n",
    "    # get the condition for each of the samples\n",
    "    # do / with 64 to get the condition\n",
    "    condition_samples = []\n",
    "    for i in range(len(inputs)):\n",
    "        condition_samples.append(inputs[i][0] // 64)\n",
    "\n",
    "\n",
    "    labels_ctrl = []\n",
    "    genes = []\n",
    "    transcripts = []\n",
    "    sequences_ds = []\n",
    "\n",
    "    sequence_list = list(ds['sequence'])\n",
    "    ctrl_sequence_list_ds = list(ds['ctrl_sequence'])\n",
    "    genes_list_ds = list(ds['gene'])\n",
    "    transcripts_list_ds = list(ds['transcript'])\n",
    "    condition_list = list(ds['condition'])\n",
    "    codon_sequences = []\n",
    "\n",
    "    # for loop which takes the original transcript one-hot sequence and converts into the (64*(condition_one-hot) + x) version (stored in sequences_ds) \n",
    "    for i in range(len(sequence_list)):\n",
    "        x = sequence_list[i][1:-1].split(', ')\n",
    "        x = [int(i) for i in x]\n",
    "        cond_val = condition_values[condition_list[i]]\n",
    "        # get codon sequence from x\n",
    "        codon_seq = [id_to_codon[i] for i in x]\n",
    "        # convert to string\n",
    "        codon_seq = ''.join(codon_seq)\n",
    "        codon_sequences.append(codon_seq)\n",
    "        # get the remainder\n",
    "        add_val = (cond_val) * 64\n",
    "        x = [i + add_val for i in x]\n",
    "        sequences_ds.append(x)\n",
    "\n",
    "    # for loop to get the control label for all the transcripts\n",
    "    for i in range(len(inputs)):\n",
    "        condition_sample = inverse_condition_values[condition_samples[i]]\n",
    "        # search for inputs[i] in sequences_ds get index\n",
    "        for j in range(len(sequences_ds)):\n",
    "            if sequences_ds[j] == inputs[i] and condition_sample == condition_list[j]:\n",
    "                index = j\n",
    "                break\n",
    "\n",
    "        ctrl_sample = ctrl_sequence_list_ds[index]\n",
    "        ctrl_sample = ctrl_sample[1:-1].split(', ')\n",
    "        ctrl_sample = [float(k) for k in ctrl_sample]\n",
    "        labels_ctrl.append(ctrl_sample)\n",
    "        genes.append(genes_list_ds[index])\n",
    "        transcripts.append(transcripts_list_ds[index])\n",
    "\n",
    "    # Model output: the first dim of pred is ctrl, second is depr difference\n",
    "    # process ctrl predictions\n",
    "    ctrl_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_sample = preds[i]\n",
    "        pred_sample = np.asarray(pred_sample)\n",
    "        # get first dim\n",
    "        pred_sample = pred_sample[:, 0]\n",
    "        ctrl_preds.append(pred_sample)\n",
    "\n",
    "    # process depr difference predictions\n",
    "    depr_diffs = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_sample = preds[i]\n",
    "        pred_sample = np.asarray(pred_sample)\n",
    "        # get second dim\n",
    "        pred_sample = pred_sample[:, 1]\n",
    "        depr_diffs.append(pred_sample)\n",
    "\n",
    "    # obtain the full predictions: the summation of the ctrl and depr difference predictions\n",
    "    full_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        full_preds.append(ctrl_preds[i] + depr_diffs[i])\n",
    "\n",
    "    # np log 1 + x the labels\n",
    "    labels_ctrl = [np.log1p(label) for label in labels_ctrl]\n",
    "\n",
    "    # labels depr difference\n",
    "    labels_depr_diff = []\n",
    "    for i in range(len(labels)):\n",
    "        labels_depr_diff.append(np.asarray(labels[i]) - np.asarray(labels_ctrl[i]))\n",
    "\n",
    "    # plot ten best samples\n",
    "    # get pearson corr for each sample\n",
    "    pearson_corrs = []\n",
    "    for i in range(len(full_preds)):\n",
    "        pearson_corrs.append(pearson_mask(full_preds[i], labels[i]))\n",
    "\n",
    "    pearson_corrs_ctrl = []\n",
    "    for i in range(len(ctrl_preds)):\n",
    "        pearson_corrs_ctrl.append(pearson_mask(ctrl_preds[i], labels_ctrl[i]))\n",
    "\n",
    "    print(\"#\"*20)\n",
    "\n",
    "    num_plots = 10\n",
    "\n",
    "    ######### \n",
    "    # plot (num_plots) best samples\n",
    "    #########\n",
    "    best_samples = sorted(range(len(pearson_corrs)), key = lambda sub: pearson_corrs[sub])[-num_plots:]\n",
    "    # print best pearson corrs\n",
    "    print(\"List of best pearson correlations: \", [pearson_corrs[i] for i in best_samples])\n",
    "\n",
    "    print(\"#\"*20)\n",
    "\n",
    "    for i in range(num_plots):\n",
    "        out_loc = output_loc + \"/interpretability_panels/sample_\" + str(best_samples[i]) + '_' + str(inverse_condition_values[condition_samples[best_samples[i]]]) + \"_best_\" + transcripts[best_samples[i]] + \"_\" + genes[best_samples[i]] + \".png\"\n",
    "        interpretability_plot(model, inputs[best_samples[i]], labels[best_samples[i]], labels_ctrl[best_samples[i]], full_preds[best_samples[i]], out_loc)\n",
    "\n",
    "    ######### \n",
    "    # plot (num_plots) worst samples\n",
    "    #########\n",
    "    worst_samples = sorted(range(len(pearson_corrs)), key = lambda sub: pearson_corrs[sub])[:num_plots]\n",
    "    \n",
    "    # print worst pearson corrs\n",
    "    print(\"Worst Pearson Correlations: \", [pearson_corrs[i] for i in worst_samples])\n",
    "\n",
    "    print(\"#\"*20)\n",
    "\n",
    "    for i in range(num_plots):\n",
    "        out_loc = output_loc + \"/interpretability_panels/sample_\" + str(worst_samples[i]) + '_' + str(inverse_condition_values[condition_samples[worst_samples[i]]]) + \"_worst_\" + transcripts[worst_samples[i]] + \"_\" + genes[worst_samples[i]] + \".png\"\n",
    "        interpretability_plot(model, inputs[worst_samples[i]], labels[worst_samples[i]], labels_ctrl[worst_samples[i]], full_preds[worst_samples[i]], out_loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d8cddb8cf669a224cfe7de41be728b42e6d1e4d2fa8033c260d761c14134291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
