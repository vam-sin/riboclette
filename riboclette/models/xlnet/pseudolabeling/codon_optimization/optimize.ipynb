{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import torch\n",
    "from transformers import XLNetConfig, XLNetForTokenClassification\n",
    "import pytorch_lightning as pl\n",
    "import itertools\n",
    "import hyperopt\n",
    "import pygad\n",
    "import matplotlib.pyplot as plt\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "seed_val = 4\n",
    "pl.seed_everything(seed_val)\n",
    "\n",
    "# one-hot encoding for the conditions\n",
    "condition_values = {'CTRL': 64, 'ILE': 65, 'LEU': 66, 'LEU_ILE': 67, 'LEU_ILE_VAL': 68, 'VAL': 69}\n",
    "inverse_condition_values = {64: 'CTRL', 65: 'ILE', 66: 'LEU', 67: 'LEU_ILE', 68: 'LEU_ILE_VAL', 69: 'VAL'}\n",
    "\n",
    "codon_to_aa = {\n",
    "        'ATA':'I', 'ATC':'I', 'ATT':'I', 'ATG':'M',\n",
    "        'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n",
    "        'AAC':'N', 'AAT':'N', 'AAA':'K', 'AAG':'K',\n",
    "        'AGC':'S', 'AGT':'S', 'AGA':'R', 'AGG':'R',                \n",
    "        'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n",
    "        'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n",
    "        'CAC':'H', 'CAT':'H', 'CAA':'Q', 'CAG':'Q',\n",
    "        'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n",
    "        'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',\n",
    "        'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n",
    "        'GAC':'D', 'GAT':'D', 'GAA':'E', 'GAG':'E',\n",
    "        'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n",
    "        'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n",
    "        'TTC':'F', 'TTT':'F', 'TTA':'L', 'TTG':'L',\n",
    "        'TAC':'Y', 'TAT':'Y', 'TAA':'_', 'TAG':'_',\n",
    "        'TGC':'C', 'TGT':'C', 'TGA':'_', 'TGG':'W'\n",
    "    }\n",
    "\n",
    "aa_to_codon = {}\n",
    "for codon, aa in codon_to_aa.items():\n",
    "    if aa in aa_to_codon:\n",
    "        aa_to_codon[aa].append(codon)\n",
    "    else:\n",
    "        aa_to_codon[aa] = [codon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "annot_thresh = 0.3\n",
    "longZerosThresh_val = 20\n",
    "percNansThresh_val = 0.05\n",
    "d_model_val = 256\n",
    "n_layers_val = 3\n",
    "n_heads_val = 8\n",
    "dropout_val = 0.1\n",
    "lr_val = 1e-4\n",
    "batch_size_val = 1\n",
    "loss_fun_name = '5L' # 5L\n",
    "\n",
    "# model name and output folder path\n",
    "model_name = 'PLabelXLNetDHConds DS: DeprNA [3, 256, 8] FT: [PEL] BS: 1 Loss: 5L Data Conds: [NZ: 20, PNTh: 0.05, AnnotThresh: 0.3] Seed: 4[Exp: set1, PLQ1: 1, PLQ2: None, ImpDen: impute] Noisy: False'\n",
    "output_loc = 'saved_models/' + model_name \n",
    "\n",
    "condition_dict_values = {64: 'CTRL', 65: 'ILE', 66: 'LEU', 67: 'LEU_ILE', 68: 'LEU_ILE_VAL', 69: 'VAL'}\n",
    "\n",
    "class XLNetDH(XLNetForTokenClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.classifier = torch.nn.Linear(d_model_val, 2, bias=True)\n",
    "\n",
    "config = XLNetConfig(vocab_size=71, pad_token_id=70, d_model = d_model_val, n_layer = n_layers_val, n_head = n_heads_val, d_inner = d_model_val, num_labels = 1, dropout=dropout_val) # 64*6 tokens + 1 for padding\n",
    "model = XLNetDH(config)\n",
    "\n",
    "# load model from the saved model\n",
    "model = model.from_pretrained(output_loc + \"/best_model\")\n",
    "model.to(device)\n",
    "# set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getDensity(codon_list):\n",
    "    codon_list = list(codon_list)\n",
    "    # add 64 in the beginning of each list inside the list\n",
    "    for i in range(len(codon_list)):\n",
    "        new_list = [64]\n",
    "        new_list.extend(codon_list[i])\n",
    "        codon_list[i] = new_list\n",
    "    codon_list = torch.tensor(codon_list, dtype=torch.long).to(device)\n",
    "    pred = model(codon_list)\n",
    "    # remove dim 0\n",
    "    # pred = torch.squeeze(pred[\"logits\"], dim=0)\n",
    "    pred = pred[\"logits\"][:, 1:, :]\n",
    "    pred = torch.sum(pred, dim=2)\n",
    "    sum_value = torch.sum(pred, dim=1)\n",
    "    \n",
    "    return {'loss': sum_value, 'input': codon_list, 'status': hyperopt.STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# original sequence which you wish to mutate (provide this without the stop codon)\n",
    "orig_sequence = 'ATGGAAGACGCCAAAAACATAAAGAAAGGCCCGGCGCCATTCTATCCTCTAGAGGATGGAACCGCTGGAGAGCAACTGCATAAGGCTATGAAGAGATACGCCCTGGTTCCTGGAACAATTGCTTTTACAGATGCACATATCGAGGTGAACATCACGTACGCGGAATACTTCGAAATGTCCGTTCGGTTGGCAGAAGCTATGAAACGATATGGGCTGAATACAAATCACAGAATCGTCGTATGCAGTGAAAACTCTCTTCAATTCTTTATGCCGGTGTTGGGCGCGTTATTTATCGGAGTTGCAGTTGCGCCCGCGAACGACATTTATAATGAACGTGAATTGCTCAACAGTATGAACATTTCGCAGCCTACCGTAGTGTTTGTTTCCAAAAAGGGGTTGCAAAAAATTTTGAACGTGCAAAAAAAATTACCAATAATCCAGAAAATTATTATCATGGATTCTAAAACGGATTACCAGGGATTTCAGTCGATGTACACGTTCGTCACATCTCATCTACCTCCCGGTTTTAATGAATACGATTTTGTACCAGAGTCCTTTGATCGTGACAAAACAATTGCACTGATAATGAATTCCTCTGGATCTACTGGGTTACCTAAGGGTGTGGCCCTTCCGCATAGAACTGCCTGCGTCAGATTCTCGCATGCCAGAGATCCTATTTTTGGCAATCAAATCATTCCGGATACTGCGATTTTAAGTGTTGTTCCATTCCATCACGGTTTTGGAATGTTTACTACACTCGGATATTTGATATGTGGATTTCGAGTCGTCTTAATGTATAGATTTGAAGAAGAGCTGTTTTTACGATCCCTTCAGGATTACAAAATTCAAAGTGCGTTGCTAGTACCAACCCTATTTTCATTCTTCGCCAAAAGCACTCTGATTGACAAATACGATTTATCTAATTTACACGAAATTGCTTCTGGGGGCGCACCTCTTTCGAAAGAAGTCGGGGAAGCGGTTGCAAAACGCTTCCATCTTCCAGGGATACGACAAGGATATGGGCTCACTGAGACTACATCAGCTATTCTGATTACACCCGAGGGGGATGATAAACCGGGCGCGGTCGGTAAAGTTGTTCCATTTTTTGAAGCGAAGGTTGTGGATCTGGATACCGGGAAAACGCTGGGCGTTAATCAGAGAGGCGAATTATGTGTCAGAGGACCTATGATTATGTCCGGTTATGTAAACAATCCGGAAGCGACCAACGCCTTGATTGACAAGGATGGATGGCTACATTCTGGAGACATAGCTTACTGGGACGAAGACGAACACTTCTTCATAGTTGACCGCTTGAAGTCTTTAATTAAATACAAAGGATATCAGGTGGCCCCCGCTGAATTGGAATCGATATTGTTACAACACCCCAACATCTTCGACGCGGGCGTGGCAGGTCTTCCCGACGATGACGCCGGTGAACTTCCCGCCGCCGTTGTTGTTTTGGAGCACGGAAAGACGATGACGGAAAAAGAGATCGTGGATTACGTCGCCAGTCAAGTAACAACCGCGAAAAAGTTGCGCGGAGGAGTTGTGTTTGTGGACGAAGTACCGAAAGGTCTTACCGGAAAACTCGACGCAAGAAAAATCAGAGAGATCCTCATAAAGGCCAAGAAGGGCGGAAAGTCCAAATTG' \n",
    "stop_codon = 'TAA'\n",
    "gene_name = 'luciferase'\n",
    "num_gen = 20\n",
    "num_sequence = 10\n",
    "hyperopt_max_evals = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to uppercase\n",
    "orig_sequence = orig_sequence.upper()\n",
    "stop_codon = stop_codon.upper()\n",
    "\n",
    "# one-hot encoding for the codons\n",
    "id_to_codon = {idx:''.join(el) for idx, el in enumerate(itertools.product(['A', 'T', 'C', 'G'], repeat=3))}\n",
    "codon_to_id = {v:k for k,v in id_to_codon.items()}\n",
    "\n",
    "# convert to codon_int\n",
    "orig_codon_list = []\n",
    "for i in range(0, len(orig_sequence), 3):\n",
    "    codon = orig_sequence[i:i+3]\n",
    "    if len(codon) == 3:\n",
    "        orig_codon_list.append(codon_to_id[codon])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "orig_aa_sequence = ''.join([codon_to_aa[id_to_codon[codon]] for codon in orig_codon_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_pred_densitySum = getDensity([orig_codon_list])\n",
    "print(\"Original Sequence Density: \", orig_pred_densitySum['loss'][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a full space of all the possible codon lists that can give this aa_sequence\n",
    "possibilities_codons = []\n",
    "for i in range(len(orig_aa_sequence)):\n",
    "    aa = orig_aa_sequence[i]\n",
    "    codon_possible = aa_to_codon[aa]\n",
    "    possibilities_codons.append([codon_to_id[codon] for codon in codon_possible])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm Based Gene Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial population that has the original sequence + some random sequences\n",
    "initial_population = []\n",
    "# add the original sequence\n",
    "initial_population.append(orig_codon_list)\n",
    "\n",
    "# make up 19 more sequences which are random permutations of the original sequence but according to the possibilites space\n",
    "for i in range(19):\n",
    "    new_sequence = [possibilities_codons[j][torch.randint(0, len(possibilities_codons[j]), (1,)).item()] for j in range(len(orig_aa_sequence))]\n",
    "    initial_population.append(new_sequence)\n",
    "\n",
    "# genetic algorithm based optimization\n",
    "def fitness_func(ga_instance, solutions, solution_idx): # a fitness function is maximized by a genetic algorithm\n",
    "    pred_densitySum = getDensity(solutions)\n",
    "    fitness_scores = pred_densitySum['loss']\n",
    "    batch_fitness = [1.0 / x.item() for x in fitness_scores]\n",
    "    return batch_fitness\n",
    "\n",
    "last_fitness = 0\n",
    "def on_generation(ga_instance):\n",
    "    global last_fitness\n",
    "    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
    "    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution(pop_fitness=ga_instance.last_generation_fitness)[1]))\n",
    "    print(\"Change     = {change}\".format(change=ga_instance.best_solution(pop_fitness=ga_instance.last_generation_fitness)[1] - last_fitness))\n",
    "    last_fitness = ga_instance.best_solution(pop_fitness=ga_instance.last_generation_fitness)[1]\n",
    "\n",
    "# create an instance of the pygad.GA class\n",
    "ga_instance = pygad.GA(num_generations=num_gen, \n",
    "                    num_parents_mating=10, \n",
    "                    fitness_func=fitness_func, \n",
    "                    sol_per_pop=20, \n",
    "                    num_genes=len(orig_aa_sequence), \n",
    "                    initial_population=initial_population,\n",
    "                    gene_type=int,\n",
    "                    gene_space = possibilities_codons,\n",
    "                    allow_duplicate_genes=False,\n",
    "                    fitness_batch_size=20,\n",
    "                    save_solutions=True,\n",
    "                    on_generation=on_generation)\n",
    "\n",
    "# run the genetic algorithm\n",
    "ga_instance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best solution after running the genetic algorithm\n",
    "solution = ga_instance.best_solution()\n",
    "print(\"Genetic Algorithm Best Solution: \", ''.join([id_to_codon[solution[0][j]] for j in range(len(orig_aa_sequence))]) + stop_codon, \" \\nDensity: \", getDensity([solution[0]])['loss'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot with the distribution on the orig sequence and the best sequence\n",
    "orig_pred = model(torch.tensor([64] + orig_codon_list).long().unsqueeze(0).to(device))\n",
    "orig_pred = torch.sum(torch.squeeze(orig_pred[\"logits\"], dim=0), dim=1)[1:].detach().cpu().numpy()\n",
    "\n",
    "best_pred = model(torch.tensor([64] + solution[0].tolist()).long().unsqueeze(0).to(device))\n",
    "best_pred = torch.sum(torch.squeeze(best_pred[\"logits\"], dim=0), dim=1)[1:].detach().cpu().numpy()\n",
    "\n",
    "# plot the distributions\n",
    "plt.figure()\n",
    "plt.plot(orig_pred, label='Original Sequence')\n",
    "plt.plot(best_pred, label='Best Sequence')\n",
    "plt.legend()\n",
    "plt.title('GA - Density Distribution')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top 5 solutions\n",
    "solutions = ga_instance.population\n",
    "solutions_density = [getDensity([sol])['loss'].item() for sol in solutions]\n",
    "# solutions = sorted(solutions, key=lambda x: x)\n",
    "# sort both solutions and solutions_density according to solutions_density\n",
    "solutions = [x for _, x in sorted(zip(solutions_density, solutions), key=lambda pair: pair[0])]\n",
    "solutions_density = sorted(solutions_density)\n",
    "\n",
    "print(\"Top Solutions: \")\n",
    "for i in range(num_sequence):\n",
    "    print(\"Sequence: \", ''.join([id_to_codon[solutions[i][j]] for j in range(len(orig_aa_sequence))]) + stop_codon, \" \\nDensity: \", solutions_density[i])\n",
    "\n",
    "# make a fasta file out of this\n",
    "f = open('best_solutionsGA_' + gene_name + '_numgen' + str(num_gen) + '.fasta', 'w')\n",
    "\n",
    "for i in range(num_sequence):\n",
    "    f.write(\">Seq\" + str(i+1) + \" density: \" + str(solutions_density[i]) + \"\\n\")\n",
    "    f.write(''.join([id_to_codon[solutions[i][j]] for j in range(len(orig_aa_sequence))]) + stop_codon + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt Based Gene Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # manipulate orig_aa_sequence so that the codon_list has the least densitySum\n",
    "\n",
    "# # sequence of codons with num_codon variables\n",
    "# optimized_sequence = []\n",
    "# for i in range(len(orig_aa_sequence)):\n",
    "#     optimized_sequence.append(hyperopt.hp.choice(f\"codon_{i}\", possibilities_codons[i]))\n",
    "\n",
    "# # define the objective function\n",
    "# def objective(codon_list):\n",
    "#     pred_densitySum = getDensity([codon_list])\n",
    "#     return pred_densitySum\n",
    "\n",
    "# # run the hyperparameter search\n",
    "# trials = hyperopt.Trials()\n",
    "# best = hyperopt.fmin(objective, optimized_sequence, algo=hyperopt.tpe.suggest, max_evals=hyperopt_max_evals, trials=trials)\n",
    "\n",
    "# # print best sequence\n",
    "# best_sequence = [possibilities_codons[i][best[f\"codon_{i}\"]] for i in range(len(orig_aa_sequence))]\n",
    "# print(\"Best Sequence: \", best_sequence, \" \\n Density: \", objective(best_sequence)['loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # best sequence\n",
    "# trials = sorted(trials.results, key=lambda x: x['loss'])\n",
    "# print(\"Best Sequence: \", ''.join([id_to_codon[trials[0]['input'][0][j].item()] for j in range(1, len(orig_aa_sequence))]) + stop_codon, \" \\nDensity: \", trials[0]['loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a plot with the distribution on the orig sequence and the best sequence\n",
    "# orig_pred = model(torch.tensor([64] + orig_codon_list).long().unsqueeze(0).to(device))\n",
    "# orig_pred = torch.sum(torch.squeeze(orig_pred[\"logits\"], dim=0), dim=1)[1:].detach().cpu().numpy()\n",
    "\n",
    "# best_pred = model(torch.tensor(trials[0]['input'][0].tolist()).long().unsqueeze(0).to(device))\n",
    "# best_pred = torch.sum(torch.squeeze(best_pred[\"logits\"], dim=0), dim=1)[1:].detach().cpu().numpy()\n",
    "\n",
    "# # plot the distributions\n",
    "# plt.figure()\n",
    "# plt.plot(orig_pred, label='Original Sequence')\n",
    "# plt.plot(best_pred, label='Best Sequence')\n",
    "# plt.legend()\n",
    "# plt.title('Hyperopt - Density Distribution')\n",
    "# plt.xlabel('Position')\n",
    "# plt.ylabel('Density')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Top Trials: \")\n",
    "# # print top 5 trials w sequence and loss\n",
    "# for i in range(num_sequence):\n",
    "#     print(\"Sequence: \", ''.join([id_to_codon[trials[i]['input'][0][j].item()] for j in range(1, len(orig_aa_sequence))]) + stop_codon, \" \\nDensity: \", trials[i]['loss'])\n",
    "\n",
    "# # make a fasta file out of this\n",
    "# f = open('best_solutionsHopt_' + gene_name + '_evals' + str(hyperopt_max_evals) + '.fasta', 'w')\n",
    "\n",
    "# for i in range(num_sequence):\n",
    "#     f.write(\">Seq\" + str(i+1) + \" density: \" + str(trials[i]['loss']) + \"\\n\")\n",
    "#     f.write(''.join([id_to_codon[trials[i]['input'][0][j].item()] for j in range(1, len(orig_aa_sequence))]) + stop_codon + \"\\n\")\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riboclette",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
