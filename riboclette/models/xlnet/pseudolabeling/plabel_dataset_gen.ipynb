{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from transformers import XLNetConfig, XLNetForTokenClassification\n",
    "from Bio import SeqIO\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions\n",
    "conditions_list = ['CTRL', 'LEU', 'ILE', 'VAL', 'LEU_ILE', 'LEU_ILE_VAL']\n",
    "condition_values = {'CTRL': 64, 'ILE': 65, 'LEU': 66, 'LEU_ILE': 67, 'LEU_ILE_VAL': 68, 'VAL': 69}\n",
    "id_to_codon = {idx:''.join(el) for idx, el in enumerate(itertools.product(['A', 'T', 'C', 'G'], repeat=3))}\n",
    "codon_to_id = {v:k for k,v in id_to_codon.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverageMod(a, window_size=30):\n",
    "    '''\n",
    "    returns the modified coverage function val in the sequence\n",
    "    '''\n",
    "    a = a[1:-1].split(', ')\n",
    "    a = [float(k) for k in a]\n",
    "    for i in range(len(a) - window_size):\n",
    "        if np.all(a[i:i+window_size] == 0.0):\n",
    "            a[i:i+window_size] = np.nan\n",
    "\n",
    "    # num non zero, non nan\n",
    "    num = 0\n",
    "    den = 0\n",
    "    for i in a:\n",
    "        if i != 0.0 and not np.isnan(i):\n",
    "            num += 1\n",
    "        if not np.isnan(i):\n",
    "            den += 1\n",
    "    \n",
    "    return num / den\n",
    "\n",
    "def ntseqtoCodonSeq(seq, condition, add_cond=True):\n",
    "    \"\"\"\n",
    "    Convert nucleotide sequence to codon sequence\n",
    "    \"\"\"\n",
    "    codon_seq = []\n",
    "    # cut seq to remove last codon if not complete\n",
    "    for i in range(0, len(seq), 3):\n",
    "        # check if codon is complete\n",
    "        if len(seq[i:i+3]) == 3:\n",
    "            codon_seq.append(seq[i:i+3])\n",
    "\n",
    "    codon_seq = [codon_to_id[codon] for codon in codon_seq]\n",
    "\n",
    "    if add_cond:\n",
    "        # prepend condition token\n",
    "        codon_seq = [condition_values[condition]] + codon_seq\n",
    "\n",
    "    return codon_seq\n",
    "\n",
    "def sequenceLength(a):\n",
    "    '''\n",
    "    returns the length of the sequence\n",
    "    '''\n",
    "    a = a[1:-1].split(', ')\n",
    "    a = [float(k) for k in a]\n",
    "    return len(a)\n",
    "\n",
    "def mergeAnnotations(annots):\n",
    "    '''\n",
    "    merge the annotations for the same gene\n",
    "    '''\n",
    "    # get the annotations\n",
    "    annots = [a[1:-1].split(', ') for a in annots]\n",
    "    annots = [[float(k) for k in a] for a in annots]\n",
    "\n",
    "    # merge the annotations\n",
    "    merged_annots = []\n",
    "    for i in range(len(annots[0])):\n",
    "        # get the ith annotation for all the transcripts, only non zero and non nan\n",
    "        ith_annots = [a[i] for a in annots if a[i] != 0.0 and not np.isnan(a[i])]\n",
    "        # take the mean of the ith annotation\n",
    "        ith_mean = np.mean(ith_annots)\n",
    "        merged_annots.append(ith_mean)\n",
    "\n",
    "    return merged_annots\n",
    "\n",
    "def uniqueGenes(df):\n",
    "    # add sequence length column\n",
    "    df['sequence_length'] = df['annotations'].apply(sequenceLength)\n",
    "\n",
    "    unique_genes = list(df['gene'].unique())\n",
    "\n",
    "    # iterate through each gene, and choose the longest transcript, for the annotation, merge the annotations\n",
    "    for gene in unique_genes:\n",
    "        # get the df for the gene\n",
    "        df_gene = df[df['gene'] == gene]\n",
    "        if len(df_gene) > 1:\n",
    "            # get the transcript with the longest sequence\n",
    "            df_gene = df_gene.sort_values('sequence_length', ascending=False)\n",
    "            # chosen transcript\n",
    "            chosen_transcript = df_gene['transcript'].values[0]\n",
    "            other_transcripts = df_gene['transcript'].values[1:]\n",
    "            # merge the annotations\n",
    "            annotations = df_gene['annotations'].values\n",
    "            merged_annotations = mergeAnnotations(annotations)\n",
    "            # drop the other transcripts from the df\n",
    "            df = df[~df['transcript'].isin(other_transcripts)]\n",
    "\n",
    "            # change the annotations for the chosen transcript\n",
    "            df.loc[df['transcript'] == chosen_transcript, 'annotations'] = str(merged_annotations)\n",
    "\n",
    "    # drop sequence length column\n",
    "    df = df.drop(columns=['sequence_length'])\n",
    "\n",
    "    assert len(df['gene'].unique()) == len(df['gene'])\n",
    "    assert len(df['transcript'].unique()) == len(df['transcript'])\n",
    "    assert len(df['transcript']) == len(df['gene'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def seqLenMouse(a):\n",
    "    '''\n",
    "    returns the length of the sequence\n",
    "    '''\n",
    "    return len(a)\n",
    "\n",
    "def removeFullGenes(df_mouse, df_full):\n",
    "    '''\n",
    "    remove the genes that are already in df_full\n",
    "    '''\n",
    "    # gene transcript dict\n",
    "    tr_unique_full = list(df_full['transcript'].unique())\n",
    "    transcripts_full_sans_version = [tr.split('.')[0] for tr in tr_unique_full]\n",
    "\n",
    "    df_mouse_tr_sans_version = [tr.split('.')[0] for tr in df_mouse['transcript']]\n",
    "    df_mouse_genes = list(df_mouse['gene'])\n",
    "\n",
    "    mouse_tg_dict = dict(zip(df_mouse_tr_sans_version, df_mouse_genes))\n",
    "\n",
    "    # for each transcript in df_full, remove the gene from df_mouse\n",
    "    for tran in transcripts_full_sans_version:\n",
    "        mouse_gene_for_full_transcript = mouse_tg_dict[tran]\n",
    "        # remove the gene from df_mouse\n",
    "        df_mouse = df_mouse[df_mouse['gene'] != mouse_gene_for_full_transcript]\n",
    "\n",
    "    # get one transcript per gene, choose the longest one\n",
    "    df_mouse['sequence_length'] = df_mouse['sequence'].apply(seqLenMouse)\n",
    "    df_mouse = df_mouse.sort_values('sequence_length', ascending=False).drop_duplicates('gene')\n",
    "    df_mouse = df_mouse.drop(columns=['sequence_length'])\n",
    "\n",
    "    return df_mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "annot_thresh = 0.3\n",
    "longZerosThresh_val = 20\n",
    "percNansThresh_val = 0.05\n",
    "d_model_val = 512\n",
    "n_layers_val = 3\n",
    "n_heads_val = 4\n",
    "dropout_val = 0.1\n",
    "lr_val = 1e-4\n",
    "batch_size_val = 1\n",
    "loss_fun_name = '4L' # 4L, 5L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name and output folder path\n",
    "model_name1 = '../dh/saved_models/XLNet-DH ' + '[NL: ' + str(n_layers_val) + ', NH: ' + str(n_heads_val) + ', D: ' + str(d_model_val) + ', LR: ' + str(lr_val) + ', BS: ' + str(batch_size_val) + ', LF: ' + loss_fun_name + ', Dr: ' + str(dropout_val) + ', S: 1]'\n",
    "model_name2 = '../dh/saved_models/XLNet-DH ' + '[NL: ' + str(n_layers_val) + ', NH: ' + str(n_heads_val) + ', D: ' + str(d_model_val) + ', LR: ' + str(lr_val) + ', BS: ' + str(batch_size_val) + ', LF: ' + loss_fun_name + ', Dr: ' + str(dropout_val) + ', S: 2]'\n",
    "model_name3 = '../dh/saved_models/XLNet-DH ' + '[NL: ' + str(n_layers_val) + ', NH: ' + str(n_heads_val) + ', D: ' + str(d_model_val) + ', LR: ' + str(lr_val) + ', BS: ' + str(batch_size_val) + ', LF: ' + loss_fun_name + ', Dr: ' + str(dropout_val) + ', S: 3]'\n",
    "model_name4 = '../dh/saved_models/XLNet-DH ' + '[NL: ' + str(n_layers_val) + ', NH: ' + str(n_heads_val) + ', D: ' + str(d_model_val) + ', LR: ' + str(lr_val) + ', BS: ' + str(batch_size_val) + ', LF: ' + loss_fun_name + ', Dr: ' + str(dropout_val) + ', S: 4]'\n",
    "model_name42 = '../dh/saved_models/XLNet-DH ' + '[NL: ' + str(n_layers_val) + ', NH: ' + str(n_heads_val) + ', D: ' + str(d_model_val) + ', LR: ' + str(lr_val) + ', BS: ' + str(batch_size_val) + ', LF: ' + loss_fun_name + ', Dr: ' + str(dropout_val) + ', S: 42]'\n",
    "\n",
    "class XLNetDH(XLNetForTokenClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.classifier = torch.nn.Linear(d_model_val, 2, bias=True)\n",
    "\n",
    "config = XLNetConfig(vocab_size=71, pad_token_id=70, d_model = d_model_val, n_layer = n_layers_val, n_head = n_heads_val, d_inner = d_model_val, num_labels = 1, dropout=dropout_val) # 64*6 tokens + 1 for padding\n",
    "model = XLNetDH(config)\n",
    "\n",
    "# load model best weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# load model from the saved model\n",
    "model1 = model.from_pretrained(model_name1 + \"/best_model\")\n",
    "model2 = model.from_pretrained(model_name2 + \"/best_model\")\n",
    "model3 = model.from_pretrained(model_name3 + \"/best_model\")\n",
    "model4 = model.from_pretrained(model_name4 + \"/best_model\")\n",
    "model42 = model.from_pretrained(model_name42 + \"/best_model\")\n",
    "\n",
    "models_list = [model1, model2, model3, model4, model42]\n",
    "\n",
    "for model_chosen in models_list:\n",
    "    model_chosen.to(device)\n",
    "    model_chosen.eval()\n",
    "\n",
    "print(\"Loaded all the models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depr_folder = '/net/lts2gdk0/mnt/scratch/lts2/nallapar/rb-prof/data/Jan_2024/Lina/processed/' # depr data folder\n",
    "\n",
    "ctrl_depr_path = depr_folder + 'CTRL_AA.csv'\n",
    "ile_path = depr_folder + 'ILE_AA.csv'\n",
    "leu_path = depr_folder + 'LEU_AA.csv'\n",
    "val_path = depr_folder + 'VAL_AA.csv'\n",
    "leu_ile_path = depr_folder + 'LEU-ILE_AA_remBadRep.csv'\n",
    "leu_ile_val_path = depr_folder + 'LEU-ILE-VAL_AA.csv'\n",
    "liver_path = depr_folder + 'LIVER.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the control liver data\n",
    "df_liver = pd.read_csv(liver_path)\n",
    "df_liver['condition'] = 'CTRL'\n",
    "\n",
    "# load ctrl_aa data\n",
    "df_ctrl_depr = pd.read_csv(ctrl_depr_path)\n",
    "df_ctrl_depr['condition'] = 'CTRL'\n",
    "\n",
    "# add to the liver data the genes from ctrl depr which are not in liver\n",
    "tr_liver = df_liver['transcript'].unique()\n",
    "tr_ctrl_depr = df_ctrl_depr['transcript'].unique()\n",
    "tr_to_add = [g for g in tr_liver if g not in tr_ctrl_depr]\n",
    "\n",
    "df_liver = df_liver[df_liver['transcript'].isin(tr_to_add)]\n",
    "\n",
    "# df ctrldepr without liver intersection\n",
    "df_ctrldepr_liver = pd.concat([df_liver, df_ctrl_depr], axis=0)\n",
    "\n",
    "# unique genes\n",
    "df_ctrldepr_liver = uniqueGenes(df_ctrldepr_liver)\n",
    "\n",
    "# get ctrl gene, transcript tuple pairs from the df_ctrldepr_liver\n",
    "ctrl_genes_transcripts = list(zip(df_ctrldepr_liver['gene'], df_ctrldepr_liver['transcript']))\n",
    "# make a list of lists\n",
    "ctrl_genes_transcripts = [[gene, transcript] for gene, transcript in ctrl_genes_transcripts]\n",
    "\n",
    "print(\"CTRL Done\")\n",
    "\n",
    "# other conditions\n",
    "df_ile = pd.read_csv(ile_path)\n",
    "df_ile['condition'] = 'ILE'\n",
    "# unique genes\n",
    "df_ile = uniqueGenes(df_ile)\n",
    "# only choose those genes+transcripts that are in ctrl_depr_liver\n",
    "# iterate through the df_ile and choose those genes that are in ctrl_genes_transcripts\n",
    "for index, row in df_ile.iterrows():\n",
    "    if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "        df_ile.drop(index, inplace=True) \n",
    "\n",
    "print(\"ILE Done\")\n",
    "\n",
    "df_leu = pd.read_csv(leu_path)\n",
    "df_leu['condition'] = 'LEU'\n",
    "# unique genes\n",
    "df_leu = uniqueGenes(df_leu)\n",
    "# choose those transcripts that are in ctrl_depr_liver\n",
    "for index, row in df_leu.iterrows():\n",
    "    if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "        df_leu.drop(index, inplace=True)\n",
    "\n",
    "print(\"LEU Done\")\n",
    "\n",
    "df_val = pd.read_csv(val_path)\n",
    "df_val['condition'] = 'VAL'\n",
    "# unique genes\n",
    "df_val = uniqueGenes(df_val)\n",
    "# choose those transcripts that are in ctrl_depr_liver\n",
    "for index, row in df_val.iterrows():\n",
    "    if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "        df_val.drop(index, inplace=True)\n",
    "\n",
    "print(\"VAL Done\")\n",
    "\n",
    "df_leu_ile = pd.read_csv(leu_ile_path)\n",
    "df_leu_ile['condition'] = 'LEU_ILE'\n",
    "# unique genes\n",
    "df_leu_ile = uniqueGenes(df_leu_ile)\n",
    "# choose those transcripts that are in ctrl_depr_liver\n",
    "for index, row in df_leu_ile.iterrows():\n",
    "    if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "        df_leu_ile.drop(index, inplace=True)\n",
    "\n",
    "print(\"LEU_ILE Done\")\n",
    "\n",
    "df_leu_ile_val = pd.read_csv(leu_ile_val_path)\n",
    "df_leu_ile_val['condition'] = 'LEU_ILE_VAL'\n",
    "# unique genes\n",
    "df_leu_ile_val = uniqueGenes(df_leu_ile_val)\n",
    "# choose those transcripts that are in ctrl_depr_liver\n",
    "for index, row in df_leu_ile_val.iterrows():\n",
    "    if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "        df_leu_ile_val.drop(index, inplace=True)\n",
    "\n",
    "print(\"LEU_ILE_VAL Done\")\n",
    "\n",
    "df_full = pd.concat([df_ctrldepr_liver, df_ile, df_leu, df_val, df_leu_ile, df_leu_ile_val], axis=0) # liver + ctrl depr + ile + leu + val + leu ile + leu ile val\n",
    "\n",
    "df_full.columns = ['index_val', 'gene', 'transcript', 'sequence', 'annotations', 'perc_non_zero_annots', 'condition']\n",
    "\n",
    "# drop index_val column\n",
    "df_full = df_full.drop(columns=['index_val'])\n",
    "\n",
    "df_full.to_csv(\"../../data/plabel/tmp/df_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed All Ribo-Seq Lina Data\")\n",
    "\n",
    "print(\"Number of Genes in Full Data: \", len(df_full['gene'].unique()))\n",
    "print(\"Number of Transcripts in Full Data: \", len(df_full['transcript'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the gene, transcript, sequence data \n",
    "df_set1 = df_full[['gene', 'transcript', 'sequence']]\n",
    "\n",
    "# drop duplicates\n",
    "df_set1 = df_set1.drop_duplicates()\n",
    "\n",
    "len_sf_set1 = len(df_set1)\n",
    "\n",
    "# replicate this 6 times, and add condition column\n",
    "df_set1 = pd.concat([df_set1]*6, ignore_index=True)\n",
    "\n",
    "# add condition column\n",
    "cond_col = []\n",
    "for i in range(6):\n",
    "    for j in range(len_sf_set1):\n",
    "        cond_col.append(conditions_list[i])\n",
    "\n",
    "df_set1['condition'] = cond_col\n",
    "\n",
    "print(df_set1)\n",
    "\n",
    "# save this dataframe\n",
    "df_set1.to_csv(\"../../data/plabel/df_set1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mean_preds_list_set1 = []\n",
    "final_stds_preds_list_set1 = []\n",
    "final_conditions_list_set1 = []\n",
    "final_genes_list_set1 = []\n",
    "final_transcripts_list_set1 = []\n",
    "final_sequence_list_set1 = []\n",
    "final_annots_list_set1 = []\n",
    "\n",
    "# load genes file \n",
    "sequences_df_set1 = list(df_set1['sequence'])\n",
    "genes_df_set1 = list(df_set1['gene'])\n",
    "transcripts_df_set1 = list(df_set1['transcript'])\n",
    "conditions_df_set1 = list(df_set1['condition'])\n",
    "\n",
    "# make predictions on all the sequences, using the five models\n",
    "for j in tqdm(range(len(sequences_df_set1))):\n",
    "    X = sequences_df_set1[j]\n",
    "    X = X[1:-1].split(', ')\n",
    "    X = [int(k) for k in X]\n",
    "\n",
    "    # prepend condition token\n",
    "    X = [condition_values[conditions_df_set1[j]]] + X\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    X = torch.from_numpy(X).long()\n",
    "\n",
    "    preds_list_sample = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for model_chosen in models_list:\n",
    "            y_pred = model_chosen(X.unsqueeze(0).to(device).to(torch.int32))\n",
    "            y_pred = torch.sum(y_pred[\"logits\"], dim=2)\n",
    "            y_pred = y_pred.squeeze(0)\n",
    "\n",
    "            # remove first token which is condition token\n",
    "            y_pred = y_pred[1:]\n",
    "\n",
    "            preds_list_sample.append(y_pred.detach().cpu().numpy())\n",
    "\n",
    "    # add preds_list_sample to genes_file \n",
    "    preds_list_sample = np.asarray(preds_list_sample)\n",
    "    # take mean and std of the predictions over each codon\n",
    "    mean_preds = np.mean(preds_list_sample, axis=0)\n",
    "    stds_preds = np.std(preds_list_sample, axis=0)\n",
    "\n",
    "    # print(mean_preds.shape, stds_preds.shape)\n",
    "\n",
    "    # check if this transcript has an annotation in df_full with this condition\n",
    "    df_full_sample = df_full[df_full['condition'] == conditions_df_set1[j]]\n",
    "    df_full_sample = df_full_sample[df_full_sample['transcript'] == transcripts_df_set1[j]]\n",
    "\n",
    "    if len(df_full_sample) > 0:\n",
    "        # substitute the mean_preds with the annotations if they are not nan or 0\n",
    "        annots_sample = df_full_sample['annotations'].values[0]\n",
    "        final_annots_list_set1.append(annots_sample)\n",
    "    else:\n",
    "        final_annots_list_set1.append('NA')\n",
    "\n",
    "    final_mean_preds_list_set1.append(mean_preds)\n",
    "    final_stds_preds_list_set1.append(stds_preds)\n",
    "    final_conditions_list_set1.append(conditions_df_set1[j])\n",
    "    final_genes_list_set1.append(genes_df_set1[j])\n",
    "    final_transcripts_list_set1.append(transcripts_df_set1[j])\n",
    "    final_sequence_list_set1.append(sequences_df_set1[j])\n",
    "\n",
    "# create a dataframe with the final predictions\n",
    "df_final_preds = pd.DataFrame({'gene': final_genes_list_set1, 'transcript': final_transcripts_list_set1, 'sequence': final_sequence_list_set1, 'mean_preds': final_mean_preds_list_set1, 'stds_preds': final_stds_preds_list_set1, 'condition': final_conditions_list_set1, 'annotations': final_annots_list_set1})\n",
    "print(df_final_preds)\n",
    "# save the dataframe\n",
    "df_final_preds.to_pickle(\"../../data/plabel/exp1_preds.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # ensembl cds file process for mouse genome, and remove those transcripts that are already in train and test\n",
    "# # read fasta file\n",
    "fasta_file = \"/net/lts2gdk0/mnt/scratch/lts2/nallapar/rb-prof/data/Jan_2024/Lina/reference/ensembl.cds.fa\"\n",
    "max_codon_len = 2000\n",
    "\n",
    "gene_id_mouse  = []\n",
    "transcript_id_mouse = []\n",
    "sequences_mouse = []\n",
    "\n",
    "for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "    rec = record.description.split(' ')[3]\n",
    "    gene_id = rec.split(':')[1]\n",
    "    transcript_id = record.description.split(' ')[0]\n",
    "\n",
    "    if len(str(record.seq)) <= max_codon_len*3 and len(str(record.seq)) >= 120: # 2000 codons max length for the genes and min 40 codons \n",
    "        gene_id_mouse.append(gene_id)\n",
    "        sequences_mouse.append(str(record.seq))\n",
    "        transcript_id_mouse.append(transcript_id)\n",
    "\n",
    "# create dataframe\n",
    "df_mouse = pd.DataFrame({'gene': gene_id_mouse, 'transcript': transcript_id_mouse, 'sequence': sequences_mouse})\n",
    "\n",
    "# remove those genes that had a 'N' in the sequence\n",
    "df_mouse = df_mouse[~df_mouse['sequence'].str.contains('N')]\n",
    "\n",
    "print(\"Number of Genes in Mouse Data: \", len(df_mouse['gene'].unique()))\n",
    "\n",
    "# # remove the genes that are already in df_full\n",
    "df_mouse = removeFullGenes(df_mouse, df_full)\n",
    "\n",
    "print(\"Number of Genes in Mouse Data After Removing those from DF Full: \", len(df_mouse['gene'].unique()))\n",
    "\n",
    "len_sf_set2 = len(df_mouse)\n",
    "\n",
    "# replicate this 6 times, and add condition column\n",
    "df_set2 = pd.concat([df_mouse]*6, ignore_index=True)\n",
    "\n",
    "# add condition column\n",
    "cond_col = []\n",
    "for i in range(6):\n",
    "    for j in range(len_sf_set2):\n",
    "        cond_col.append(conditions_list[i])\n",
    "\n",
    "df_set2['condition'] = cond_col\n",
    "\n",
    "print(df_set2)\n",
    "\n",
    "# save this dataframe\n",
    "df_set2.to_csv(\"../../data/plabel/df_set2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mean_preds_list_set2 = []\n",
    "final_stds_preds_list_set2 = []\n",
    "final_sequence_list_set2 = []\n",
    "\n",
    "# load genes file \n",
    "sequences_df_set2 = list(df_set2['sequence'])\n",
    "genes_df_set2 = list(df_set2['gene'])\n",
    "transcripts_df_set2 = list(df_set2['transcript'])\n",
    "conditions_df_set2 = list(df_set2['condition'])\n",
    "\n",
    "# make predictions on all the sequences, using the five models\n",
    "for j in tqdm(range(len(sequences_df_set2))):\n",
    "    # process the sequence\n",
    "    X_non_cond = ntseqtoCodonSeq(sequences_df_set2[j], conditions_df_set2[j], add_cond=False)\n",
    "    final_sequence_list_set2.append(X_non_cond)\n",
    "\n",
    "    # process with condition for the model\n",
    "    X = sequences_df_set2[j]\n",
    "    X = ntseqtoCodonSeq(X, conditions_df_set2[j], add_cond=True)\n",
    "    X = np.asarray(X)\n",
    "    X = torch.from_numpy(X).long()\n",
    "\n",
    "    preds_list_sample = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for model_chosen in models_list:\n",
    "            y_pred = model_chosen(X.unsqueeze(0).to(device).to(torch.int32))\n",
    "            y_pred = torch.sum(y_pred[\"logits\"], dim=2)\n",
    "            y_pred = y_pred.squeeze(0)\n",
    "\n",
    "            # remove first token which is condition token\n",
    "            y_pred = y_pred[1:]\n",
    "\n",
    "            preds_list_sample.append(y_pred.detach().cpu().numpy())\n",
    "\n",
    "    # add preds_list_sample to genes_file \n",
    "    preds_list_sample = np.asarray(preds_list_sample)\n",
    "    # take mean and std of the predictions over each codon\n",
    "    mean_preds = np.mean(preds_list_sample, axis=0)\n",
    "    stds_preds = np.std(preds_list_sample, axis=0)\n",
    "\n",
    "    # print(mean_preds.shape, stds_preds.shape)\n",
    "\n",
    "    final_mean_preds_list_set2.append(mean_preds)\n",
    "    final_stds_preds_list_set2.append(stds_preds)\n",
    "\n",
    "# create a dataframe with the final predictions\n",
    "df_final_preds = pd.DataFrame({'gene': genes_df_set2, 'transcript': transcripts_df_set2, 'sequence': final_sequence_list_set2, 'mean_preds': final_mean_preds_list_set2, 'stds_preds': final_stds_preds_list_set2, 'condition': conditions_df_set2})\n",
    "print(df_final_preds)\n",
    "# save the dataframe\n",
    "df_final_preds.to_pickle(\"../../data/plabel/exp2_preds.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
