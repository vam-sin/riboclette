{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import torch\n",
    "from transformers import XLNetConfig, XLNetForTokenClassification, TrainingArguments, EarlyStoppingCallback\n",
    "from xlnet_plabel_utils import RegressionTrainerFour, RiboDatasetExp1, RiboDatasetExp2, GWSDatasetFromPandas, collate_fn, compute_metrics  # custom dataset and trainer\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "import optuna\n",
    "from optuna.study import MaxTrialsCallback\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "annot_thresh = 0.3\n",
    "longZerosThresh_val = 20\n",
    "percNansThresh_val = 0.05\n",
    "loss_fun_name = '4L' # 4L\n",
    "seed_val = 1\n",
    "experiment_type = 'exp2' # exp1, exp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "if experiment_type == 'exp1': # impute all train genes\n",
    "    train_dataset, val_dataset, test_dataset = RiboDatasetExp1(threshold = annot_thresh, longZerosThresh = longZerosThresh_val, percNansThresh = percNansThresh_val)\n",
    "elif experiment_type == 'exp2': # impute train genes + extra mouse genome genes\n",
    "    train_dataset, val_dataset, test_dataset = RiboDatasetExp2(threshold = annot_thresh, longZerosThresh = longZerosThresh_val, percNansThresh = percNansThresh_val)\n",
    "\n",
    "# convert pandas dataframes into torch datasets\n",
    "train_dataset = GWSDatasetFromPandas(train_dataset)\n",
    "val_dataset = GWSDatasetFromPandas(val_dataset)\n",
    "test_dataset = GWSDatasetFromPandas(test_dataset)\n",
    "\n",
    "print(\"samples in train dataset: \", len(train_dataset))\n",
    "print(\"samples in val dataset: \", len(val_dataset))\n",
    "print(\"samples in test dataset: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "pl.seed_everything(seed_val)\n",
    "\n",
    "# dataset paths \n",
    "data_folder = '/net/lts2gdk0/mnt/scratch/lts2/nallapar/rb-prof/data/Jan_2024/Lina/processed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trials):\n",
    "    # hyperparameters\n",
    "    tot_epochs = 10\n",
    "    n_layers_val = trials.suggest_categorical('num_layers', [2, 3, 4, 5, 6])\n",
    "    batch_size_val = trials.suggest_categorical('batch_size', [1, 2, 4])\n",
    "    dropout_val = trials.suggest_categorical('dropout_val', [0.0, 0.1])\n",
    "    lr_val = trials.suggest_categorical('lr', [1e-3, 1e-4, 1e-5])\n",
    "    d_model_val = trials.suggest_categorical('num_nodes', [64, 128, 256, 512])\n",
    "    n_heads_val = trials.suggest_categorical('n_heads_val', [4, 8, 16])\n",
    "\n",
    "    # model name and output folder path\n",
    "    model_name = 'XLNet-PLabelDHOptuna ' + ' Exp: ' + experiment_type + ' [NL: ' + str(n_layers_val) + ', NH: ' + str(n_heads_val) + ', D: ' + str(d_model_val) + ', LR: ' + str(lr_val) + ', BS: ' + str(batch_size_val) + ', LF: ' + loss_fun_name + ', Dr: ' + str(dropout_val) + ', S: ' + str(seed_val) + ']'\n",
    "    output_loc = \"saved_models/\" + model_name\n",
    "\n",
    "    # set wandb name to model_name\n",
    "    wandb.init(project=\"PLabel-Optuna2\", name=model_name)\n",
    "\n",
    "    # load xlnet to train from scratch\n",
    "    config = XLNetConfig(vocab_size=71, pad_token_id=70, d_model = d_model_val, n_layer = n_layers_val, n_head = n_heads_val, d_inner = d_model_val, num_labels = 1, dropout=dropout_val) # 6 conds, 64 codons, 1 for padding\n",
    "    model = XLNetForTokenClassification(config)\n",
    "\n",
    "    # modify the output layer\n",
    "    # model.classifier is a linear layer followed by a softmax layer\n",
    "    model.classifier = torch.nn.Linear(d_model_val, 2, bias=True)\n",
    "\n",
    "    # xlnet training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = output_loc,\n",
    "        learning_rate = lr_val,\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = batch_size_val, # training batch size = per_device_train_batch_size * gradient_accumulation_steps\n",
    "        per_device_eval_batch_size = 1,\n",
    "        eval_accumulation_steps = 4, \n",
    "        num_train_epochs = tot_epochs,\n",
    "        weight_decay = 0.01,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        load_best_model_at_end = True,\n",
    "        push_to_hub = False,\n",
    "        dataloader_pin_memory = True,\n",
    "        save_total_limit = 5,\n",
    "        dataloader_num_workers = 4,\n",
    "        include_inputs_for_metrics = True\n",
    "    )\n",
    "\n",
    "    # initialize trainer\n",
    "    trainer = RegressionTrainerFour(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    trainer.train()\n",
    "\n",
    "    # save best model\n",
    "    trainer.save_model(output_loc + \"/best_model\")\n",
    "\n",
    "    # evaluate model\n",
    "    res = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    mini_perf = 1 - res['eval/r']\n",
    "\n",
    "    return mini_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.load_study(\n",
    "    study_name='xlnet_dh_plabel_exp2', \n",
    "    storage='mysql://optuna_vamsi:RJ-zf8JJFLxwK3ey1WbrRLbHmmz-W_EznNkdDefMsNyLHTehNgXa4TtX92qa1kmI@lts2srv1.epfl.ch/optuna_vamsi'\n",
    "    )\n",
    "study.optimize(\n",
    "    objective,\n",
    "    callbacks=[MaxTrialsCallback(50, states=(TrialState.COMPLETE,))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best results\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
