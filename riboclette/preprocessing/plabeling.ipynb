{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from transformers import XLNetConfig, XLNetForTokenClassification\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "longZerosThresh = 20\n",
    "percNansThresh = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditions\n",
    "conditions_list = ['CTRL', 'LEU', 'ILE', 'VAL', 'LEU_ILE', 'LEU_ILE_VAL']\n",
    "condition_values = {'CTRL': 64, 'ILE': 65, 'LEU': 66, 'LEU_ILE': 67, 'LEU_ILE_VAL': 68, 'VAL': 69}\n",
    "id_to_codon = {idx:''.join(el) for idx, el in enumerate(itertools.product(['A', 'T', 'C', 'G'], repeat=3))}\n",
    "codon_to_id = {v:k for k,v in id_to_codon.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudolabel(ground_truth, mean_preds):\n",
    "    # process ground truth\n",
    "    if ground_truth == 'NA':\n",
    "        # make a list of np.nans\n",
    "        ground_truth = [np.nan for j in range(len(mean_preds))]\n",
    "    else:\n",
    "        ground_truth = [float(k) for k in ground_truth]\n",
    "\n",
    "    annot = []\n",
    "    for j in range(len(mean_preds)):\n",
    "        if (np.isnan(ground_truth[j]) or ground_truth[j] == 0.0):\n",
    "            annot.append(np.abs(mean_preds[j]))\n",
    "        else:\n",
    "            annot.append(ground_truth[j])\n",
    "    \n",
    "    return annot\n",
    "\n",
    "# dataset generation functions\n",
    "def longestZeroSeqLength(a):\n",
    "    '''\n",
    "    length of the longest sub-sequence of zeros\n",
    "    '''\n",
    "    a = [float(k) for k in a]\n",
    "    # longest sequence of zeros\n",
    "    longest = 0\n",
    "    current = 0\n",
    "    for i in a:\n",
    "        if i == 0.0:\n",
    "            current += 1\n",
    "        else:\n",
    "            longest = max(longest, current)\n",
    "            current = 0\n",
    "    longest = max(longest, current)\n",
    "    return longest\n",
    "\n",
    "def percNans(a):\n",
    "    '''\n",
    "    returns the percentage of nans in the sequence\n",
    "    '''\n",
    "    a = [float(k) for k in a]\n",
    "    a = np.asarray(a)\n",
    "    perc = np.count_nonzero(np.isnan(a)) / len(a)\n",
    "\n",
    "    return perc\n",
    "\n",
    "def coverageMod(a, window_size=30):\n",
    "    '''\n",
    "    returns the modified coverage function val in the sequence\n",
    "    '''\n",
    "    a = [float(k) for k in a]\n",
    "    for i in range(len(a) - window_size):\n",
    "        if np.all(a[i:i+window_size] == 0.0):\n",
    "            a[i:i+window_size] = np.nan\n",
    "\n",
    "    # num non zero, non nan\n",
    "    num = 0\n",
    "    den = 0\n",
    "    for i in a:\n",
    "        if i != 0.0 and not np.isnan(i):\n",
    "            num += 1\n",
    "        if not np.isnan(i):\n",
    "            den += 1\n",
    "\n",
    "    if den == 0:\n",
    "        return 0\n",
    "    \n",
    "    return num / den\n",
    "\n",
    "def ntseqtoCodonSeq(seq, condition, add_cond=True):\n",
    "    \"\"\"\n",
    "    Convert nucleotide sequence to codon sequence\n",
    "    \"\"\"\n",
    "    codon_seq = []\n",
    "    # cut seq to remove last codon if not complete\n",
    "    for i in range(0, len(seq), 3):\n",
    "        # check if codon is complete\n",
    "        if len(seq[i:i+3]) == 3:\n",
    "            codon_seq.append(seq[i:i+3])\n",
    "\n",
    "    codon_seq = [codon_to_id[codon] for codon in codon_seq]\n",
    "\n",
    "    if add_cond:\n",
    "        # prepend condition token\n",
    "        codon_seq = [condition_values[condition]] + codon_seq\n",
    "\n",
    "    return codon_seq\n",
    "\n",
    "def sequenceLength(a):\n",
    "    '''\n",
    "    returns the length of the sequence\n",
    "    '''\n",
    "    a = [float(k) for k in a]\n",
    "    return len(a)\n",
    "\n",
    "def mergeAnnotations(annots):\n",
    "    '''\n",
    "    merge the annotations for the same gene\n",
    "    '''\n",
    "    # get the annotations\n",
    "    annots = [a[1:-1].split(', ') for a in annots]\n",
    "    annots = [[float(k) for k in a] for a in annots]\n",
    "\n",
    "    # merge the annotations\n",
    "    merged_annots = []\n",
    "    for i in range(len(annots[0])):\n",
    "        # get the ith annotation for all the transcripts, only non zero and non nan\n",
    "        ith_annots = [a[i] for a in annots if a[i] != 0.0 and not np.isnan(a[i])]\n",
    "        # take the mean of the ith annotation\n",
    "        ith_mean = np.mean(ith_annots)\n",
    "        merged_annots.append(ith_mean)\n",
    "\n",
    "    return merged_annots\n",
    "\n",
    "def uniqueGenes(df):\n",
    "    # add sequence length column\n",
    "    df['sequence_length'] = df['annotations'].apply(sequenceLength)\n",
    "\n",
    "    unique_genes = list(df['gene'].unique())\n",
    "\n",
    "    # iterate through each gene, and choose the longest transcript, for the annotation, merge the annotations\n",
    "    for gene in unique_genes:\n",
    "        # get the df for the gene\n",
    "        df_gene = df[df['gene'] == gene]\n",
    "        if len(df_gene) > 1:\n",
    "            # get the transcript with the longest sequence\n",
    "            df_gene = df_gene.sort_values('sequence_length', ascending=False)\n",
    "            # chosen transcript\n",
    "            chosen_transcript = df_gene['transcript'].values[0]\n",
    "            other_transcripts = df_gene['transcript'].values[1:]\n",
    "            # merge the annotations\n",
    "            annotations = df_gene['annotations'].values\n",
    "            merged_annotations = mergeAnnotations(annotations)\n",
    "            # drop the other transcripts from the df\n",
    "            df = df[~df['transcript'].isin(other_transcripts)]\n",
    "\n",
    "            # change the annotations for the chosen transcript\n",
    "            df.loc[df['transcript'] == chosen_transcript, 'annotations'] = str(merged_annotations)\n",
    "\n",
    "    # drop sequence length column\n",
    "    df = df.drop(columns=['sequence_length'])\n",
    "\n",
    "    assert len(df['gene'].unique()) == len(df['gene'])\n",
    "    assert len(df['transcript'].unique()) == len(df['transcript'])\n",
    "    assert len(df['transcript']) == len(df['gene'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def removeFullGenes(df_mouse, df_full):\n",
    "    '''\n",
    "    remove the genes that are already in df_full\n",
    "    '''\n",
    "    # gene transcript dict\n",
    "    tr_unique_full = list(df_full['transcript'].unique())\n",
    "    transcripts_full_sans_version = [tr.split('.')[0] for tr in tr_unique_full]\n",
    "\n",
    "    df_mouse_tr_sans_version = [tr.split('.')[0] for tr in df_mouse['transcript']]\n",
    "    df_mouse_genes = list(df_mouse['gene'])\n",
    "\n",
    "    mouse_tg_dict = dict(zip(df_mouse_tr_sans_version, df_mouse_genes))\n",
    "\n",
    "    # for each transcript in df_full, remove the gene from df_mouse\n",
    "    for tran in transcripts_full_sans_version:\n",
    "        mouse_gene_for_full_transcript = mouse_tg_dict[tran]\n",
    "        # remove the gene from df_mouse\n",
    "        df_mouse = df_mouse[df_mouse['gene'] != mouse_gene_for_full_transcript]\n",
    "\n",
    "    # get one transcript per gene, choose the longest one\n",
    "    df_mouse['sequence_length'] = df_mouse['sequence'].apply(seqLenMouse)\n",
    "    df_mouse = df_mouse.sort_values('sequence_length', ascending=False).drop_duplicates('gene')\n",
    "    df_mouse = df_mouse.drop(columns=['sequence_length'])\n",
    "\n",
    "    return df_mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "annot_thresh = 0.3\n",
    "longZerosThresh_val = 20\n",
    "percNansThresh_val = 0.05\n",
    "d_model_val = 512\n",
    "n_layers_val = 3\n",
    "n_heads_val = 4\n",
    "dropout_val = 0.1\n",
    "lr_val = 1e-4\n",
    "batch_size_val = 1\n",
    "loss_fun_name = '4L' # 4L, 5L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name and output folder path\n",
    "model_name1 = '../checkpoints/XLNet-DH_S1'\n",
    "model_name2 = '../checkpoints/XLNet-DH_S2'\n",
    "model_name3 = '../checkpoints/XLNet-DH_S3'\n",
    "model_name4 = '../checkpoints/XLNet-DH_S4'\n",
    "model_name42 = '../checkpoints/XLNet-DH_S42'\n",
    "\n",
    "class XLNetDH(XLNetForTokenClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.classifier = torch.nn.Linear(d_model_val, 2, bias=True)\n",
    "\n",
    "config = XLNetConfig(vocab_size=71, pad_token_id=70, d_model = d_model_val, n_layer = n_layers_val, n_head = n_heads_val, d_inner = d_model_val, num_labels = 1, dropout=dropout_val) # 64*6 tokens + 1 for padding\n",
    "model = XLNetDH(config)\n",
    "\n",
    "# load model best weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# load model from the saved model\n",
    "model1 = model.from_pretrained(model_name1 + \"/best_model\")\n",
    "model2 = model.from_pretrained(model_name2 + \"/best_model\")\n",
    "model3 = model.from_pretrained(model_name3 + \"/best_model\")\n",
    "model4 = model.from_pretrained(model_name4 + \"/best_model\")\n",
    "model42 = model.from_pretrained(model_name42 + \"/best_model\")\n",
    "\n",
    "models_list = [model1, model2, model3, model4, model42]\n",
    "\n",
    "for model_chosen in models_list:\n",
    "    model_chosen.to(device)\n",
    "    model_chosen.eval()\n",
    "\n",
    "print(\"Loaded all the models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depr_folder = '../data/processed/' # depr data folder\n",
    "\n",
    "ctrl_depr_path = depr_folder + 'CTRL.csv'\n",
    "ile_path = depr_folder + 'ILE.csv'\n",
    "leu_path = depr_folder + 'LEU.csv'\n",
    "val_path = depr_folder + 'VAL.csv'\n",
    "leu_ile_path = depr_folder + 'LEU_ILE.csv'\n",
    "leu_ile_val_path = depr_folder + 'LEU_ILE_VAL.csv'\n",
    "liver_path = depr_folder + 'LIVER.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the control liver data\n",
    "df_liver = pd.read_csv(liver_path)\n",
    "df_liver['condition'] = 'CTRL'\n",
    "\n",
    "# load ctrl_aa data\n",
    "df_ctrl_depr = pd.read_csv(ctrl_depr_path)\n",
    "df_ctrl_depr['condition'] = 'CTRL'\n",
    "\n",
    "# add to the liver data the genes from ctrl depr which are not in liver\n",
    "tr_liver = df_liver['transcript'].unique()\n",
    "tr_ctrl_depr = df_ctrl_depr['transcript'].unique()\n",
    "tr_to_add = [g for g in tr_liver if g not in tr_ctrl_depr]\n",
    "\n",
    "df_liver = df_liver[df_liver['transcript'].isin(tr_to_add)]\n",
    "\n",
    "# df ctrldepr without liver intersection\n",
    "df_ctrldepr_liver = pd.concat([df_liver, df_ctrl_depr], axis=0)\n",
    "\n",
    "# unique genes\n",
    "df_ctrldepr_liver = uniqueGenes(df_ctrldepr_liver)\n",
    "\n",
    "# get ctrl gene, transcript tuple pairs from the df_ctrldepr_liver\n",
    "ctrl_genes_transcripts = list(zip(df_ctrldepr_liver['gene'], df_ctrldepr_liver['transcript']))\n",
    "# make a list of lists\n",
    "ctrl_genes_transcripts = [[gene, transcript] for gene, transcript in ctrl_genes_transcripts]\n",
    "\n",
    "print(\"CTRL Done\")\n",
    "\n",
    "# other conditions\n",
    "df_ile = pd.read_csv(ile_path)\n",
    "df_ile['condition'] = 'ILE'\n",
    "# unique genes\n",
    "df_ile = uniqueGenes(df_ile)\n",
    "# only choose those genes+transcripts that are in ctrl_depr_liver\n",
    "# iterate through the df_ile and choose those genes that are in ctrl_genes_transcripts\n",
    "for index, row in df_ile.iterrows():\n",
    "    if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "        df_ile.drop(index, inplace=True) \n",
    "\n",
    "print(\"ILE Done\")\n",
    "\n",
    "df_leu = pd.read_csv(leu_path)\n",
    "df_leu['condition'] = 'LEU'\n",
    "# unique genes\n",
    "df_leu = uniqueGenes(df_leu)\n",
    "# choose those transcripts that are in ctrl_depr_liver\n",
    "for index, row in df_leu.iterrows():\n",
    "    if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "        df_leu.drop(index, inplace=True)\n",
    "\n",
    "print(\"LEU Done\")\n",
    "\n",
    "df_val = pd.read_csv(val_path)\n",
    "df_val['condition'] = 'VAL'\n",
    "# unique genes\n",
    "df_val = uniqueGenes(df_val)\n",
    "# choose those transcripts that are in ctrl_depr_liver\n",
    "for index, row in df_val.iterrows():\n",
    "    if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "        df_val.drop(index, inplace=True)\n",
    "\n",
    "print(\"VAL Done\")\n",
    "\n",
    "df_leu_ile = pd.read_csv(leu_ile_path)\n",
    "df_leu_ile['condition'] = 'LEU_ILE'\n",
    "# unique genes\n",
    "df_leu_ile = uniqueGenes(df_leu_ile)\n",
    "# choose those transcripts that are in ctrl_depr_liver\n",
    "for index, row in df_leu_ile.iterrows():\n",
    "    if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "        df_leu_ile.drop(index, inplace=True)\n",
    "\n",
    "print(\"LEU_ILE Done\")\n",
    "\n",
    "df_leu_ile_val = pd.read_csv(leu_ile_val_path)\n",
    "df_leu_ile_val['condition'] = 'LEU_ILE_VAL'\n",
    "# unique genes\n",
    "df_leu_ile_val = uniqueGenes(df_leu_ile_val)\n",
    "# choose those transcripts that are in ctrl_depr_liver\n",
    "for index, row in df_leu_ile_val.iterrows():\n",
    "    if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "        df_leu_ile_val.drop(index, inplace=True)\n",
    "\n",
    "print(\"LEU_ILE_VAL Done\")\n",
    "\n",
    "df_full = pd.concat([df_ctrldepr_liver, df_ile, df_leu, df_val, df_leu_ile, df_leu_ile_val], axis=0) # liver + ctrl depr + ile + leu + val + leu ile + leu ile val\n",
    "\n",
    "df_full.columns = ['index_val', 'gene', 'transcript', 'sequence', 'annotations', 'perc_non_zero_annots', 'condition']\n",
    "\n",
    "# drop index_val column\n",
    "df_full = df_full.drop(columns=['index_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the gene, transcript, sequence data \n",
    "df_set1 = df_full[['gene', 'transcript', 'sequence']]\n",
    "\n",
    "# drop duplicates\n",
    "df_set1 = df_set1.drop_duplicates()\n",
    "\n",
    "len_sf_set1 = len(df_set1)\n",
    "\n",
    "# replicate this 6 times, and add condition column\n",
    "df_set1 = pd.concat([df_set1]*6, ignore_index=True)\n",
    "\n",
    "# add condition column\n",
    "cond_col = []\n",
    "for i in range(6):\n",
    "    for j in range(len_sf_set1):\n",
    "        cond_col.append(conditions_list[i])\n",
    "\n",
    "df_set1['condition'] = cond_col\n",
    "\n",
    "print(df_set1)\n",
    "\n",
    "# save this dataframe\n",
    "df_set1.to_csv(\"../data/plabel/df_set1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mean_preds_list_set1 = []\n",
    "final_stds_preds_list_set1 = []\n",
    "final_conditions_list_set1 = []\n",
    "final_genes_list_set1 = []\n",
    "final_transcripts_list_set1 = []\n",
    "final_sequence_list_set1 = []\n",
    "final_annots_list_set1 = []\n",
    "\n",
    "# load genes file \n",
    "sequences_df_set1 = list(df_set1['sequence'])\n",
    "genes_df_set1 = list(df_set1['gene'])\n",
    "transcripts_df_set1 = list(df_set1['transcript'])\n",
    "conditions_df_set1 = list(df_set1['condition'])\n",
    "\n",
    "# make predictions on all the sequences, using the five models\n",
    "for j in tqdm(range(len(sequences_df_set1))):\n",
    "    X = sequences_df_set1[j]\n",
    "    X = X[1:-1].split(', ')\n",
    "    X = [int(k) for k in X]\n",
    "\n",
    "    # prepend condition token\n",
    "    X = [condition_values[conditions_df_set1[j]]] + X\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    X = torch.from_numpy(X).long()\n",
    "\n",
    "    preds_list_sample = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for model_chosen in models_list:\n",
    "            y_pred = model_chosen(X.unsqueeze(0).to(device).to(torch.int32))\n",
    "            y_pred = torch.sum(y_pred[\"logits\"], dim=2)\n",
    "            y_pred = y_pred.squeeze(0)\n",
    "\n",
    "            # remove first token which is condition token\n",
    "            y_pred = y_pred[1:]\n",
    "\n",
    "            preds_list_sample.append(y_pred.detach().cpu().numpy())\n",
    "\n",
    "    # add preds_list_sample to genes_file \n",
    "    preds_list_sample = np.asarray(preds_list_sample)\n",
    "    # take mean and std of the predictions over each codon\n",
    "    mean_preds = np.mean(preds_list_sample, axis=0)\n",
    "    stds_preds = np.std(preds_list_sample, axis=0)\n",
    "\n",
    "    # print(mean_preds.shape, stds_preds.shape)\n",
    "\n",
    "    # check if this transcript has an annotation in df_full with this condition\n",
    "    df_full_sample = df_full[df_full['condition'] == conditions_df_set1[j]]\n",
    "    df_full_sample = df_full_sample[df_full_sample['transcript'] == transcripts_df_set1[j]]\n",
    "\n",
    "    if len(df_full_sample) > 0:\n",
    "        # substitute the mean_preds with the annotations if they are not nan or 0\n",
    "        annots_sample = df_full_sample['annotations'].values[0]\n",
    "        final_annots_list_set1.append(annots_sample)\n",
    "    else:\n",
    "        final_annots_list_set1.append('NA')\n",
    "\n",
    "    final_mean_preds_list_set1.append(mean_preds)\n",
    "    final_stds_preds_list_set1.append(stds_preds)\n",
    "    final_conditions_list_set1.append(conditions_df_set1[j])\n",
    "    final_genes_list_set1.append(genes_df_set1[j])\n",
    "    final_transcripts_list_set1.append(transcripts_df_set1[j])\n",
    "    final_sequence_list_set1.append(sequences_df_set1[j])\n",
    "\n",
    "# create a dataframe with the final predictions\n",
    "df_final_preds = pd.DataFrame({'gene': final_genes_list_set1, 'transcript': final_transcripts_list_set1, 'sequence': final_sequence_list_set1, 'mean_preds': final_mean_preds_list_set1, 'stds_preds': final_stds_preds_list_set1, 'condition': final_conditions_list_set1, 'annotations': final_annots_list_set1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and testing original sets\n",
    "df_test_orig = pd.read_csv('../data/orig/test.csv')\n",
    "df_val_orig = pd.read_csv('../data/orig/val.csv')\n",
    "\n",
    "orig_test_genes = list(set(list(df_test_orig['gene'])))\n",
    "orig_test_transcripts = list(set(list(df_test_orig['transcript'])))\n",
    "\n",
    "orig_val_genes = list(set(list(df_val_orig['gene'])))\n",
    "orig_val_transcripts = list(set(list(df_val_orig['transcript'])))\n",
    "\n",
    "# remove those for test genes and transcripts\n",
    "df_final_preds = df_final_preds[~df_final_preds['gene'].isin(orig_test_genes)]\n",
    "df_final_preds = df_final_preds[~df_final_preds['transcript'].isin(orig_test_transcripts)]\n",
    "\n",
    "# remove those for val genes and transcripts\n",
    "df_final_preds = df_final_preds[~df_final_preds['gene'].isin(orig_val_genes)]\n",
    "df_final_preds = df_final_preds[~df_final_preds['transcript'].isin(orig_val_transcripts)]\n",
    "\n",
    "annots_imputed = []\n",
    "\n",
    "# go through each of the samples in the df_full \n",
    "# and impute the predictions\n",
    "for i in tqdm(range(len(df_final_preds))):\n",
    "    # get the condition\n",
    "    ground_truth_sample = df_final_preds['annotations'].iloc[i]\n",
    "    mean_preds_sample = df_final_preds['mean_preds'].iloc[i]\n",
    "\n",
    "    pred_sample = pseudolabel(ground_truth_sample, mean_preds_sample)\n",
    "\n",
    "    annots_imputed.append(pred_sample)\n",
    "\n",
    "df_final_preds['annotations'] = annots_imputed\n",
    "\n",
    "# drop those that have NA in the annotations\n",
    "df_final_preds = df_final_preds[df_final_preds['annotations'] != 'NA']\n",
    "\n",
    "# coverage threshold\n",
    "df_final_preds['coverage_mod'] = df_final_preds['annotations'].apply(coverageMod)\n",
    "df_final_preds = df_final_preds[df_final_preds['coverage_mod'] >= threshold]\n",
    "\n",
    "# add the longest zero sequence length to the df\n",
    "df_final_preds['longest_zero_seq_length_annotation'] = df_final_preds['annotations'].apply(longestZeroSeqLength)\n",
    "# add the number of nans to the df\n",
    "df_final_preds['perc_nans_annotation'] = df_final_preds['annotations'].apply(percNans)\n",
    "\n",
    "# apply the threshold for the longest zero sequence length\n",
    "df_final_preds = df_final_preds[df_final_preds['longest_zero_seq_length_annotation'] <= longZerosThresh]\n",
    "\n",
    "# apply the threshold for the number of nans\n",
    "df_final_preds = df_final_preds[df_final_preds['perc_nans_annotation'] <= percNansThresh]\n",
    "\n",
    "print(\"Added Thresholds on all the factors\")\n",
    "\n",
    "# for all the sequences in a condition that is not CTRL, add their respective CTRL sequence to them\n",
    "sequences_ctrl = []\n",
    "annotations_list = list(df_final_preds['annotations'])\n",
    "condition_df_list = list(df_final_preds['condition'])\n",
    "transcripts_list = list(df_final_preds['transcript'])\n",
    "\n",
    "for i in tqdm(range(len(condition_df_list))):\n",
    "    try:\n",
    "        if condition_df_list[i] != 'CTRL':\n",
    "            # find the respective CTRL sequence for the transcript\n",
    "            ctrl_sequence = df_final_preds[(df_final_preds['transcript'] == transcripts_list[i]) & (df_final_preds['condition'] == 'CTRL')]['annotations'].iloc[0]\n",
    "            sequences_ctrl.append(ctrl_sequence)\n",
    "        else:\n",
    "            sequences_ctrl.append(annotations_list[i])\n",
    "    except:\n",
    "        sequences_ctrl.append('NA')\n",
    "\n",
    "# add the sequences_ctrl to the df\n",
    "print(len(sequences_ctrl), len(annotations_list))\n",
    "df_final_preds['ctrl_sequence'] = sequences_ctrl\n",
    "\n",
    "# remove those rows where the ctrl_sequence is NA\n",
    "df_final_preds = df_final_preds[df_final_preds['ctrl_sequence'] != 'NA']\n",
    "\n",
    "# sanity check for the ctrl sequences\n",
    "# get the ds with only condition as CTRL\n",
    "df_ctrl_full = df_final_preds[df_final_preds['condition'] == 'CTRL']\n",
    "ctrl_sequences_san = list(df_ctrl_full['annotations'])\n",
    "ctrl_sequences_san2 = list(df_ctrl_full['ctrl_sequence'])\n",
    "\n",
    "for i in range(len(ctrl_sequences_san)):\n",
    "    assert ctrl_sequences_san[i] == ctrl_sequences_san2[i]\n",
    "\n",
    "print(\"Sanity Checked\")\n",
    "\n",
    "out_train_path = '../data/plabel/plabel_train_trial.csv'\n",
    "df_final_preds.to_csv(out_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
