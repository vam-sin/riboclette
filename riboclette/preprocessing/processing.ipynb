{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import numpy as np\n",
    "from pyhere import here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = here('riboclette/data', 'Lina') # path to the Lina dataset\n",
    "LIVER_FOLDER = here('riboclette/data', 'Liver') # path to the liver dataset\n",
    "fa_path = here('riboclette/data', 'ensembl.cds.fa') # path to the fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id to codon and codon to id\n",
    "id_to_codon = {idx:''.join(el) for idx, el in enumerate(itertools.product(['A', 'T', 'C', 'G'], repeat=3))}\n",
    "codon_to_id = {v:k for k,v in id_to_codon.items()}\n",
    "\n",
    "def make_dataframe(ribo_fname: str, data_path: str, df_trans_to_seq, count_norm: str = \"mean\"):\n",
    "    '''\n",
    "    inputs: path to ribosome data, path to transcript to sequence mapping dataframe\n",
    "    outputs: processed dataframe with one transcript per gene, with the normalized counts\n",
    "    '''\n",
    "    ribo_fpath = os.path.join(data_path, ribo_fname)\n",
    "\n",
    "    # Import dataset with ribosome data\n",
    "    df_ribo = pd.read_csv(\n",
    "        ribo_fpath,\n",
    "        sep=\" \",\n",
    "        on_bad_lines=\"warn\",\n",
    "        dtype=dict(gene=\"category\", transcript=\"category\"),\n",
    "    ).rename(columns={\"count\": \"counts\"})\n",
    "\n",
    "    # Define count normalization function\n",
    "    if count_norm == \"max\":\n",
    "        f_norm = lambda x: x / x.max()\n",
    "    elif count_norm == \"mean\":\n",
    "        f_norm = lambda x: x / x.mean()\n",
    "    elif count_norm == \"sum\":\n",
    "        f_norm = lambda x: x / x.sum()\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "    # Create final dataframe\n",
    "    final_df = (\n",
    "        df_ribo.merge(df_trans_to_seq).assign(fname=ribo_fname)\n",
    "        # Filter spurious positions at the end of the sequence\n",
    "        .query(\"position_A_site <= n_codons * 3\")\n",
    "        # Compute normalized counts\n",
    "        .assign(\n",
    "            norm_counts=lambda df: df.groupby(\"gene\", observed=True).counts.transform(\n",
    "                f_norm\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def make_all_dataframes(data_dirpath: str, fa_fpath: str, max_n_codons: int = 2000, count_norm: str = \"mean\"):\n",
    "    '''\n",
    "    inputs: path to ribosome data, path to ensembl fasta, maximum number of codons in the sequence, count normalization method\n",
    "    outputs: merged dataframe with the ribosome read counts\n",
    "    '''\n",
    "    data = []\n",
    "    with open(fa_fpath, mode=\"r\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            data.append([record.id, str(record.seq)])\n",
    "\n",
    "    # Create transcripts to sequences mapping\n",
    "\n",
    "    df_trans_to_seq = pd.DataFrame(data, columns=[\"transcript\", \"sequence\"])\n",
    "\n",
    "    # Removes those sequences that have Ns\n",
    "    sequence_has_n = df_trans_to_seq.sequence.str.contains(\"N\", regex=False)\n",
    "    df_trans_to_seq = df_trans_to_seq.loc[~sequence_has_n]\n",
    "\n",
    "    # Number of codons in sequence\n",
    "    df_trans_to_seq = df_trans_to_seq.assign(\n",
    "        n_codons=lambda df: df.sequence.str.len() // 3\n",
    "    )\n",
    "\n",
    "    # Compute and merge dataframes\n",
    "    dfs = [\n",
    "        make_dataframe(\n",
    "            f,\n",
    "            df_trans_to_seq=df_trans_to_seq.drop(\"sequence\", axis=1),\n",
    "            data_path=data_dirpath,\n",
    "            count_norm=count_norm,\n",
    "        )\n",
    "        for f in tqdm(os.listdir(data_dirpath))\n",
    "        if not f.startswith(\"ensembl\")\n",
    "    ]\n",
    "    dfs = pd.concat(dfs)\n",
    "    for col in [\"transcript\", \"gene\", \"fname\"]:\n",
    "        dfs[col] = dfs[col].astype(\"category\")\n",
    "\n",
    "    dfs = dfs.groupby([\"transcript\", \"position_A_site\"], observed=True)\n",
    "\n",
    "    # Average replicates\n",
    "    dfs = dfs.agg(dict(norm_counts=\"mean\", gene=\"first\")).reset_index()\n",
    "    \n",
    "    dfs = dfs.assign(codon_idx=lambda df: df.position_A_site // 3)\n",
    "    dfs = dfs.groupby(\"transcript\", observed=True)\n",
    "    dfs = dfs.agg(\n",
    "        {\n",
    "            \"norm_counts\": lambda x: x.tolist(),\n",
    "            \"codon_idx\": lambda x: x.tolist(),\n",
    "            \"gene\": \"first\",\n",
    "        }\n",
    "    ).reset_index()\n",
    "    dfs = dfs.merge(df_trans_to_seq)\n",
    "\n",
    "    dfs = dfs.assign(\n",
    "        n_annot=lambda df: df.norm_counts.transform(lambda x: len(x))\n",
    "        / (df.sequence.str.len() // 3)\n",
    "    )\n",
    "\n",
    "    dfs = dfs.assign(perc_annot=lambda df: df.n_annot / df.n_codons)\n",
    "\n",
    "    # Filter by max sequence lenght\n",
    "    dfs = dfs.query(\"n_codons<@max_n_codons\")\n",
    "\n",
    "    return dfs\n",
    "\n",
    "def sequence2codonids(seq):\n",
    "    '''\n",
    "    converts nt sequence into one-hot codon ids\n",
    "    '''\n",
    "    codon_ids = []\n",
    "    for i in range(0, len(seq), 3):\n",
    "        codon = seq[i:i+3]\n",
    "        if len(codon) == 3:\n",
    "            codon_ids.append(codon_to_id[codon])\n",
    "\n",
    "    return codon_ids\n",
    "\n",
    "def process_merged_df(df):\n",
    "    '''\n",
    "    inputs: merged dataframe with ribosome data\n",
    "    outputs: dataframe with the sequences linked to the ribosome read count annotations, with sequences consisting of N being removed\n",
    "    '''\n",
    "    df = df[df['sequence'].str.contains('N') == False]\n",
    "\n",
    "    codon_seqs = []\n",
    "    sequences = list(df['sequence'])\n",
    "    genes = list(df['gene'])\n",
    "    transcripts = list(df['transcript'])\n",
    "    perc_non_zero_annots = []\n",
    "    norm_counts = list(df['norm_counts'])\n",
    "    codon_idx = list(df[\"codon_idx\"])\n",
    "    annot_seqs = []\n",
    "\n",
    "    for i in tqdm(range(len(sequences))):\n",
    "        seq = sequences[i]\n",
    "        seq = sequence2codonids(seq)\n",
    "        codon_seqs.append(seq)\n",
    "        codon_idx_sample = codon_idx[i]\n",
    "        norm_counts_sample = norm_counts[i]\n",
    "        annot_seq_sample = []\n",
    "        for j in range(len(seq)):\n",
    "            if j in codon_idx_sample:\n",
    "                annot_seq_sample.append(norm_counts_sample[codon_idx_sample.index(j)])\n",
    "            else:\n",
    "                annot_seq_sample.append(0.0)\n",
    "        annot_seqs.append(annot_seq_sample)\n",
    "\n",
    "        # calculate percentage of non-zero annotations\n",
    "        perc_non_zero_annots.append(sum([1 for i in annot_seq_sample if i != 0.0])/len(annot_seq_sample))\n",
    "\n",
    "    final_df = pd.DataFrame(list(zip(genes, transcripts, codon_seqs, annot_seqs, perc_non_zero_annots)), columns = ['gene', 'transcript', 'codon_sequence', 'annotations', 'perc_non_zero_annots'])\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def checkArrayEquality(arr1, arr2):\n",
    "    '''\n",
    "    inputs: two arrays\n",
    "    outputs: True if the arrays are equal, False otherwise\n",
    "    '''\n",
    "    if len(arr1) != len(arr2):\n",
    "        return False\n",
    "    \n",
    "    for i in range(len(arr1)):\n",
    "        if arr1[i] != arr2[i]:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def longestZeroSeqLength(a):\n",
    "    '''\n",
    "    length of the longest sub-sequence of zeros\n",
    "    '''\n",
    "    longest = 0\n",
    "    current = 0\n",
    "    for i in a:\n",
    "        if i == 0.0:\n",
    "            current += 1\n",
    "        else:\n",
    "            longest = max(longest, current)\n",
    "            current = 0\n",
    "    longest = max(longest, current)\n",
    "    return longest\n",
    "\n",
    "def percNans(a):\n",
    "    '''\n",
    "    returns the percentage of nans in the sequence\n",
    "    '''\n",
    "    a = np.asarray(a)\n",
    "    perc = np.count_nonzero(np.isnan(a)) / len(a)\n",
    "\n",
    "    return perc\n",
    "\n",
    "def coverageMod(a, window_size=30):\n",
    "    '''\n",
    "    returns the coverage of a sequence, defined as the percentage of (non-zero + non-nan) over (non-nan) \n",
    "    '''\n",
    "    a = a[1:-1].split(',')\n",
    "    a = [float(k) for k in a]\n",
    "    a = np.asarray(a)\n",
    "    for i in range(len(a) - window_size):\n",
    "        if np.all(a[i:i+window_size] == 0.0):\n",
    "            a[i:i+window_size] = np.nan\n",
    "\n",
    "    # num non zero, non nan\n",
    "    num = 0\n",
    "    den = 0\n",
    "    for i in a:\n",
    "        if i != 0.0 and not np.isnan(i):\n",
    "            num += 1\n",
    "        if not np.isnan(i):\n",
    "            den += 1\n",
    "    \n",
    "    return num / den\n",
    "\n",
    "def sequenceLength(a):\n",
    "    '''\n",
    "    returns the length of the sequence\n",
    "    '''\n",
    "    return len(a)\n",
    "\n",
    "def mergeAnnotations(annots):\n",
    "    '''\n",
    "    merge the annotations for the same gene\n",
    "    '''\n",
    "    # merge the annotations\n",
    "    merged_annots = []\n",
    "    for i in range(len(annots[0])):\n",
    "        # get the ith annotation for all the transcripts, only non zero and non nan\n",
    "        ith_annots = [a[i] for a in annots if a[i] != 0.0 and not np.isnan(a[i])]\n",
    "        # take the mean of the ith annotation\n",
    "        ith_mean = np.mean(ith_annots)\n",
    "        merged_annots.append(ith_mean)\n",
    "\n",
    "    return merged_annots\n",
    "\n",
    "def uniqueGenes(df):\n",
    "    '''\n",
    "    processes an input dataframe to have only one transcript per gene\n",
    "    '''\n",
    "    df['sequence_length'] = df['annotations'].apply(sequenceLength)\n",
    "\n",
    "    unique_genes = list(df['gene'].unique())\n",
    "\n",
    "    # iterate through each gene, and choose the longest transcript, for the annotation, merge the annotations\n",
    "    for gene in unique_genes:\n",
    "        # get the df for the gene\n",
    "        df_gene = df[df['gene'] == gene]\n",
    "        if len(df_gene) > 1:\n",
    "            # get the transcript with the longest sequence\n",
    "            df_gene = df_gene.sort_values('sequence_length', ascending=False)\n",
    "            # chosen transcript\n",
    "            chosen_transcript = df_gene['transcript'].values[0]\n",
    "            other_transcripts = df_gene['transcript'].values[1:]\n",
    "            # merge the annotations\n",
    "            annotations = df_gene['annotations'].values\n",
    "            merged_annotations = mergeAnnotations(annotations)\n",
    "            # drop the other transcripts from the df\n",
    "            df = df[~df['transcript'].isin(other_transcripts)]\n",
    "\n",
    "            # change the annotations for the chosen transcript\n",
    "            df.loc[df['transcript'] == chosen_transcript, 'annotations'] = str(merged_annotations)\n",
    "\n",
    "    # drop sequence length column\n",
    "    df = df.drop(columns=['sequence_length'])\n",
    "\n",
    "    assert len(df['gene'].unique()) == len(df['gene'])\n",
    "    assert len(df['transcript'].unique()) == len(df['transcript'])\n",
    "    assert len(df['transcript']) == len(df['gene'])\n",
    "\n",
    "    return df\n",
    "    \n",
    "def slidingWindowZeroToNan(a, window_size=30):\n",
    "    '''\n",
    "    use a sliding window, if all the values in the window are 0, then replace them with nan.\n",
    "    this is done to assign nan values to zero-counts that are presumed to be artifacts of the sequencing process\n",
    "    '''\n",
    "    a = [float(k) for k in a]\n",
    "    a = np.asarray(a)\n",
    "    for i in range(len(a) - window_size):\n",
    "        if np.all(a[i:i+window_size] == 0.0):\n",
    "            a[i:i+window_size] = np.nan\n",
    "\n",
    "    return a\n",
    "\n",
    "def RiboDatasetGWS(df_dict, threshold: float = 0.3, longZerosThresh: int = 20, percNansThresh: float = 0.05):\n",
    "    '''\n",
    "    inputs: dictionary of processed dataframes, coverage threshold, longest zero sequence length threshold, percentage of nans threshold\n",
    "    outputs: train, validation, and test dataframes\n",
    "    '''\n",
    "    # Liver data (in CTRL condition)\n",
    "    df_liver = df_dict['LIVER']\n",
    "    df_liver['condition'] = 'CTRL'\n",
    "\n",
    "    # Lina: CTRL data\n",
    "    df_ctrl_depr = df_dict['CTRL']\n",
    "    df_ctrl_depr['condition'] = 'CTRL'\n",
    "\n",
    "    # merge the separate ctrl datasets\n",
    "    tr_liver = df_liver['transcript'].unique()\n",
    "    tr_ctrl_depr = df_ctrl_depr['transcript'].unique()\n",
    "    tr_to_add = [g for g in tr_liver if g not in tr_ctrl_depr]\n",
    "    df_liver = df_liver[df_liver['transcript'].isin(tr_to_add)]\n",
    "    df_ctrldepr_liver = pd.concat([df_liver, df_ctrl_depr], axis=0)\n",
    "    df_ctrldepr_liver = uniqueGenes(df_ctrldepr_liver)\n",
    "    ctrl_genes_transcripts = list(zip(df_ctrldepr_liver['gene'], df_ctrldepr_liver['transcript']))\n",
    "    ctrl_genes_transcripts = [[gene, transcript] for gene, transcript in ctrl_genes_transcripts]\n",
    "\n",
    "    # Lina: ILE data\n",
    "    df_ile = df_dict['ILE']\n",
    "    df_ile['condition'] = 'ILE'\n",
    "    df_ile = uniqueGenes(df_ile)\n",
    "    for index, row in df_ile.iterrows():\n",
    "        if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "            df_ile.drop(index, inplace=True) \n",
    "\n",
    "    # Lina: LEU data\n",
    "    df_leu = df_dict['LEU']\n",
    "    df_leu['condition'] = 'LEU'\n",
    "    df_leu = uniqueGenes(df_leu)\n",
    "    for index, row in df_leu.iterrows():\n",
    "        if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "            df_leu.drop(index, inplace=True)\n",
    "\n",
    "    # Lina: VAL data\n",
    "    df_val = df_dict['VAL']\n",
    "    df_val['condition'] = 'VAL'\n",
    "    df_val = uniqueGenes(df_val)\n",
    "    for index, row in df_val.iterrows():\n",
    "        if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "            df_val.drop(index, inplace=True)\n",
    "\n",
    "    # Lina: LEU_ILE data\n",
    "    df_leu_ile = df_dict['LEU_ILE']\n",
    "    df_leu_ile['condition'] = 'LEU_ILE'\n",
    "    df_leu_ile = uniqueGenes(df_leu_ile)\n",
    "    for index, row in df_leu_ile.iterrows():\n",
    "        if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "            df_leu_ile.drop(index, inplace=True)\n",
    "\n",
    "    # Lina: LEU_ILE_VAL data\n",
    "    df_leu_ile_val = df_dict['LEU_ILE_VAL']\n",
    "    df_leu_ile_val['condition'] = 'LEU_ILE_VAL'\n",
    "    df_leu_ile_val = uniqueGenes(df_leu_ile_val)\n",
    "    for index, row in df_leu_ile_val.iterrows():\n",
    "        if [row['gene'], row['transcript']] not in ctrl_genes_transcripts:\n",
    "            df_leu_ile_val.drop(index, inplace=True)\n",
    "\n",
    "    # concenate all the data from the different conditions\n",
    "    df_full = pd.concat([df_ctrldepr_liver, df_ile, df_leu, df_val, df_leu_ile, df_leu_ile_val], axis=0) \n",
    "    df_full.columns = ['gene', 'transcript', 'sequence', 'annotations', 'perc_non_zero_annots', 'condition']\n",
    "\n",
    "    # sanity check to see if the number of unique genes is equal to the number of unique transcripts\n",
    "    assert len(df_full['transcript'].unique()) == len(df_full['gene'].unique())\n",
    "\n",
    "    # apply coverage threshold\n",
    "    df_full['coverage_mod'] = df_full['annotations'].apply(coverageMod)\n",
    "    df_full = df_full[df_full['coverage_mod'] >= threshold]\n",
    "\n",
    "    # for all the sequences in a condition that is not CTRL, add their respective CTRL sequence to them\n",
    "    sequences_ctrl = []\n",
    "    annotations_list = list(df_full['annotations'])\n",
    "    condition_df_list = list(df_full['condition'])\n",
    "    genes_list = list(df_full['gene'])\n",
    "\n",
    "    for i in range(len(condition_df_list)):\n",
    "        try:\n",
    "            if condition_df_list[i] != 'CTRL':\n",
    "                # find the respective CTRL sequence for the transcript\n",
    "                ctrl_sequence = df_full[(df_full['gene'] == genes_list[i]) & (df_full['condition'] == 'CTRL')]['annotations'].iloc[0]\n",
    "                sequences_ctrl.append(ctrl_sequence)\n",
    "            else:\n",
    "                sequences_ctrl.append(annotations_list[i])\n",
    "        except:\n",
    "            sequences_ctrl.append('NA')\n",
    "\n",
    "    # add the sequences_ctrl to the df\n",
    "    df_full['ctrl_sequence'] = sequences_ctrl\n",
    "\n",
    "    # remove those rows where the ctrl_sequence is NA\n",
    "    df_full = df_full[df_full['ctrl_sequence'] != 'NA']\n",
    "\n",
    "    # sanity check for the ctrl sequences\n",
    "    # get the ds with only condition as CTRL\n",
    "    df_ctrl_full = df_full[df_full['condition'] == 'CTRL']\n",
    "    ctrl_sequences_san = list(df_ctrl_full['annotations'])\n",
    "    ctrl_sequences_san2 = list(df_ctrl_full['ctrl_sequence'])\n",
    "\n",
    "    for i in range(len(ctrl_sequences_san)):\n",
    "        assert ctrl_sequences_san[i] == ctrl_sequences_san2[i]\n",
    "\n",
    "    # add the longest zero sequence length to the df\n",
    "    df_full['longest_zero_seq_length_annotation'] = df_full['annotations'].apply(longestZeroSeqLength)\n",
    "    df_full['longest_zero_seq_length_ctrl_sequence'] = df_full['ctrl_sequence'].apply(longestZeroSeqLength)\n",
    "\n",
    "    # add the number of nans to the df\n",
    "    df_full['perc_nans_annotation'] = df_full['annotations'].apply(percNans)\n",
    "    df_full['perc_nans_ctrl_sequence'] = df_full['ctrl_sequence'].apply(percNans)\n",
    "\n",
    "    # apply the threshold for the longest zero sequence length\n",
    "    df_full = df_full[df_full['longest_zero_seq_length_annotation'] <= longZerosThresh]\n",
    "    df_full = df_full[df_full['longest_zero_seq_length_ctrl_sequence'] <= longZerosThresh]\n",
    "\n",
    "    # apply the threshold for the number of nans\n",
    "    df_full = df_full[df_full['perc_nans_annotation'] <= percNansThresh]\n",
    "    df_full = df_full[df_full['perc_nans_ctrl_sequence'] <= percNansThresh]\n",
    "\n",
    "    # Gene-Wise Split (GWS) for each condition\n",
    "    genes = df_full['gene'].unique()\n",
    "    gene_mean_coverage_mod = []\n",
    "    for gene in genes:\n",
    "        gene_mean_coverage_mod.append(df_full[df_full['gene'] == gene]['coverage_mod'].mean())\n",
    "\n",
    "    gene_mean_coverage_mod = np.asarray(gene_mean_coverage_mod)\n",
    "    genes = np.asarray(genes)\n",
    "\n",
    "    # sort the genes by coverage_mod in descending order\n",
    "    genes = genes[np.argsort(gene_mean_coverage_mod)[::-1]]\n",
    "\n",
    "    num_test_genes = int(0.2 * len(genes))\n",
    "    num_valid_genes = int(0.05 * len(genes))\n",
    "    \n",
    "    test_genes = []\n",
    "    train_genes = []\n",
    "    valid_genes = []\n",
    "\n",
    "    for i in range(len(genes)):\n",
    "        # alternating until 20% of the genes are in the test set, 5% in the val set\n",
    "        # the rest are in the train set\n",
    "        if i % 3 == 0 and len(test_genes) < num_test_genes:\n",
    "            test_genes.append(genes[i])\n",
    "        elif i % 3 == 1 and len(valid_genes) < num_valid_genes:\n",
    "            valid_genes.append(genes[i])\n",
    "        else:\n",
    "            train_genes.append(genes[i])\n",
    "\n",
    "    # split the dataframe\n",
    "    df_train = df_full[df_full['gene'].isin(train_genes)]\n",
    "    df_valid = df_full[df_full['gene'].isin(valid_genes)]\n",
    "    df_test = df_full[df_full['gene'].isin(test_genes)]\n",
    "\n",
    "    return df_train, df_valid, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['CTRL', 'ILE', 'LEU', 'LEU_ILE', 'LEU_ILE_VAL', 'VAL', 'LIVER']\n",
    "df_dict = {}\n",
    "\n",
    "for cond in conditions:\n",
    "    if cond == 'LIVER':\n",
    "        dir_path = LIVER_FOLDER\n",
    "    else:\n",
    "        dir_path = f'{DATA_FOLDER}/{cond}/'\n",
    "    df = make_all_dataframes(dir_path, fa_path)\n",
    "    df_proc = process_merged_df(df)\n",
    "    df_dict[cond] = df_proc\n",
    "\n",
    "    print(f'{cond} done')\n",
    "\n",
    "    # save the dataframe\n",
    "    df_proc.to_csv(f'../data/processed/{cond}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid, df_test = RiboDatasetGWS(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../../data/orig/train.csv', index=False)\n",
    "df_valid.to_csv('../../data/orig/valid.csv', index=False)\n",
    "df_test.to_csv('../../data/orig/test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kd2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
