{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "from scipy.stats import pearsonr, spearmanr \n",
    "from torchmetrics.functional import pearson_corrcoef\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from scipy.interpolate import make_interp_spline\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables defined for all the function to use\n",
    "# one-hot encoding for the conditions\n",
    "condition_values = {'CTRL': 0, 'LEU': 1, 'ARG': 2}\n",
    "inverse_condition_values = {0: 'CTRL', 1: 'LEU', 2: 'ARG'}\n",
    "\n",
    "# one-hot encoding for the codons\n",
    "id_to_codon = {idx:''.join(el) for idx, el in enumerate(itertools.product(['A', 'T', 'C', 'G'], repeat=3))}\n",
    "codon_to_id = {v:k for k,v in id_to_codon.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric functions\n",
    "def pearson_mask(pred, label):\n",
    "    '''\n",
    "    inputs: model prediction, true label\n",
    "    outputs: pearson correlation coefficient\n",
    "    '''\n",
    "    # take the prediction and label\n",
    "    full_pred_tensor = torch.tensor(pred)\n",
    "    label_tensor = torch.tensor(label)\n",
    "\n",
    "    # make mask tensor\n",
    "    # remove the end token from the mask\n",
    "    mask = label_tensor != -100.0\n",
    "    mask = torch.tensor(mask)\n",
    "\n",
    "    # remove the nans from the mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(label_tensor)))\n",
    "    # double mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(full_pred_tensor)))\n",
    "\n",
    "    # set model prediction to same length as label tensor\n",
    "    full_pred_tensor = full_pred_tensor[:len(mask)]\n",
    "\n",
    "    assert full_pred_tensor.shape == label_tensor.shape\n",
    "    assert label_tensor.shape == mask.shape\n",
    "\n",
    "    # select the elements from the tensors that are not nan\n",
    "    mp, mt = torch.masked_select(full_pred_tensor, mask), torch.masked_select(label_tensor, mask)\n",
    "\n",
    "    # calculate pearson correlation coefficient\n",
    "    temp_pearson = pearson_corrcoef(mp, mt)\n",
    "\n",
    "    # get float value from tensor\n",
    "    temp_pearson = temp_pearson.item()\n",
    "\n",
    "    return temp_pearson\n",
    "\n",
    "def mae_mask(pred, label):\n",
    "    '''\n",
    "    inputs: model prediction, true label\n",
    "    outputs: mean absolute error\n",
    "    '''\n",
    "    # take the prediction and label\n",
    "    full_pred_tensor = torch.tensor(pred)\n",
    "    label_tensor = torch.tensor(label)\n",
    "\n",
    "    # make mask tensor\n",
    "    # remove the end token from the mask\n",
    "    mask = label_tensor != -100.0\n",
    "    mask = torch.tensor(mask)\n",
    "\n",
    "    # remove the nans from the mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(label_tensor)))\n",
    "    # double mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(full_pred_tensor)))\n",
    "\n",
    "    # set model prediction to same length as label tensor\n",
    "    full_pred_tensor = full_pred_tensor[:len(mask)]\n",
    "\n",
    "    assert full_pred_tensor.shape == label_tensor.shape\n",
    "    assert label_tensor.shape == mask.shape\n",
    "\n",
    "    # select the elements from the tensors that are not nan\n",
    "    mp, mt = torch.masked_select(full_pred_tensor, mask), torch.masked_select(label_tensor, mask)\n",
    "\n",
    "    # calculate mean absolute error\n",
    "    temp_mae = torch.mean(torch.abs(mp - mt))\n",
    "\n",
    "    # get float value from tensor\n",
    "    temp_mae = temp_mae.item()\n",
    "\n",
    "    return temp_mae\n",
    "\n",
    "def mape_mask(pred, label):\n",
    "    '''\n",
    "    inputs: model prediction, true label\n",
    "    outputs: mean absolute error\n",
    "    '''\n",
    "    # take the prediction and label\n",
    "    full_pred_tensor = torch.tensor(pred)\n",
    "    label_tensor = torch.tensor(label)\n",
    "\n",
    "    # make mask tensor\n",
    "    # remove the end token from the mask\n",
    "    mask = label_tensor != -100.0\n",
    "    mask = torch.tensor(mask)\n",
    "\n",
    "    # remove the nans from the mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(label_tensor)))\n",
    "    # double mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(full_pred_tensor)))\n",
    "\n",
    "    # set model prediction to same length as label tensor\n",
    "    full_pred_tensor = full_pred_tensor[:len(mask)]\n",
    "\n",
    "    assert full_pred_tensor.shape == label_tensor.shape\n",
    "    assert label_tensor.shape == mask.shape\n",
    "\n",
    "    # select the elements from the tensors that are not nan\n",
    "    mp, mt = torch.masked_select(full_pred_tensor, mask), torch.masked_select(label_tensor, mask)\n",
    "\n",
    "    # calculate mean absolute percentage error\n",
    "    temp_mape = torch.mean(torch.abs((mp - mt) / mt))\n",
    "\n",
    "    # get float value from tensor\n",
    "    temp_mape = temp_mape.item()\n",
    "\n",
    "    return temp_mape\n",
    "\n",
    "def f1_score_masked(pred, label):\n",
    "    '''\n",
    "    inputs: model prediction, true label\n",
    "    outputs: f1 score\n",
    "    '''\n",
    "    # take the prediction and label\n",
    "    full_pred_tensor = torch.tensor(pred)\n",
    "    label_tensor = torch.tensor(label)\n",
    "\n",
    "    # make mask tensor\n",
    "    # remove the end token from the mask\n",
    "    mask = label_tensor != -100.0\n",
    "    mask = torch.tensor(mask)\n",
    "\n",
    "    # remove the nans from the mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(label_tensor)))\n",
    "\n",
    "    # set model prediction to same length as label tensor\n",
    "    full_pred_tensor = full_pred_tensor[:len(mask)]\n",
    "\n",
    "    assert full_pred_tensor.shape == label_tensor.shape\n",
    "    assert label_tensor.shape == mask.shape\n",
    "\n",
    "    # select the elements from the tensors that are not nan\n",
    "    mp, mt = torch.masked_select(full_pred_tensor, mask), torch.masked_select(label_tensor, mask)\n",
    "\n",
    "    # calculate f1 score\n",
    "    temp_f1 = f1_score(mp, mt, average='macro')\n",
    "\n",
    "    # get float value from tensor\n",
    "    temp_f1 = temp_f1.item()\n",
    "\n",
    "    return temp_f1\n",
    "\n",
    "def prec_score_masked(pred, label):\n",
    "    '''\n",
    "    inputs: model prediction, true label\n",
    "    outputs: precision score\n",
    "    ''' \n",
    "    # take the prediction and label\n",
    "    full_pred_tensor = torch.tensor(pred)\n",
    "    label_tensor = torch.tensor(label)\n",
    "\n",
    "    # make mask tensor\n",
    "    # remove the end token from the mask\n",
    "    mask = label_tensor != -100.0\n",
    "    mask = torch.tensor(mask)\n",
    "\n",
    "    # remove the nans from the mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(label_tensor)))\n",
    "\n",
    "    # set model prediction to same length as label tensor\n",
    "    full_pred_tensor = full_pred_tensor[:len(mask)]\n",
    "\n",
    "    assert full_pred_tensor.shape == label_tensor.shape\n",
    "    assert label_tensor.shape == mask.shape\n",
    "\n",
    "    # select the elements from the tensors that are not nan\n",
    "    mp, mt = torch.masked_select(full_pred_tensor, mask), torch.masked_select(label_tensor, mask)\n",
    "\n",
    "    # calculate precision score\n",
    "    temp_prec = precision_score(mp, mt, average='macro')\n",
    "\n",
    "    # get float value from tensor\n",
    "    temp_prec = temp_prec.item()\n",
    "\n",
    "    return temp_prec \n",
    "\n",
    "def recall_score_masked(pred, label):\n",
    "    '''\n",
    "    inputs: model prediction, true label\n",
    "    outputs: recall score\n",
    "    '''\n",
    "    # take the prediction and label\n",
    "    full_pred_tensor = torch.tensor(pred)\n",
    "    label_tensor = torch.tensor(label)\n",
    "\n",
    "    # make mask tensor\n",
    "    # remove the end token from the mask\n",
    "    mask = label_tensor != -100.0\n",
    "    mask = torch.tensor(mask)\n",
    "\n",
    "    # remove the nans from the mask\n",
    "    mask = torch.logical_and(mask, torch.logical_not(torch.isnan(label_tensor)))\n",
    "\n",
    "    # set model prediction to same length as label tensor\n",
    "    full_pred_tensor = full_pred_tensor[:len(mask)]\n",
    "\n",
    "    assert full_pred_tensor.shape == label_tensor.shape\n",
    "    assert label_tensor.shape == mask.shape\n",
    "\n",
    "    # select the elements from the tensors that are not nan\n",
    "    mp, mt = torch.masked_select(full_pred_tensor, mask), torch.masked_select(label_tensor, mask)\n",
    "\n",
    "    # calculate recall score\n",
    "    temp_rec = recall_score(mp, mt, average='macro')\n",
    "\n",
    "    # get float value from tensor\n",
    "    temp_rec = temp_rec.item()\n",
    "\n",
    "    return temp_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(full_preds_sample, depr_diffs_sample, ctrl_preds_sample, labels_sample, labels_ctrl_sample, labels_depr_diff_sample, out_loc_sample, transcript_sample, gene_sample, inverse_condition_values_sample):\n",
    "    pearson_corr_full = pearson_mask(full_preds_sample, labels_sample)\n",
    "    pearson_corr_ctrl = pearson_mask(ctrl_preds_sample, labels_ctrl_sample)\n",
    "\n",
    "    plot_title = \"Transcript: \" + str(transcript_sample) + \" Gene: \" + str(gene_sample) + \" Condition: \" + str(inverse_condition_values_sample) + \"\\n\\nPearson Correlation \" + str(inverse_condition_values_sample) + \": \" + str(pearson_corr_full) + \"\\n\\nPearson Correlation CTRL: \" + str(pearson_corr_ctrl) + \"\\n\\n\"\n",
    "    \n",
    "    print(plot_title)    \n",
    "\n",
    "    # set min and max y values for the sub plot\n",
    "    # control\n",
    "    min_y_c = min(min(labels_ctrl_sample), min(ctrl_preds_sample)) - 0.1\n",
    "    max_y_c = max(max(labels_ctrl_sample), max(ctrl_preds_sample)) + 0.1\n",
    "\n",
    "    # deprivation difference\n",
    "    min_y_diff = min(min(depr_diffs_sample), min(labels_depr_diff_sample)) - 0.1\n",
    "    max_y_diff = max(max(depr_diffs_sample), max(labels_depr_diff_sample)) + 0.1\n",
    "\n",
    "    # full prediction\n",
    "    min_y_f = min(min(labels_sample), min(full_preds_sample)) - 0.1\n",
    "    max_y_f = max(max(labels_sample), max(full_preds_sample)) + 0.1\n",
    "\n",
    "    # subplots for ctrl, deprivation difference, and full prediction (with labels)\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(20, 20))\n",
    "    # add title \n",
    "    fig.suptitle(plot_title, fontsize=16)\n",
    "    # add space after title\n",
    "    fig.tight_layout(pad=10.0)\n",
    "\n",
    "    # FIRST SUBPLOT: CTRL\n",
    "    axs[0, 0].plot(ctrl_preds_sample, color='#00A757', label='CTRL Prediction')\n",
    "    axs[0, 1].plot(labels_ctrl_sample, color='#82BA4F', label='CTRL Label')\n",
    "\n",
    "    # make a vector marking the nans with 0, and the rest of the values with nan\n",
    "    # make a vector of nans the same size as ctrl_preds_sample\n",
    "    labels_ctrl_nans = np.empty(len(labels_ctrl_sample))\n",
    "    labels_ctrl_nans[:] = np.nan\n",
    "    for k in range(len(labels_ctrl_sample)):\n",
    "        if np.isnan(labels_ctrl_sample[k]):\n",
    "            labels_ctrl_nans[k] = 0\n",
    "            try:\n",
    "                labels_ctrl_nans[k+1] = 0\n",
    "                labels_ctrl_nans[k-1] = 0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # plot the nans\n",
    "    axs[0, 1].plot(labels_ctrl_nans, color='black')\n",
    "\n",
    "    # set y limits\n",
    "    axs[0, 0].set_ylim([min_y_c, max_y_c])\n",
    "    axs[0, 1].set_ylim([min_y_c, max_y_c])\n",
    "\n",
    "    # SECOND SUBPLOT: DEPRIVATION DIFFERENCE\n",
    "    axs[1, 0].plot(depr_diffs_sample, color='#C82E6B', label='Deprivation Difference Prediction')\n",
    "    axs[1, 1].plot(labels_depr_diff_sample, color='#D4668F', label='Deprivation Difference Label')\n",
    "\n",
    "    # make a vector marking the nans in labels_depr_diff_sample\n",
    "    labels_depr_diff_nans = np.empty(len(labels_depr_diff_sample))\n",
    "    labels_depr_diff_nans[:] = np.nan\n",
    "    for k in range(len(labels_depr_diff_sample)):\n",
    "        if np.isnan(labels_depr_diff_sample[k]):\n",
    "            labels_depr_diff_nans[k] = 0\n",
    "            try:\n",
    "                labels_depr_diff_nans[k+1] = 0\n",
    "                labels_depr_diff_nans[k-1] = 0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # plot the nans\n",
    "    axs[1, 1].plot(labels_depr_diff_nans, color='black')\n",
    "\n",
    "    # set y limits\n",
    "    axs[1, 0].set_ylim([min_y_diff, max_y_diff])\n",
    "    axs[1, 1].set_ylim([min_y_diff, max_y_diff])\n",
    "\n",
    "    # THIRD SUBPLOT: DEPRIVATION FULL PREDICTION\n",
    "    axs[2, 0].plot(full_preds_sample, color='#65BADA', label= inverse_condition_values_sample + ' Prediction')\n",
    "    axs[2, 1].plot(labels_sample, color='#87D0E2', label=inverse_condition_values_sample + ' Prediction')\n",
    "\n",
    "    # make a vector marking the nans in labels_sample\n",
    "    labels_full_nans = np.empty(len(labels_sample))\n",
    "    labels_full_nans[:] = np.nan\n",
    "    for k in range(len(labels_sample)):\n",
    "        if np.isnan(labels_sample[k]):\n",
    "            labels_full_nans[k] = 0\n",
    "            try:\n",
    "                labels_full_nans[k+1] = 0\n",
    "                labels_full_nans[k-1] = 0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # plot the nans\n",
    "    axs[2, 1].plot(labels_full_nans, color='black')\n",
    "\n",
    "    # set y limits\n",
    "    axs[2, 0].set_ylim([min_y_f, max_y_f])\n",
    "    axs[2, 1].set_ylim([min_y_f, max_y_f])\n",
    "\n",
    "    # set x and y labels for all the subplots \n",
    "    for k in range(3):\n",
    "        for j in range(2):\n",
    "            axs[k, j].set_xlabel('Codon Position', fontsize=16)\n",
    "            axs[k, j].set_ylabel('Ribosome Read Counts', fontsize=16)\n",
    "            axs[k, j].legend(fontsize=16, loc=\"upper right\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(out_loc_sample)\n",
    "\n",
    "    # display the plot\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def analyse_dh_outputs(preds, labels, inputs, output_loc, test_data_path):\n",
    "    '''\n",
    "    inputs: model predictions, true labels, inputs, output folder path\n",
    "    outputs: \n",
    "    1. prints the condition wise mean pearson correlation coefficient\n",
    "    2. generates plots for the two heads (ctrl, and deprivation difference), full prediction, and the two labels (ctrl, and deprivation) for the 10 best and 10 worst performing transcripts\n",
    "    '''\n",
    "\n",
    "    # load data\n",
    "    ds = pd.read_csv(test_data_path)\n",
    "\n",
    "    # make masks for all the transcripts\n",
    "    mask = inputs != -100.0\n",
    "\n",
    "    # obtain lengths of all the transcripts\n",
    "    lengths = np.sum(mask, axis=1)\n",
    "\n",
    "    # convert to lists and remove padding\n",
    "    preds = preds.tolist()\n",
    "    preds = [pred[:lengths[i]] for i, pred in enumerate(preds)]\n",
    "\n",
    "    labels = labels.tolist()\n",
    "    labels = [label[:lengths[i]] for i, label in enumerate(labels)]\n",
    "\n",
    "    inputs = inputs.tolist()\n",
    "    inputs = [input[:lengths[i]] for i, input in enumerate(inputs)]\n",
    "\n",
    "    # get the condition for each of the samples\n",
    "    # do / with 64 to get the condition\n",
    "    condition_samples = []\n",
    "    for i in range(len(inputs)):\n",
    "        condition_samples.append(inputs[i][0] // 64)\n",
    "\n",
    "\n",
    "    labels_ctrl = []\n",
    "    genes = []\n",
    "    transcripts = []\n",
    "    sequences_ds = []\n",
    "\n",
    "    sequence_list = list(ds['sequence'])\n",
    "    ctrl_sequence_list_ds = list(ds['ctrl_sequence'])\n",
    "    genes_list_ds = list(ds['gene'])\n",
    "    transcripts_list_ds = list(ds['transcript'])\n",
    "    condition_list = list(ds['condition'])\n",
    "    codon_sequences = []\n",
    "\n",
    "    # for loop which takes the original transcript one-hot sequence and converts into the (64*(condition_one-hot) + x) version (stored in sequences_ds) \n",
    "    for i in range(len(sequence_list)):\n",
    "        x = sequence_list[i][1:-1].split(', ')\n",
    "        x = [int(i) for i in x]\n",
    "        cond_val = condition_values[condition_list[i]]\n",
    "        # get codon sequence from x\n",
    "        codon_seq = [id_to_codon[i] for i in x]\n",
    "        # convert to string\n",
    "        codon_seq = ''.join(codon_seq)\n",
    "        codon_sequences.append(codon_seq)\n",
    "        # get the remainder\n",
    "        add_val = (cond_val) * 64\n",
    "        x = [i + add_val for i in x]\n",
    "        sequences_ds.append(x)\n",
    "\n",
    "    # for loop to get the control label for all the transcripts\n",
    "    for i in range(len(inputs)):\n",
    "        condition_sample = inverse_condition_values[condition_samples[i]]\n",
    "        # search for inputs[i] in sequences_ds get index\n",
    "        for j in range(len(sequences_ds)):\n",
    "            if sequences_ds[j] == inputs[i] and condition_sample == condition_list[j]:\n",
    "                index = j\n",
    "                break\n",
    "\n",
    "        ctrl_sample = ctrl_sequence_list_ds[index]\n",
    "        ctrl_sample = ctrl_sample[1:-1].split(', ')\n",
    "        ctrl_sample = [float(k) for k in ctrl_sample]\n",
    "        labels_ctrl.append(ctrl_sample)\n",
    "        genes.append(genes_list_ds[index])\n",
    "        transcripts.append(transcripts_list_ds[index])\n",
    "\n",
    "    # Model output: the first dim of pred is ctrl, second is depr difference\n",
    "    # process ctrl predictions\n",
    "    ctrl_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_sample = preds[i]\n",
    "        pred_sample = np.asarray(pred_sample)\n",
    "        # get first dim\n",
    "        pred_sample = pred_sample[:, 0]\n",
    "        ctrl_preds.append(pred_sample)\n",
    "\n",
    "    # process depr difference predictions\n",
    "    depr_diffs = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_sample = preds[i]\n",
    "        pred_sample = np.asarray(pred_sample)\n",
    "        # get second dim\n",
    "        pred_sample = pred_sample[:, 1]\n",
    "        depr_diffs.append(pred_sample)\n",
    "\n",
    "    # obtain the full predictions: the summation of the ctrl and depr difference predictions\n",
    "    full_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        full_preds.append(ctrl_preds[i] + depr_diffs[i])\n",
    "\n",
    "    # np log 1 + x the labels\n",
    "    labels_ctrl = [np.log1p(label) for label in labels_ctrl]\n",
    "\n",
    "    # labels depr difference\n",
    "    labels_depr_diff = []\n",
    "    for i in range(len(labels)):\n",
    "        labels_depr_diff.append(np.asarray(labels[i]) - np.asarray(labels_ctrl[i]))\n",
    "\n",
    "    # plot ten best samples\n",
    "    # get pearson corr for each sample\n",
    "    pearson_corrs = []\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    for i in range(len(full_preds)):\n",
    "        pearson_corrs.append(pearson_mask(full_preds[i], labels[i]))\n",
    "        mae_list.append(mae_mask(full_preds[i], labels[i]))\n",
    "        mape_list.append(mape_mask(full_preds[i], labels[i]))\n",
    "\n",
    "    # pearson mean for each condition\n",
    "    pearson_means = [[] for i in range(3)]\n",
    "    for i in range(len(pearson_corrs)):\n",
    "        pearson_means[condition_samples[i]].append(pearson_corrs[i])\n",
    "    \n",
    "    # print means\n",
    "    print(\"#\"*20)\n",
    "    print(\"Pearson Correlation Coefficient Means - Per Condition\")\n",
    "    for i in range(len(pearson_means)):\n",
    "        print(\"Condition: \", inverse_condition_values[i], \" Mean: \", np.mean(pearson_means[i]), \" Std: \", np.std(pearson_means[i]), \" Num Samples: \", len(pearson_means[i]))\n",
    "    print(\"#\"*20)\n",
    "\n",
    "    conds_colors = [\"#27ae60\", \"#e67e22\", \"#3498db\"]\n",
    "    # make distribution plot for each condition\n",
    "    for i in range(len(pearson_means)):\n",
    "        # sns histogram\n",
    "        sns.histplot(pearson_means[i], color=conds_colors[i], kde=True, bins=100)\n",
    "        plt.title(\"Pearson Correlation Coefficient Distribution - \" + inverse_condition_values[i])\n",
    "        plt.xlabel(\"Pearson Correlation Coefficient\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.savefig(output_loc + \"/condition_dists/pearson_\" + inverse_condition_values[i] + \".png\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "    pearson_corrs_ctrl = []\n",
    "    for i in range(len(ctrl_preds)):\n",
    "        pearson_corrs_ctrl.append(pearson_mask(ctrl_preds[i], labels_ctrl[i]))\n",
    "\n",
    "    # output all the predictions into df from lists\n",
    "    output_analysis_df = pd.DataFrame(list(zip(transcripts, genes, codon_sequences, pearson_corrs, pearson_corrs_ctrl, mae_list, mape_list, condition_list)), columns =['Transcript', 'Gene', 'Sequence', 'Full Prediction Pearson Correlation', 'Control Prediction Pearson Correlation', 'MAE', 'MAPE', 'Deprivation Condition'])\n",
    "    output_analysis_df.to_csv(output_loc + \"/analysis.csv\", index=False)\n",
    "\n",
    "    print(\"Saved model prediction outputs file to \", output_loc + \"/analysis.csv\")\n",
    "\n",
    "    print(\"#\"*20)\n",
    "\n",
    "    num_plots = 10\n",
    "\n",
    "    ######### \n",
    "    # plot (num_plots) best samples\n",
    "    #########\n",
    "    best_samples = sorted(range(len(pearson_corrs)), key = lambda sub: pearson_corrs[sub])[-num_plots:]\n",
    "    # print best pearson corrs\n",
    "    print(\"List of best pearson correlations: \", [pearson_corrs[i] for i in best_samples])\n",
    "\n",
    "    print(\"#\"*20)\n",
    "\n",
    "    for i in range(num_plots):\n",
    "        out_loc = output_loc + \"/full_plots/sample_\" + str(best_samples[i]) + '_' + str(inverse_condition_values[condition_samples[best_samples[i]]]) + \"_best_\" + transcripts[best_samples[i]] + \"_\" + genes[best_samples[i]] + \".png\"\n",
    "        make_plot(full_preds[best_samples[i]], depr_diffs[best_samples[i]], ctrl_preds[best_samples[i]], labels[best_samples[i]], labels_ctrl[best_samples[i]], labels_depr_diff[best_samples[i]], out_loc, transcripts[best_samples[i]], genes[best_samples[i]], inverse_condition_values[condition_samples[best_samples[i]]])\n",
    "\n",
    "    ######### \n",
    "    # plot (num_plots) worst samples\n",
    "    #########\n",
    "    worst_samples = sorted(range(len(pearson_corrs)), key = lambda sub: pearson_corrs[sub])[:num_plots]\n",
    "    \n",
    "    # print worst pearson corrs\n",
    "    print(\"Worst Pearson Correlations: \", [pearson_corrs[i] for i in worst_samples])\n",
    "\n",
    "    print(\"#\"*20)\n",
    "\n",
    "    for i in range(num_plots):\n",
    "        out_loc = output_loc + \"/full_plots/sample_\" + str(worst_samples[i]) + '_' + str(inverse_condition_values[condition_samples[worst_samples[i]]]) + \"_worst_\" + transcripts[worst_samples[i]] + \"_\" + genes[worst_samples[i]] + \".png\"\n",
    "        make_plot(full_preds[worst_samples[i]], depr_diffs[worst_samples[i]], ctrl_preds[worst_samples[i]], labels[worst_samples[i]], labels_ctrl[worst_samples[i]], labels_depr_diff[worst_samples[i]], out_loc, transcripts[worst_samples[i]], genes[worst_samples[i]], inverse_condition_values[condition_samples[worst_samples[i]]])\n",
    "\n",
    "def quantile_metric(preds, labels, inputs, output_loc, test_data_path):\n",
    "    # make mask removing those that have a input of -100\n",
    "    mask = inputs != -100.0\n",
    "\n",
    "    # get lengths of each sequence\n",
    "    lengths = np.sum(mask, axis=1)\n",
    "\n",
    "    # convert to lists and remove padding\n",
    "    preds = preds.tolist()\n",
    "    labels = labels.tolist()\n",
    "    inputs = inputs.tolist()\n",
    "\n",
    "    preds = [pred[:lengths[i]] for i, pred in enumerate(preds)]\n",
    "    labels = [label[:lengths[i]] for i, label in enumerate(labels)]\n",
    "    inputs = [input[:lengths[i]] for i, input in enumerate(inputs)]\n",
    "\n",
    "    condition_samples = []\n",
    "\n",
    "    # get conditions for each sample\n",
    "    # do a / with 64 to get the condition\n",
    "    for i in range(len(inputs)):\n",
    "        condition_samples.append(inputs[i][0] // 64)\n",
    "\n",
    "    genes = []\n",
    "    transcripts = []\n",
    "\n",
    "    ds = pd.read_csv(test_data_path)\n",
    "\n",
    "    sequences_ds = []\n",
    "\n",
    "    sequence_list = list(ds['sequence'])\n",
    "    genes_list_ds = list(ds['gene'])\n",
    "    transcripts_list_ds = list(ds['transcript'])\n",
    "    condition_list = list(ds['condition'])\n",
    "    codon_sequences = []\n",
    "\n",
    "    for i in range(len(sequence_list)):\n",
    "        x = sequence_list[i][1:-1].split(', ')\n",
    "        x = [int(i) for i in x]\n",
    "        cond_val = condition_values[condition_list[i]]\n",
    "        # get codon sequence from x\n",
    "        codon_seq = [id_to_codon[i] for i in x]\n",
    "        # convert to string\n",
    "        codon_seq = ''.join(codon_seq)\n",
    "        codon_sequences.append(codon_seq)\n",
    "        # get the remainder\n",
    "        add_val = (cond_val) * 64\n",
    "        x = [i + add_val for i in x]\n",
    "        sequences_ds.append(x)\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "        condition_sample = inverse_condition_values[condition_samples[i]]\n",
    "        # search for inputs[i] in sequences_ds get index\n",
    "        for j in range(len(sequences_ds)):\n",
    "            if sequences_ds[j] == inputs[i] and condition_sample == condition_list[j]:\n",
    "                index = j\n",
    "                break\n",
    "\n",
    "        genes.append(genes_list_ds[index])\n",
    "        transcripts.append(transcripts_list_ds[index])\n",
    "\n",
    "    # ctrl predictions\n",
    "    ctrl_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_sample = preds[i]\n",
    "        pred_sample = np.asarray(pred_sample)\n",
    "        # get first dim\n",
    "        pred_sample = pred_sample[:, 0]\n",
    "        ctrl_preds.append(pred_sample)\n",
    "\n",
    "    depr_diffs = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_sample = preds[i]\n",
    "        pred_sample = np.asarray(pred_sample)\n",
    "        # get second dim\n",
    "        pred_sample = pred_sample[:, 1]\n",
    "        depr_diffs.append(pred_sample)\n",
    "\n",
    "    full_preds = []\n",
    "    for i in range(len(preds)):\n",
    "        full_preds.append(ctrl_preds[i] + depr_diffs[i])\n",
    "        # print(len(full_preds[i]), len(labels[i]))\n",
    "\n",
    "    # plot ten best samples\n",
    "    # get pearson corr for each sample\n",
    "    pearson_corrs = []\n",
    "    for i in range(len(full_preds)):\n",
    "        pearson_corrs.append(pearson_mask(full_preds[i], labels[i]))\n",
    "\n",
    "    # go through each prediction\n",
    "    quantiles = [j*0.1 for j in range(10)]\n",
    "    all_f1_scores = []\n",
    "    all_prec_scores = []\n",
    "    all_recall_scores = []\n",
    "    for i in range(len(pearson_corrs)):\n",
    "        # for each sample do metric for each quantile\n",
    "        # 10 quantiles\n",
    "        f1_sample = []  # f1 score for each sample\n",
    "        prec_sample = []  # precision score for each sample\n",
    "        recall_sample = []  # recall score for each sample\n",
    "        # iterate through each quantile\n",
    "        for k in range(len(quantiles)): # label quantile val is nan\n",
    "            pred_quantile_val = np.quantile(full_preds[i], quantiles[k])\n",
    "            label_quantile_val = np.nanquantile(labels[i], quantiles[k])\n",
    "\n",
    "            # binarize the prediction and label based on the quantile including the nans\n",
    "            # copy the prediction\n",
    "            pred_quantile = np.copy(full_preds[i])\n",
    "            # set all values below the quantile to 0\n",
    "            pred_quantile[pred_quantile < pred_quantile_val] = 0\n",
    "            # set all values above the quantile to 1\n",
    "            pred_quantile[pred_quantile >= pred_quantile_val] = 1\n",
    "            \n",
    "            # copy the label\n",
    "            label_quantile = np.copy(labels[i])\n",
    "            # set all values below the quantile to 0\n",
    "            label_quantile[label_quantile < label_quantile_val] = 0\n",
    "            # set all values above the quantile to 1\n",
    "            label_quantile[label_quantile >= label_quantile_val] = 1\n",
    "\n",
    "            # get f1 score for the pred_quantile and label_quantile\n",
    "            # print(\"label quantile val:\", label_quantile_val)\n",
    "            # print(\"pred quantile val:\", pred_quantile_val)\n",
    "            # print(pred_quantile, label_quantile)\n",
    "            # print(\"TO FUNCTION\")\n",
    "            f1_val = f1_score_masked(pred_quantile, label_quantile)\n",
    "            prec_val = prec_score_masked(pred_quantile, label_quantile)\n",
    "            recall_val = recall_score_masked(pred_quantile, label_quantile)\n",
    "\n",
    "            # print(f1_val)\n",
    "\n",
    "            f1_sample.append(f1_val)\n",
    "            prec_sample.append(prec_val)\n",
    "            recall_sample.append(recall_val)\n",
    "\n",
    "        all_f1_scores.append(f1_sample)\n",
    "        all_prec_scores.append(prec_sample)\n",
    "        all_recall_scores.append(recall_sample)\n",
    "\n",
    "    # make stats for each quantile from all samples\n",
    "    all_f1_scores = np.asarray(all_f1_scores)\n",
    "    all_f1_scores = np.transpose(all_f1_scores)\n",
    "\n",
    "    all_prec_scores = np.asarray(all_prec_scores)\n",
    "    all_prec_scores = np.transpose(all_prec_scores)\n",
    "\n",
    "    all_recall_scores = np.asarray(all_recall_scores)\n",
    "    all_recall_scores = np.transpose(all_recall_scores)\n",
    "\n",
    "    # print(all_f1_scores.shape)\n",
    "\n",
    "    # get mean and std for each quantile\n",
    "    quantile_means_f1 = []\n",
    "    quantile_stds_f1 = []\n",
    "    quantile_means_prec = []\n",
    "    quantile_stds_prec = []\n",
    "    quantile_means_recall = []\n",
    "    quantile_stds_recall = []\n",
    "    for f in range(len(all_f1_scores)):\n",
    "        quantile_means_f1.append(np.mean(all_f1_scores[f]))\n",
    "        quantile_stds_f1.append(np.std(all_f1_scores[f]))\n",
    "\n",
    "        quantile_means_prec.append(np.mean(all_prec_scores[f]))\n",
    "        quantile_stds_prec.append(np.std(all_prec_scores[f]))\n",
    "\n",
    "        quantile_means_recall.append(np.mean(all_recall_scores[f]))\n",
    "        quantile_stds_recall.append(np.std(all_recall_scores[f]))\n",
    "\n",
    "    # plot the mean and std for each quantile\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.errorbar(quantiles, quantile_means_f1, yerr=quantile_stds_f1, fmt='o', color='#130f40')\n",
    "    # draw a smooth curve connecting the points from means\n",
    "    X_Y_Spline = make_interp_spline(quantiles, quantile_means_f1)\n",
    "    # Returns evenly spaced numbers\n",
    "    # over a specified interval.\n",
    "    X_ = np.linspace(min(quantiles), max(quantiles), 500)\n",
    "    Y_ = X_Y_Spline(X_)\n",
    "    plt.plot(X_, Y_, color='#0097e6')\n",
    "    # shade the area under the curve\n",
    "    plt.fill_between(X_, Y_, color='#0097e6', alpha=0.4)\n",
    "    plt.xlabel('Quantile')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs Quantile')\n",
    "    plt.savefig(output_loc + 'f1_quantile.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.errorbar(quantiles, quantile_means_prec, yerr=quantile_stds_prec, fmt='o', color='#130f40')\n",
    "    # draw a smooth curve connecting the points from means\n",
    "    X_Y_Spline = make_interp_spline(quantiles, quantile_means_prec)\n",
    "    # Returns evenly spaced numbers\n",
    "    # over a specified interval.\n",
    "    X_ = np.linspace(min(quantiles), max(quantiles), 500)\n",
    "    Y_ = X_Y_Spline(X_)\n",
    "    plt.plot(X_, Y_, color='#0097e6')\n",
    "    # shade the area under the curve\n",
    "    plt.fill_between(X_, Y_, color='#0097e6', alpha=0.4)\n",
    "    plt.xlabel('Quantile')\n",
    "    plt.ylabel('Precision Score')\n",
    "    plt.title('Precision Score vs Quantile')\n",
    "    plt.savefig(output_loc + 'prec_quantile.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.errorbar(quantiles, quantile_means_recall, yerr=quantile_stds_recall, fmt='o', color='#130f40')\n",
    "    # draw a smooth curve connecting the points from means\n",
    "    X_Y_Spline = make_interp_spline(quantiles, quantile_means_recall)\n",
    "    # Returns evenly spaced numbers\n",
    "    # over a specified interval.\n",
    "    X_ = np.linspace(min(quantiles), max(quantiles), 500)\n",
    "    Y_ = X_Y_Spline(X_)\n",
    "    plt.plot(X_, Y_, color='#0097e6')\n",
    "    # shade the area under the curve\n",
    "    plt.fill_between(X_, Y_, color='#0097e6', alpha=0.4)\n",
    "    plt.xlabel('Quantile')\n",
    "    plt.ylabel('Recall Score')\n",
    "    plt.title('Recall Score vs Quantile')\n",
    "    plt.savefig(output_loc + 'recall_quantile.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # using quantile means get the area under the curve\n",
    "    auc_f1 = np.trapz(quantile_means_f1, dx=0.1)\n",
    "    auc_prec = np.trapz(quantile_means_prec, dx=0.1)\n",
    "    auc_recall = np.trapz(quantile_means_recall, dx=0.1)\n",
    "    print(\"AUC F1 Score: \", auc_f1)\n",
    "    print(\"AUC Precision Score: \", auc_prec)\n",
    "    print(\"AUC Recall Score: \", auc_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretability functions\n",
    "def attention_maps(model, test_dataset, output_loc):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    list_attn_matrices = []\n",
    "    max_len = 0\n",
    "    lens_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y, ctrl_y, gene, transcript) in enumerate(test_dataset):\n",
    "            lengths = torch.tensor([len(x)])\n",
    "\n",
    "            x = torch.tensor(x).unsqueeze(0)\n",
    "            y = torch.tensor(y).unsqueeze(0)\n",
    "            ctrl_y = torch.tensor(ctrl_y).unsqueeze(0)\n",
    "            \n",
    "            x = pad_sequence(x, batch_first=True, padding_value=192) \n",
    "            y = pad_sequence(y, batch_first=True, padding_value=-1)\n",
    "            ctrl_y = pad_sequence(ctrl_y, batch_first=True, padding_value=-1)\n",
    "\n",
    "            out_batch = {}\n",
    "\n",
    "            out_batch[\"input_ids\"] = x\n",
    "            out_batch[\"labels\"] = y\n",
    "            out_batch[\"lengths\"] = lengths\n",
    "            out_batch[\"labels_ctrl\"] = ctrl_y\n",
    "\n",
    "            # send batch to device\n",
    "            for k, v in out_batch.items():\n",
    "                out_batch[k] = v.to(device)\n",
    "\n",
    "            out = model(out_batch[\"input_ids\"], output_attentions = True, return_dict = True)\n",
    "            attn_vec1 = out.attentions[0].cpu().detach().numpy()\n",
    "            attn_vec2 = out.attentions[1].cpu().detach().numpy()\n",
    "            attn_vec3 = out.attentions[2].cpu().detach().numpy()\n",
    "\n",
    "            attn_vec_full = attn_vec1 # only first layer because this is the only one that looks at the input\n",
    "\n",
    "            # remove dim 0\n",
    "            attn_vec_full = np.squeeze(attn_vec_full, axis=0)\n",
    "\n",
    "            # average across heads\n",
    "            attn_vec_full = np.mean(attn_vec_full, axis=0)\n",
    "\n",
    "            list_attn_matrices.append(attn_vec_full)\n",
    "\n",
    "            if len(attn_vec_full) > max_len:\n",
    "                max_len = len(attn_vec_full)\n",
    "            \n",
    "            lens_list.append(len(attn_vec_full))\n",
    "\n",
    "    mean_len = int(np.mean(lens_list))\n",
    "    # make matrix of shape max_len x max_len\n",
    "    attn_matrix = np.zeros((max_len, max_len))\n",
    "    # add all attention matrices to the matrix\n",
    "    count_lengths = np.zeros(max_len)\n",
    "    for i in range(len(list_attn_matrices)):\n",
    "        count_lengths[:len(list_attn_matrices[i])] += 1\n",
    "\n",
    "    for i in range(len(list_attn_matrices)):\n",
    "        attn_matrix[:len(list_attn_matrices[i]), :len(list_attn_matrices[i])] += (list_attn_matrices[i] / count_lengths[:len(list_attn_matrices[i])])\n",
    "\n",
    "    # crop the matrix to the mean length\n",
    "    attn_matrix = attn_matrix[:mean_len, :mean_len]\n",
    "\n",
    "    # plot the attention matrix\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(attn_matrix)\n",
    "    plt.savefig(output_loc + 'attention_map.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # log transform the attention maps\n",
    "    attention_maps_logged = np.log(attn_matrix)\n",
    "\n",
    "    # plot the log transformed attention matrix\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(attention_maps_logged)\n",
    "    plt.savefig(output_loc + 'attention_map_logged.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    # save the attention matrix\n",
    "    np.save(output_loc + 'attention_map.npy', attn_matrix)\n",
    "\n",
    "    # check how many elements next to the principal diagonal have high values on average\n",
    "    left_diag = []\n",
    "    right_diag = []\n",
    "\n",
    "    threshold_val = 3\n",
    "\n",
    "    attention_maps_logged_thresholded = [] \n",
    "\n",
    "    for i in range(attention_maps_logged.shape[0]):\n",
    "        # A site value\n",
    "        max_val_i = np.max(attention_maps_logged[i])\n",
    "\n",
    "        # left elements\n",
    "        left_i = attention_maps_logged[i, :i]\n",
    "        # number of elements with values between max_val_i and max_val_i - threshold_val\n",
    "        left_i_threshold = left_i > (max_val_i - threshold_val)\n",
    "        # sum of non zero elements\n",
    "        left_diag.append(np.sum(left_i_threshold))\n",
    "\n",
    "        # right elements\n",
    "        right_i = attention_maps_logged[i, i+1:]\n",
    "        # number of elements with values between max_val_i and max_val_i - threshold_val\n",
    "        right_i_threshold = right_i > (max_val_i - threshold_val)\n",
    "        right_diag.append(np.sum(right_i_threshold))\n",
    "\n",
    "        # row of the thresholded attention map \n",
    "        row = np.concatenate((left_i_threshold, [1], right_i_threshold))\n",
    "        attention_maps_logged_thresholded.append(row)\n",
    "\n",
    "    # print means of left and right diagonals\n",
    "    print(\"Number of important codons to the left (avg): \", np.mean(left_diag))\n",
    "    print(\"Number of important codons to the right (avg): \", np.mean(right_diag))\n",
    "\n",
    "    attention_maps_logged_thresholded = np.array(attention_maps_logged_thresholded)\n",
    "\n",
    "    # plot the log transformed attention matrix\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(attention_maps_logged_thresholded)\n",
    "    plt.savefig(output_loc + 'attention_map_logged_Thresh3.png')\n",
    "    plt.show()\n",
    "    plt.clf()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d8cddb8cf669a224cfe7de41be728b42e6d1e4d2fa8033c260d761c14134291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
