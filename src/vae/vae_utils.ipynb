{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import pearson_corrcoef\n",
    "from torchmetrics.regression import MeanAbsolutePercentageError, MeanAbsoluteError\n",
    "from torchmetrics import Metric\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightning as L\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "id_to_codon = {idx:''.join(el) for idx, el in enumerate(itertools.product(['A', 'T', 'C', 'G'], repeat=3))}\n",
    "codon_to_id = {v:k for k,v in id_to_codon.items()}\n",
    "\n",
    "# dataset generation functions\n",
    "def longestZeroSeqLength(a):\n",
    "    '''\n",
    "    length of the longest sub-sequence of zeros\n",
    "    '''\n",
    "    a = a[1:-1].split(', ')\n",
    "    a = [float(k) for k in a]\n",
    "    # longest sequence of zeros\n",
    "    longest = 0\n",
    "    current = 0\n",
    "    for i in a:\n",
    "        if i == 0.0:\n",
    "            current += 1\n",
    "        else:\n",
    "            longest = max(longest, current)\n",
    "            current = 0\n",
    "    return longest\n",
    "\n",
    "def percNans(a):\n",
    "    '''\n",
    "    returns the percentage of nans in the sequence\n",
    "    '''\n",
    "    a = a[1:-1].split(', ')\n",
    "    a = [float(k) for k in a]\n",
    "    a = np.asarray(a)\n",
    "    perc = np.count_nonzero(np.isnan(a)) / len(a)\n",
    "\n",
    "    return perc\n",
    "\n",
    "def RiboDatasetGWS(data_folder: str, dataset_split: str, ds: str, threshold: float = 0.6, longZerosThresh: int = 20, percNansThresh: float = 0.1):\n",
    "    if ds == 'ALL':\n",
    "        # # paths \n",
    "        # ctrl_path = data_folder + 'CTRL.csv'\n",
    "        # leu_path = data_folder + 'LEU.csv'\n",
    "        # arg_path = data_folder + 'ARG.csv'\n",
    "\n",
    "        # # load the data\n",
    "        # df_ctrl = pd.read_csv(ctrl_path)\n",
    "        # df_ctrl['condition'] = 'CTRL'\n",
    "        # df_leu = pd.read_csv(leu_path)\n",
    "        # df_leu['condition'] = 'LEU'\n",
    "        # df_arg = pd.read_csv(arg_path)\n",
    "        # df_arg['condition'] = 'ARG'\n",
    "\n",
    "        # df_full = pd.concat([df_ctrl, df_leu, df_arg], axis=0) # ctrl, leu, arg\n",
    "\n",
    "        # # drop first column\n",
    "        # df_full = df_full.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "        # df_full.columns = ['gene', 'transcript', 'sequence', 'annotations', 'perc_non_zero_annots', 'condition']\n",
    "\n",
    "        # # apply annot threshold\n",
    "        # df_full = df_full[df_full['perc_non_zero_annots'] >= threshold]\n",
    "\n",
    "        # # for all the sequences in a condition that is not CTRL, add their respective CTRL sequence to them\n",
    "        # sequences_ctrl = []\n",
    "        # annotations_list = list(df_full['annotations'])\n",
    "        # condition_df_list = list(df_full['condition'])\n",
    "        # transcripts_list = list(df_full['transcript'])\n",
    "\n",
    "        # for i in range(len(condition_df_list)):\n",
    "        #     try:\n",
    "        #         if condition_df_list[i] != 'CTRL':\n",
    "        #             # find the respective CTRL sequence for the transcript\n",
    "        #             ctrl_sequence = df_full[(df_full['transcript'] == transcripts_list[i]) & (df_full['condition'] == 'CTRL')]['annotations'].iloc[0]\n",
    "        #             sequences_ctrl.append(ctrl_sequence)\n",
    "        #         else:\n",
    "        #             sequences_ctrl.append(annotations_list[i])\n",
    "        #     except:\n",
    "        #         sequences_ctrl.append('NA')\n",
    "\n",
    "        # # add the sequences_ctrl to the df\n",
    "        # print(len(sequences_ctrl), len(annotations_list))\n",
    "        # df_full['ctrl_sequence'] = sequences_ctrl\n",
    "\n",
    "        # # remove those rows where the ctrl_sequence is NA\n",
    "        # df_full = df_full[df_full['ctrl_sequence'] != 'NA']\n",
    "\n",
    "        # # sanity check for the ctrl sequences\n",
    "        # # get the ds with only condition as CTRL\n",
    "        # df_ctrl_full = df_full[df_full['condition'] == 'CTRL']\n",
    "        # ctrl_sequences_san = list(df_ctrl_full['annotations'])\n",
    "        # ctrl_sequences_san2 = list(df_ctrl_full['ctrl_sequence'])\n",
    "\n",
    "        # for i in range(len(ctrl_sequences_san)):\n",
    "        #     assert ctrl_sequences_san[i] == ctrl_sequences_san2[i]\n",
    "\n",
    "        # print(\"Sanity Checked\")\n",
    "\n",
    "        # # get longest zero sequence length for each sequence in annotations and ctrl_sequence\n",
    "        # annotations_list = list(df_full['annotations'])\n",
    "        # sequences_ctrl = list(df_full['ctrl_sequence'])\n",
    "        # annotation_long_zeros = []\n",
    "        # ctrl_sequence_long_zeros = []\n",
    "        # num_nans_full = []\n",
    "        # num_nans_ctrl = []\n",
    "        # for i in range(len(annotations_list)):\n",
    "        #     annotation_long_zeros.append(longestZeroSeqLength(annotations_list[i]))\n",
    "        #     ctrl_sequence_long_zeros.append(longestZeroSeqLength(sequences_ctrl[i]))\n",
    "        #     num_nans_full.append(percNans(annotations_list[i]))\n",
    "        #     num_nans_ctrl.append(percNans(sequences_ctrl[i]))\n",
    "\n",
    "        # # add the longest zero sequence length to the df\n",
    "        # df_full['longest_zero_seq_length_annotation'] = annotation_long_zeros\n",
    "        # df_full['longest_zero_seq_length_ctrl_sequence'] = ctrl_sequence_long_zeros\n",
    "\n",
    "        # # add the number of nans to the df\n",
    "        # df_full['perc_nans_annotation'] = num_nans_full\n",
    "        # df_full['perc_nans_ctrl_sequence'] = num_nans_ctrl\n",
    "\n",
    "        # # apply the threshold for the longest zero sequence length\n",
    "        # df_full = df_full[df_full['longest_zero_seq_length_annotation'] <= longZerosThresh]\n",
    "        # df_full = df_full[df_full['longest_zero_seq_length_ctrl_sequence'] <= longZerosThresh]\n",
    "\n",
    "        # # apply the threshold for the number of nans\n",
    "        # df_full = df_full[df_full['perc_nans_annotation'] <= percNansThresh]\n",
    "        # df_full = df_full[df_full['perc_nans_ctrl_sequence'] <= percNansThresh]\n",
    "\n",
    "        # # GWS\n",
    "        # genes = df_full['gene'].unique()\n",
    "        # genes_train, genes_test = train_test_split(genes, test_size=0.2, random_state=42)\n",
    "\n",
    "        # # split the dataframe\n",
    "        # df_train = df_full[df_full['gene'].isin(genes_train)]\n",
    "        # df_test = df_full[df_full['gene'].isin(genes_test)]\n",
    "\n",
    "        out_train_path = 'data/dh/ribo_train_' + str(dataset_split) + '-NA_dh_' + str(threshold) + '_NZ_' + str(longZerosThresh) + '_PercNan_' + str(percNansThresh) + '.csv'\n",
    "        out_test_path = 'data/dh/ribo_test_' + str(dataset_split) + '-NA_dh_' + str(threshold) + '_NZ_' + str(longZerosThresh) + '_PercNan_' + str(percNansThresh) + '.csv'\n",
    "\n",
    "        # df_train.to_csv(out_train_path, index=False)\n",
    "        # df_test.to_csv(out_test_path, index=False)\n",
    "\n",
    "        df_train = pd.read_csv(out_train_path)\n",
    "        df_test = pd.read_csv(out_test_path)\n",
    "\n",
    "        return df_train, df_test\n",
    "\n",
    "class GWSDatasetFromPandasVAE(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.counts = list(self.df['annotations'])\n",
    "        self.sequences = list(self.df['sequence'])\n",
    "        self.condition_lists = list(self.df['condition'])\n",
    "        self.condition_values = {'CTRL': 0, 'LEU': 1, 'ARG': 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.df['sequence'].iloc[idx]\n",
    "        # convert to int\n",
    "        X = X[1:-1].split(', ')\n",
    "        X = [int(i) for i in X]\n",
    "\n",
    "        y = self.df['annotations'].iloc[idx]\n",
    "        # convert string into list of floats\n",
    "        y = y[1:-1].split(', ')\n",
    "        y = [float(i) for i in y]\n",
    "\n",
    "        y = [1+i for i in y]\n",
    "        y = np.log(y)\n",
    "\n",
    "        # ctrl sequence \n",
    "        ctrl_y = self.df['ctrl_sequence'].iloc[idx]\n",
    "        # convert string into list of floats\n",
    "        ctrl_y = ctrl_y[1:-1].split(', ')\n",
    "        ctrl_y = [float(i) for i in ctrl_y]\n",
    "\n",
    "        # no min max scaling\n",
    "        ctrl_y = [1+i for i in ctrl_y]\n",
    "        ctrl_y = np.log(ctrl_y)\n",
    "\n",
    "        X = np.array(X)\n",
    "        # multiply X with condition value times 64 + 1\n",
    "        add_factor = (self.condition_values[self.condition_lists[idx]]) * 64\n",
    "        X += add_factor\n",
    "\n",
    "        y = np.array(y)\n",
    "        len_X = len(X)\n",
    "\n",
    "        X = torch.from_numpy(X).long()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        ctrl_y = torch.from_numpy(ctrl_y).float()\n",
    "\n",
    "        gene = self.df['gene'].iloc[idx]\n",
    "        transcript = self.df['transcript'].iloc[idx]\n",
    "\n",
    "        return X, y, gene, transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_dims = latent_dims\n",
    "        self.fc1 = nn.Linear(1, 256)\n",
    "        self.fc2 = nn.Linear(256, latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dims = latent_dims\n",
    "        self.fc1 = nn.Linear(latent_dims, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class VAE(L.LightningModule):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__()\n",
    "        # # Saving hyperparameters of autoencoder\n",
    "        # self.save_hyperparameters()\n",
    "        # # Creating encoder and decoder\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        # Example input array needed for visualizing the graph of the network\n",
    "        self.example_input_array = torch.zeros(1, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case).\"\"\"\n",
    "        x, y, gene, transcript = batch  # We do not need the labels\n",
    "        y_hat = self.forward(y)\n",
    "        recon_loss = F.mse_loss(y, y_hat, reduction=\"none\")\n",
    "        vae_loss = \n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=20, min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(latent_dim, save_loc, train_loader, test_loader):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=save_loc,\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=500,\n",
    "        callbacks=[\n",
    "            L.pytorch.callbacks.ModelCheckpoint(save_weights_only=True),\n",
    "            L.pytorch.callbacks.LearningRateMonitor(\"epoch\"),\n",
    "        ],\n",
    "    )\n",
    "    trainer.logger._log_graph = False  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = save_loc\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = VAE.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = VAE(latent_dim=latent_dim)\n",
    "        trainer.fit(model, train_loader, test_loader)\n",
    "    # Test best model on test set\n",
    "    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\": test_result}\n",
    "    return model, result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d8cddb8cf669a224cfe7de41be728b42e6d1e4d2fa8033c260d761c14134291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
